{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Openshift Bootcamp Duration 2days Level Intermediate, Advanced Modules 10 Flipped Class No Customizable Yes Objectives This course serves as a accelerator program to understand, setup and master openshift container platform for professionals who already have an understanding of docker and kubernetes. Who is this for ? This course is for someone who has already taken docker fundamentals, and kubernetes bootcamp courses/have equivalent knowledge, and would like to learn how to leverage Openshift Platform which is an abstraction on top of it. If you are a Operations/Systems personnel and would like to learn how to build a production grade scalable, fault tolerant and high available openshift infrastructure, and administer it, this course it for you. If you are a developer and would like to learn how to deploy your application stacks in production, on top of paas solution and also understand the underlying primitives, this course is for you. You could be developer/operations personnel and be in charge of securing application infrastructure and setup auxiliary services such as monitoring, centralized logging etc. this course is for you. Who is this not for ? If you are a advanced user of Openshift already, this course is definitely not for you. If you are interested in learning docker/container orchestration on windows , this course is not ideal for you as it focuses on linux containers. This is mix course for both developers and operations. If you are looking for a course which is very specific to an audience e.g. administrators or developers, ask for a custom outline. What will you do as part of this course ? As part of this course you will, GO through the theory to learn what is openshift platform, the core concepts relates and the advantages of using it. Install and configure a simple(non HA) multi node Openshift cluster with ansible . You would also have a conceptual understanding of how to build a production quality cluster with high availability, scalability, redundancy and security considerations. Learn how to deploy, configure, interconnect and publish and scale applications as well as isolate those with multi tenant environments that openshift provides and underlying kubernetes primitives that it leverages. Achieve Continuous Integration and Delivery with openshift's integration with git, jenkins and its implicit primitives including builds, pipelines and image streams. Learn how to manage persistent storage in a openshift environment Learn about the network and security considerations and features offered by Openshift. Learn openshift administration tasks such as setting up quotas, managing memberships, monitoring etc. What is not covered ? Even though this course covers many concepts related to kubernetes, since its a very vast topic, it still has the following areas uncovered. Cloud specific provisioning and integration HA installation of a kubernetes cluster with multi masters In depth openshift administration Writing Micro Services Applications Alternate container runtimes e.g. rocker/rkt, runc Pre Requisites Following are the pre requisite skills to attend this course. Since its a beginner level course, no prior experience with linux containers is assumed. Courses You should have attended the following course, or have demonstrable knowledge with the topics included in the following course. Docker Fundamentals Kubernetes Bootcamp Pre Assessment test will be conducted at the beginning of the course to asses the skills. Skills Docker Kubernetes Linux/Unix Systems Fundamentals Familiarity with Command Line Interface ( CLI ) Fundamental knowledge of editors on linux (any one of vi/nano/emacs) Understanding of YAML syntax and familiarity with reading/writing basic YAML specifications Recommended to have a basic understanding of Ansible Hardware and Software Requirements These are the prerequisites for each attendee. Hardware Requirements Software Requirements Laptop/Desktop with high speed internet connection Base Operating System : Windows / Mac OSX 8 GB RAM VirtualBox 4 CPU Cores Vagrant 20 GB Disk Space available ConEmu (Windows Only) Git for Windows (windows only) minishift Lab Setup : Instructions can be found at xxx Supporting Content/Materials Following is the supporting material which will be provided to you before/during the course Slides (online) Workshop (online link) Video Course - XXX by School of Devops Pre Class Checklist All participants should have completed the following checklist before attending the course . Successfully Completed Docker Fundamentals Course, or have equivalent skills. Successfully Completed Kubernetes Fundamentals Course, or have equivalent skills. Verify your system meets the hardware pre requisites. Validate the setup : verify all pre requisite software is installed on your system and is functional. Join our kubernetes channel on gitter Topics Following are the topics which would be covered as part of this course. Detailed course outline follows. Introduction to Openshift Openshift Quick Dive Application Lifecycle Management Application Stack Mapping Continuous Integration and Delivery Designing Production Grade Openshift Architecture Setup a Openshift Cluster Securing Openshift Openshift Administration Detailed Course Outline This is the detailed course outline with day wise list of contents Day I Introduction and Pre Assessment Trainer, class and course introduction Pre Assessment Test Module 1: Introduction to Openshift What is Openshift? Kubernetes Vs Openshift Key Features Architecture Module 2: Openshift Quick Dive Minishift Setup Minishift Create a Openshift environment Login and validate Module 3: Application Lifecycle Management Projects Types of application deployments Catalogue Image Image stream Bring your own image YAML files Module 4: Application Stack Mapping Deployments Pods Service Routes ConfigMaps Secrets Auto Scaling Module 5: Continuous Integration and Delivery Image streams Builds Git Integration and auto tagging Pipelines and Jenkins integration Automated Delivery Module 6: Designing Production Grade Openshift Architecture Components of Openshift etcd Masters and Nodes Registry Web console Achieving High Availability(HA) Module 7: Setup a Openshift Cluster Types of Openshfit installations Provision nodes Installing Openshift with Ansible Configuring inventory Defining variables Run a playbook Openshift Configuration Master and Node configs Authentication Authorization Network Configuration SDN with OpenVswitch SDN plugins Alternate configs Packet flow Registry Configuration Web Console Configuration Persistent Storage Storage plugins PV, PVC, Storage Class, etc ., Module 8: Securing Openshift Authentication Authorization RBAC Cluster and Local Roles Rules and Bindings SSC Security Context Constraints Security considerations Module 9: Openshift Administration Quota Memberships Monitoring Logging","title":"Home"},{"location":"#openshift-bootcamp","text":"Duration 2days Level Intermediate, Advanced Modules 10 Flipped Class No Customizable Yes","title":"Openshift Bootcamp"},{"location":"#objectives","text":"This course serves as a accelerator program to understand, setup and master openshift container platform for professionals who already have an understanding of docker and kubernetes.","title":"Objectives"},{"location":"#who-is-this-for","text":"This course is for someone who has already taken docker fundamentals, and kubernetes bootcamp courses/have equivalent knowledge, and would like to learn how to leverage Openshift Platform which is an abstraction on top of it. If you are a Operations/Systems personnel and would like to learn how to build a production grade scalable, fault tolerant and high available openshift infrastructure, and administer it, this course it for you. If you are a developer and would like to learn how to deploy your application stacks in production, on top of paas solution and also understand the underlying primitives, this course is for you. You could be developer/operations personnel and be in charge of securing application infrastructure and setup auxiliary services such as monitoring, centralized logging etc. this course is for you.","title":"Who is this for ?"},{"location":"#who-is-this-not-for","text":"If you are a advanced user of Openshift already, this course is definitely not for you. If you are interested in learning docker/container orchestration on windows , this course is not ideal for you as it focuses on linux containers. This is mix course for both developers and operations. If you are looking for a course which is very specific to an audience e.g. administrators or developers, ask for a custom outline.","title":"Who is this not for ?"},{"location":"#what-will-you-do-as-part-of-this-course","text":"As part of this course you will, GO through the theory to learn what is openshift platform, the core concepts relates and the advantages of using it. Install and configure a simple(non HA) multi node Openshift cluster with ansible . You would also have a conceptual understanding of how to build a production quality cluster with high availability, scalability, redundancy and security considerations. Learn how to deploy, configure, interconnect and publish and scale applications as well as isolate those with multi tenant environments that openshift provides and underlying kubernetes primitives that it leverages. Achieve Continuous Integration and Delivery with openshift's integration with git, jenkins and its implicit primitives including builds, pipelines and image streams. Learn how to manage persistent storage in a openshift environment Learn about the network and security considerations and features offered by Openshift. Learn openshift administration tasks such as setting up quotas, managing memberships, monitoring etc.","title":"What will you do as part of this course ?"},{"location":"#what-is-not-covered","text":"Even though this course covers many concepts related to kubernetes, since its a very vast topic, it still has the following areas uncovered. Cloud specific provisioning and integration HA installation of a kubernetes cluster with multi masters In depth openshift administration Writing Micro Services Applications Alternate container runtimes e.g. rocker/rkt, runc","title":"What is not covered ?"},{"location":"#pre-requisites","text":"Following are the pre requisite skills to attend this course. Since its a beginner level course, no prior experience with linux containers is assumed.","title":"Pre Requisites"},{"location":"#courses","text":"You should have attended the following course, or have demonstrable knowledge with the topics included in the following course. Docker Fundamentals Kubernetes Bootcamp Pre Assessment test will be conducted at the beginning of the course to asses the skills.","title":"Courses"},{"location":"#skills","text":"Docker Kubernetes Linux/Unix Systems Fundamentals Familiarity with Command Line Interface ( CLI ) Fundamental knowledge of editors on linux (any one of vi/nano/emacs) Understanding of YAML syntax and familiarity with reading/writing basic YAML specifications Recommended to have a basic understanding of Ansible","title":"Skills"},{"location":"#hardware-and-software-requirements","text":"These are the prerequisites for each attendee. Hardware Requirements Software Requirements Laptop/Desktop with high speed internet connection Base Operating System : Windows / Mac OSX 8 GB RAM VirtualBox 4 CPU Cores Vagrant 20 GB Disk Space available ConEmu (Windows Only) Git for Windows (windows only) minishift Lab Setup : Instructions can be found at xxx","title":"Hardware and Software  Requirements"},{"location":"#supporting-contentmaterials","text":"Following is the supporting material which will be provided to you before/during the course Slides (online) Workshop (online link) Video Course - XXX by School of Devops","title":"Supporting Content/Materials"},{"location":"#pre-class-checklist","text":"All participants should have completed the following checklist before attending the course . Successfully Completed Docker Fundamentals Course, or have equivalent skills. Successfully Completed Kubernetes Fundamentals Course, or have equivalent skills. Verify your system meets the hardware pre requisites. Validate the setup : verify all pre requisite software is installed on your system and is functional. Join our kubernetes channel on gitter","title":"Pre Class Checklist"},{"location":"#topics","text":"Following are the topics which would be covered as part of this course. Detailed course outline follows. Introduction to Openshift Openshift Quick Dive Application Lifecycle Management Application Stack Mapping Continuous Integration and Delivery Designing Production Grade Openshift Architecture Setup a Openshift Cluster Securing Openshift Openshift Administration","title":"Topics"},{"location":"#detailed-course-outline","text":"This is the detailed course outline with day wise list of contents","title":"Detailed Course Outline"},{"location":"#day-i","text":"","title":"Day I"},{"location":"#introduction-and-pre-assessment","text":"Trainer, class and course introduction Pre Assessment Test","title":"Introduction and Pre Assessment"},{"location":"#module-1-introduction-to-openshift","text":"What is Openshift? Kubernetes Vs Openshift Key Features Architecture","title":"Module 1: Introduction to Openshift"},{"location":"#module-2-openshift-quick-dive","text":"Minishift Setup Minishift Create a Openshift environment Login and validate","title":"Module 2: Openshift Quick Dive"},{"location":"#module-3-application-lifecycle-management","text":"Projects Types of application deployments Catalogue Image Image stream Bring your own image YAML files","title":"Module 3: Application Lifecycle Management"},{"location":"#module-4-application-stack-mapping","text":"Deployments Pods Service Routes ConfigMaps Secrets Auto Scaling","title":"Module 4: Application Stack Mapping"},{"location":"#module-5-continuous-integration-and-delivery","text":"Image streams Builds Git Integration and auto tagging Pipelines and Jenkins integration Automated Delivery","title":"Module 5: Continuous Integration and Delivery"},{"location":"#module-6-designing-production-grade-openshift-architecture","text":"Components of Openshift etcd Masters and Nodes Registry Web console Achieving High Availability(HA)","title":"Module 6: Designing Production Grade Openshift Architecture"},{"location":"#module-7-setup-a-openshift-cluster","text":"Types of Openshfit installations Provision nodes Installing Openshift with Ansible Configuring inventory Defining variables Run a playbook Openshift Configuration Master and Node configs Authentication Authorization Network Configuration SDN with OpenVswitch SDN plugins Alternate configs Packet flow Registry Configuration Web Console Configuration Persistent Storage Storage plugins PV, PVC, Storage Class, etc .,","title":"Module 7: Setup a Openshift Cluster"},{"location":"#module-8-securing-openshift","text":"Authentication Authorization RBAC Cluster and Local Roles Rules and Bindings SSC Security Context Constraints Security considerations","title":"Module 8: Securing Openshift"},{"location":"#module-9-openshift-administration","text":"Quota Memberships Monitoring Logging","title":"Module 9: Openshift Administration"},{"location":"01_deploy_first_app_openshift/","text":"Deploy First Openshift Application. Let start to deploy our first sample application on openshift and you don't know have to docker and kubernetes because openshift is pass solution only need for the source code. * Go to openshift UI Click on myproject Go to catalog and select php click next select the php version * provide git repo. https://github.com/openshift/cakephp-ex.git click next and close click Overview and see the builds you can click on bulid it goes to build UI you can see here log, terminal, datails, Events Once you build the image it automatically trigger new Depolyment. Go to Overview you can see here deployment Go to Build -> images you can see here published images","title":"01 deploy first app openshift"},{"location":"01_deploy_first_app_openshift/#deploy-first-openshift-application","text":"Let start to deploy our first sample application on openshift and you don't know have to docker and kubernetes because openshift is pass solution only need for the source code. * Go to openshift UI Click on myproject Go to catalog and select php click next select the php version * provide git repo. https://github.com/openshift/cakephp-ex.git click next and close click Overview and see the builds you can click on bulid it goes to build UI you can see here log, terminal, datails, Events Once you build the image it automatically trigger new Depolyment. Go to Overview you can see here deployment Go to Build -> images you can see here published images","title":"Deploy First Openshift Application."},{"location":"02_devops_php_app/","text":"Deploying Devops Demo PHP Application for GitHub Repo In this section we are deploying sample devops demo PHP app from GitHub repo. we use the same cluster earlier deployed simple aaplication with diffrent project namespace. This docs demonstrates how to get a simple project up and running on OpenShift. Application that will serve a welcome page Welcome to the DevOps Demo Application. Go to Home Page. click on create project. Enter the project name Display name and describtions. After creating the project namespace swaitch to yor new project name space you can see here new worksapce. click the Browse catalog you can see here multiple programming languages and databases. select the php you can see here php+mysql and only php choose php After selecting the language provide the information , configuration In configuration you can provides version of the programming language and following GitHub repo. https://github.com/devopsdemoapps/devops-demo-app.git click on the Advance option In this section yoc can provides following options. * reference Tag Routing Deployment Configuration scaling Resource Limit Labels Then click create project. After create project automated build build your project . In the web console, view the Overview page for your project to determine the web address for your application. Click the web address displayed right side web console open this link on new tab. ### OpenShift CommandLine When we are using OpenShift many of the tasks that you need to do can be performed through the OpenShift web Broser console or directly using the oc command line tool. or you can observe here backed utility. In this case we need the OpenShift token . we can create token using opnshift web console click on user Devloper and there is option copy login command click on this paste on your console. oc login https://128.199.213.193:8443 --token=< your token> [output] Logged into \"https://128.199.213.193:8443\" as \"developer\" using the token provided. You have access to the following projects and can switch between them with 'oc project <projectname>': devops-demo-app * myproject Using project \"myproject\". here display your all the project with the curent project namespace. you can swaitch your project namespace using following Command oc project devops-demo-app [output] Now using project \"devops-demo-app\" on server \"https://128.199.213.193:8443\". see the all contexts use following Command oc config get-contexts [output] CURRENT NAME CLUSTER AUTHINFO NAMESPACE /128-199-213-193:8443/developer 128-199-213-193:8443 developer/128-199-213-193:8443 default/128-199-213-193:8443/system:admin 128-199-213-193:8443 system:admin/128-199-213-193:8443 default * myproject/128-199-213-193:8443/developer 128-199-213-193:8443 developer/128-199-213-193:8443 myproject getting the pods here you can use pods or only po oc get pods [output] NAME READY STATUS RESTARTS AGE front-end-1-build 0/1 Completed 0 22m front-end-1-x59fd 1/1 Running 0 21m getting all replicas oc get rc [output] NAME DESIRED CURRENT READY AGE front-end-1 1 1 1 22m getting all services oc get svc [output] NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE front-end ClusterIP 172.30.25.85 <none> 8080/TCP,8443/TCP 24m describe front-end services oc describe svc front-end Name: front-end Namespace: devops-demo-app Labels: app=front-end Annotations: openshift.io/generated-by=OpenShiftWebConsole Selector: deploymentconfig=front-end Type: ClusterIP IP: 172.30.25.85 Port: 8080-tcp 8080/TCP TargetPort: 8080/TCP Endpoints: 172.17.0.8:8080 Port: 8443-tcp 8443/TCP TargetPort: 8443/TCP Endpoints: 172.17.0.8:8443 Session Affinity: None Events: <none> getting deploymentconfig oc get deploymentconfig [output] NAME REVISION DESIRED CURRENT TRIGGERED BY front-end 1 1 1 config,image(front-end:latest)\\ getting build oc get build [output] NAME TYPE FROM STATUS STARTED DURATION front-end-1 Source Git@6ef2140 Complete 29 minutes ago 34s getting route oc get route [output] NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD front-end front-end-devops-demo-app.128.199.213.193.nip.io front-end 8080-tcp None describe route oc describe route front-end [ouutput] Name: front-end Namespace: devops-demo-app Created: 32 minutes ago Labels: app=front-end Annotations: openshift.io/generated-by=OpenShiftWebConsole openshift.io/host.generated=true Requested Host: front-end-devops-demo-app.128.199.213.193.nip.io exposed on router router 32 minutes ago Path: <none> TLS Termination: <none> Insecure Policy: <none> Endpoint Port: 8080-tcp Service: front-end Weight: 100 (100%) Endpoints: 172.17.0.8:8443, 172.17.0.8:8080","title":"02 devops php app"},{"location":"02_devops_php_app/#deploying-devops-demo-php-application-for-github-repo","text":"In this section we are deploying sample devops demo PHP app from GitHub repo. we use the same cluster earlier deployed simple aaplication with diffrent project namespace. This docs demonstrates how to get a simple project up and running on OpenShift. Application that will serve a welcome page Welcome to the DevOps Demo Application. Go to Home Page. click on create project. Enter the project name Display name and describtions. After creating the project namespace swaitch to yor new project name space you can see here new worksapce. click the Browse catalog you can see here multiple programming languages and databases. select the php you can see here php+mysql and only php choose php After selecting the language provide the information , configuration In configuration you can provides version of the programming language and following GitHub repo. https://github.com/devopsdemoapps/devops-demo-app.git click on the Advance option In this section yoc can provides following options. * reference Tag Routing Deployment Configuration scaling Resource Limit Labels Then click create project. After create project automated build build your project . In the web console, view the Overview page for your project to determine the web address for your application. Click the web address displayed right side web console open this link on new tab. ### OpenShift CommandLine When we are using OpenShift many of the tasks that you need to do can be performed through the OpenShift web Broser console or directly using the oc command line tool. or you can observe here backed utility. In this case we need the OpenShift token . we can create token using opnshift web console click on user Devloper and there is option copy login command click on this paste on your console. oc login https://128.199.213.193:8443 --token=< your token> [output] Logged into \"https://128.199.213.193:8443\" as \"developer\" using the token provided. You have access to the following projects and can switch between them with 'oc project <projectname>': devops-demo-app * myproject Using project \"myproject\". here display your all the project with the curent project namespace. you can swaitch your project namespace using following Command oc project devops-demo-app [output] Now using project \"devops-demo-app\" on server \"https://128.199.213.193:8443\". see the all contexts use following Command oc config get-contexts [output] CURRENT NAME CLUSTER AUTHINFO NAMESPACE /128-199-213-193:8443/developer 128-199-213-193:8443 developer/128-199-213-193:8443 default/128-199-213-193:8443/system:admin 128-199-213-193:8443 system:admin/128-199-213-193:8443 default * myproject/128-199-213-193:8443/developer 128-199-213-193:8443 developer/128-199-213-193:8443 myproject getting the pods here you can use pods or only po oc get pods [output] NAME READY STATUS RESTARTS AGE front-end-1-build 0/1 Completed 0 22m front-end-1-x59fd 1/1 Running 0 21m getting all replicas oc get rc [output] NAME DESIRED CURRENT READY AGE front-end-1 1 1 1 22m getting all services oc get svc [output] NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE front-end ClusterIP 172.30.25.85 <none> 8080/TCP,8443/TCP 24m describe front-end services oc describe svc front-end Name: front-end Namespace: devops-demo-app Labels: app=front-end Annotations: openshift.io/generated-by=OpenShiftWebConsole Selector: deploymentconfig=front-end Type: ClusterIP IP: 172.30.25.85 Port: 8080-tcp 8080/TCP TargetPort: 8080/TCP Endpoints: 172.17.0.8:8080 Port: 8443-tcp 8443/TCP TargetPort: 8443/TCP Endpoints: 172.17.0.8:8443 Session Affinity: None Events: <none> getting deploymentconfig oc get deploymentconfig [output] NAME REVISION DESIRED CURRENT TRIGGERED BY front-end 1 1 1 config,image(front-end:latest)\\ getting build oc get build [output] NAME TYPE FROM STATUS STARTED DURATION front-end-1 Source Git@6ef2140 Complete 29 minutes ago 34s getting route oc get route [output] NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD front-end front-end-devops-demo-app.128.199.213.193.nip.io front-end 8080-tcp None describe route oc describe route front-end [ouutput] Name: front-end Namespace: devops-demo-app Created: 32 minutes ago Labels: app=front-end Annotations: openshift.io/generated-by=OpenShiftWebConsole openshift.io/host.generated=true Requested Host: front-end-devops-demo-app.128.199.213.193.nip.io exposed on router router 32 minutes ago Path: <none> TLS Termination: <none> Insecure Policy: <none> Endpoint Port: 8080-tcp Service: front-end Weight: 100 (100%) Endpoints: 172.17.0.8:8443, 172.17.0.8:8080","title":"Deploying Devops Demo PHP Application for GitHub Repo"},{"location":"03_rolling_out_deployment/","text":"Rolling out deployment a new version when application updated In this section we gonna roll out application we alredy deployed here observe how that happen Go to the your project namespace Select the option overview * in overview you can see right side one applications labels there are three options 1. Appications 2. Resource Type 3. Pipeline you can see here also your application buid information Go to Applications -> Deployment -> front-end In deployment shows you currrent deployment for taht application and the right side you can see Deploy and Actions two lables in actions labels you can see multiple options. Edit Pause Rollut Add Stroage Add Autoscaler Edit Resource Limit Edit Helth Checks Edit yaml delete And you can observe here History Configuration Environments Events infor Go to Application -> Deployment -> front-end -> Applications -> Edit Here, OepenShift shows you multiple options of Deployment Statrgy you can provide info here to Deployment Startegy Go to Builds -> Build In this section openshift shows you in right side two options start Build and Actions select Actions -> Edit and create new Build Here, select Advance option and change the version for applications then save when you click on save openshift automatically build your application rollut build. you can also use following command to see the history and status of your application rollout. Rollout History oc rollout history dc/front-end [output] REVISION STATUS CAUSE 1 Complete config change 2 Complete manual change Rollout Status oc rollout status dc/front-end [output] Waiting for rollout to finish: 0 out of 4 new replicas have been updated... Waiting for rollout to finish: 0 out of 4 new replicas have been updated... Waiting for rollout to finish: 1 out of 4 new replicas have been updated... Waiting for rollout to finish: 1 out of 4 new replicas have been updated... Waiting for rollout to finish: 1 out of 4 new replicas have been updated... Waiting for rollout to finish: 1 out of 4 new replicas have been updated... Waiting for rollout to finish: 1 out of 4 new replicas have been updated... Waiting for rollout to finish: 2 out of 4 new replicas have been updated... Waiting for rollout to finish: 2 out of 4 new replicas have been updated... Waiting for rollout to finish: 2 out of 4 new replicas have been updated... Waiting for rollout to finish: 2 out of 4 new replicas have been updated... Waiting for rollout to finish: 2 out of 4 new replicas have been updated... Waiting for rollout to finish: 3 out of 4 new replicas have been updated... Waiting for rollout to finish: 3 out of 4 new replicas have been updated... Waiting for rollout to finish: 3 out of 4 new replicas have been updated... Waiting for rollout to finish: 3 out of 4 new replicas have been updated... Waiting for rollout to finish: 3 out of 4 new replicas have been updated... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 3 of 4 updated replicas are available... Waiting for latest deployment config spec to be observed by the controller loop... replication controller \"front-end-3\" successfully rolled out","title":"03 rolling out deployment"},{"location":"03_rolling_out_deployment/#rolling-out-deployment-a-new-version-when-application-updated","text":"In this section we gonna roll out application we alredy deployed here observe how that happen Go to the your project namespace Select the option overview * in overview you can see right side one applications labels there are three options 1. Appications 2. Resource Type 3. Pipeline you can see here also your application buid information Go to Applications -> Deployment -> front-end In deployment shows you currrent deployment for taht application and the right side you can see Deploy and Actions two lables in actions labels you can see multiple options. Edit Pause Rollut Add Stroage Add Autoscaler Edit Resource Limit Edit Helth Checks Edit yaml delete And you can observe here History Configuration Environments Events infor Go to Application -> Deployment -> front-end -> Applications -> Edit Here, OepenShift shows you multiple options of Deployment Statrgy you can provide info here to Deployment Startegy Go to Builds -> Build In this section openshift shows you in right side two options start Build and Actions select Actions -> Edit and create new Build Here, select Advance option and change the version for applications then save when you click on save openshift automatically build your application rollut build. you can also use following command to see the history and status of your application rollout. Rollout History oc rollout history dc/front-end [output] REVISION STATUS CAUSE 1 Complete config change 2 Complete manual change Rollout Status oc rollout status dc/front-end [output] Waiting for rollout to finish: 0 out of 4 new replicas have been updated... Waiting for rollout to finish: 0 out of 4 new replicas have been updated... Waiting for rollout to finish: 1 out of 4 new replicas have been updated... Waiting for rollout to finish: 1 out of 4 new replicas have been updated... Waiting for rollout to finish: 1 out of 4 new replicas have been updated... Waiting for rollout to finish: 1 out of 4 new replicas have been updated... Waiting for rollout to finish: 1 out of 4 new replicas have been updated... Waiting for rollout to finish: 2 out of 4 new replicas have been updated... Waiting for rollout to finish: 2 out of 4 new replicas have been updated... Waiting for rollout to finish: 2 out of 4 new replicas have been updated... Waiting for rollout to finish: 2 out of 4 new replicas have been updated... Waiting for rollout to finish: 2 out of 4 new replicas have been updated... Waiting for rollout to finish: 3 out of 4 new replicas have been updated... Waiting for rollout to finish: 3 out of 4 new replicas have been updated... Waiting for rollout to finish: 3 out of 4 new replicas have been updated... Waiting for rollout to finish: 3 out of 4 new replicas have been updated... Waiting for rollout to finish: 3 out of 4 new replicas have been updated... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 1 old replicas are pending termination... Waiting for rollout to finish: 3 of 4 updated replicas are available... Waiting for latest deployment config spec to be observed by the controller loop... replication controller \"front-end-3\" successfully rolled out","title":"Rolling out deployment a new version when application updated"},{"location":"04_database_connection/","text":"Service Discovert, Deploying and conneting to a database In this section we gonna to connect databse to you r application Deployment on the database. Here, we are connect to mariaDB database. Go to overview In the overview section openshift shows you in right side Add to Project click on this and select databases and either you can use mysql or maraiaDB. MariaDB is just the fork of mysql database. After selecting the database provide some configuration of the databes like databae name, database connection, database password etc","title":"04 database connection"},{"location":"04_database_connection/#service-discovert-deploying-and-conneting-to-a-database","text":"In this section we gonna to connect databse to you r application Deployment on the database. Here, we are connect to mariaDB database. Go to overview In the overview section openshift shows you in right side Add to Project click on this and select databases and either you can use mysql or maraiaDB. MariaDB is just the fork of mysql database. After selecting the database provide some configuration of the databes like databae name, database connection, database password etc","title":"Service Discovert, Deploying and conneting to a database"},{"location":"2. kubernetes_deployment/","text":"Creating a Deployment A Deployment is a higher level abstraction which sits on top of replica sets and allows you to manage the way applications are deployed, rolled back at a controlled rate. Topics * Rollout a Replicaset * Deploy a new version : Creates a new replica set every time, moves pods from RS(n) to RS(n+1) * Rollback to previous RS * Auto Scaling * Pause Deployments File: vote-deploy.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: vote namespace: dev spec: replicas: 8 selector: matchLabels: tier: front app: vote matchExpressions: - {key: tier, operator: In, values: [front]} revisionHistoryLimit: 4 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 2 minReadySeconds: 20 paused: false template: metadata: labels: app: vote role: ui tier: front spec: containers: - image: schoolofdevops/vote imagePullPolicy: Always name: vote ports: - containerPort: 80 protocol: TCP Deployment spec (deployment.spec) contains the following, replicaset specs selectors replicas deployment spec strategy rollingUpdate minReadySeconds pod template metadata, labels container specs Lets create the Deployment oc apply -f vote_deploy.yaml --record Now that the deployment is created. To validate, oc get deployment oc get rs oc rollout status deployment/vote oc get pods --show-labels Sample Output oc get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE vote 3 3 3 1 3m Scaling a deployment To scale a deployment in Openshift: oc scale deployment/vote --replicas=5 Sample output: oc scale deployment/vote --replicas=5 deployment \"vote\" scaled","title":"Creating Deployments"},{"location":"2. kubernetes_deployment/#creating-a-deployment","text":"A Deployment is a higher level abstraction which sits on top of replica sets and allows you to manage the way applications are deployed, rolled back at a controlled rate. Topics * Rollout a Replicaset * Deploy a new version : Creates a new replica set every time, moves pods from RS(n) to RS(n+1) * Rollback to previous RS * Auto Scaling * Pause Deployments File: vote-deploy.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: vote namespace: dev spec: replicas: 8 selector: matchLabels: tier: front app: vote matchExpressions: - {key: tier, operator: In, values: [front]} revisionHistoryLimit: 4 strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 maxSurge: 2 minReadySeconds: 20 paused: false template: metadata: labels: app: vote role: ui tier: front spec: containers: - image: schoolofdevops/vote imagePullPolicy: Always name: vote ports: - containerPort: 80 protocol: TCP Deployment spec (deployment.spec) contains the following, replicaset specs selectors replicas deployment spec strategy rollingUpdate minReadySeconds pod template metadata, labels container specs Lets create the Deployment oc apply -f vote_deploy.yaml --record Now that the deployment is created. To validate, oc get deployment oc get rs oc rollout status deployment/vote oc get pods --show-labels Sample Output oc get deployments NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE vote 3 3 3 1 3m","title":"Creating a Deployment"},{"location":"2. kubernetes_deployment/#scaling-a-deployment","text":"To scale a deployment in Openshift: oc scale deployment/vote --replicas=5 Sample output: oc scale deployment/vote --replicas=5 deployment \"vote\" scaled","title":"Scaling a deployment"},{"location":"6. Kubernetes Autoscaling/","text":"Openshift Horizonntal Pod Autoscaling With Horizontal Pod Autoscaling, Openshift automatically scales the number of pods in a replication controller, deployment or replica set based on observed CPU utilization (or, with alpha support, on some other, application-provided metrics). The Horizontal Pod Autoscaler is implemented as a Openshift API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user Prerequisites Heapster monitoring needs to be deployed in the cluster as Horizontal Pod Autoscaler uses it to collect metrics. Deploying Heapster Go to the below directory and create the deployment and services. git clone https://github.com/Openshift/heapster.git cd heapster oc apply -f deploy/kube-config/influxdb/ oc apply -f deploy/kube-config/rbac/heapster-rbac.yaml Validate that heapster, influxdb and grafana are started oc get pods -n kube-system oc get svc -n kube-system Now this will deploy the heapster monitoring. Run & expose php-apache server To demonstrate Horizontal Pod Autoscaler we will use a custom docker image based on the php-apache image oc run php-apache --image=gcr.io/google_containers/hpa-example --requests=cpu=200m --expose --port=80 Sample Output oc run php-apache --image=gcr.io/google_containers/hpa-example --requests=cpu=200m --expose --port=80 service \"php-apache\" created deployment \"php-apache\" created To verify the created pod: oc get pods Wait untill the pod changes to running state. Create Horizontal Pod Autoscaler Now that the server is running, we will create the autoscaler using oc autoscale. The following command will create a Horizontal Pod Autoscaler that maintains between 1 and 10 replicas of the Pods controlled by the php-apache deployment we created in the first step of these instructions. oc autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10 Sample Output oc autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10 deployment \"php-apache\" autoscaled We may check the current status of autoscaler by running: oc get hpa Sample Output: oc get hpa NAME REFERENCE TARGET CURRENT MINPODS MAXPODS AGE php-apache Deployment/php-apache 50% 0% 1 10 18s Increase load Now we can increase the load and trying testing what will happen. We will start a container, and send an infinite loop of queries to the php-apache service oc run -i --tty -n dev load-generator --image=busybox /bin/sh Hit enter for command prompt while true; do wget -q -O- http://php-apache; done Now open a new window of the same machine. And check the status of the hpa oc get hpa Sample Output: oc get hpa NAME REFERENCE TARGET CURRENT MINPODS MAXPODS AGE php-apache Deployment/php-apache/scale 50% 305% 1 10 3m Now if you check the pods it will be automatically scaled to the desired value. oc get pods Sample Output oc get pods NAME READY STATUS RESTARTS AGE load-generator-1930141919-1pqn0 1/1 Running 0 1h php-apache-3815965786-2jmm9 1/1 Running 0 1h php-apache-3815965786-4f0ck 1/1 Running 0 1h php-apache-3815965786-73w24 1/1 Running 0 1h php-apache-3815965786-80n2x 1/1 Running 0 1h php-apache-3815965786-c6w0k 1/1 Running 0 1h php-apache-3815965786-f06dg 1/1 Running 0 1h php-apache-3815965786-nfs8d 1/1 Running 0 1h php-apache-3815965786-phrhs 1/1 Running 0 1h php-apache-3815965786-z6rnm 1/1 Running 0 1h Stop load In the terminal where we created the container with busybox image, terminate the load generation by typing + C Then we will verify the result state (after a minute or so) oc get hpa NAME REFERENCE TARGET CURRENT MINPODS MAXPODS AGE php-apache Deployment/php-apache/scale 50% 0% 1 10 11m $ oc get deployment php-apache NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE php-apache 1 1 1 1 27m","title":"Auto Scaling Capacity with HPA"},{"location":"6. Kubernetes Autoscaling/#openshift-horizonntal-pod-autoscaling","text":"With Horizontal Pod Autoscaling, Openshift automatically scales the number of pods in a replication controller, deployment or replica set based on observed CPU utilization (or, with alpha support, on some other, application-provided metrics). The Horizontal Pod Autoscaler is implemented as a Openshift API resource and a controller. The resource determines the behavior of the controller. The controller periodically adjusts the number of replicas in a replication controller or deployment to match the observed average CPU utilization to the target specified by user","title":"Openshift Horizonntal Pod Autoscaling"},{"location":"6. Kubernetes Autoscaling/#prerequisites","text":"Heapster monitoring needs to be deployed in the cluster as Horizontal Pod Autoscaler uses it to collect metrics.","title":"Prerequisites"},{"location":"6. Kubernetes Autoscaling/#deploying-heapster","text":"Go to the below directory and create the deployment and services. git clone https://github.com/Openshift/heapster.git cd heapster oc apply -f deploy/kube-config/influxdb/ oc apply -f deploy/kube-config/rbac/heapster-rbac.yaml Validate that heapster, influxdb and grafana are started oc get pods -n kube-system oc get svc -n kube-system Now this will deploy the heapster monitoring.","title":"Deploying Heapster"},{"location":"6. Kubernetes Autoscaling/#run-expose-php-apache-server","text":"To demonstrate Horizontal Pod Autoscaler we will use a custom docker image based on the php-apache image oc run php-apache --image=gcr.io/google_containers/hpa-example --requests=cpu=200m --expose --port=80 Sample Output oc run php-apache --image=gcr.io/google_containers/hpa-example --requests=cpu=200m --expose --port=80 service \"php-apache\" created deployment \"php-apache\" created To verify the created pod: oc get pods Wait untill the pod changes to running state.","title":"Run &amp; expose php-apache server"},{"location":"6. Kubernetes Autoscaling/#create-horizontal-pod-autoscaler","text":"Now that the server is running, we will create the autoscaler using oc autoscale. The following command will create a Horizontal Pod Autoscaler that maintains between 1 and 10 replicas of the Pods controlled by the php-apache deployment we created in the first step of these instructions. oc autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10 Sample Output oc autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10 deployment \"php-apache\" autoscaled We may check the current status of autoscaler by running: oc get hpa Sample Output: oc get hpa NAME REFERENCE TARGET CURRENT MINPODS MAXPODS AGE php-apache Deployment/php-apache 50% 0% 1 10 18s","title":"Create Horizontal Pod Autoscaler"},{"location":"6. Kubernetes Autoscaling/#increase-load","text":"Now we can increase the load and trying testing what will happen. We will start a container, and send an infinite loop of queries to the php-apache service oc run -i --tty -n dev load-generator --image=busybox /bin/sh Hit enter for command prompt while true; do wget -q -O- http://php-apache; done Now open a new window of the same machine. And check the status of the hpa oc get hpa Sample Output: oc get hpa NAME REFERENCE TARGET CURRENT MINPODS MAXPODS AGE php-apache Deployment/php-apache/scale 50% 305% 1 10 3m Now if you check the pods it will be automatically scaled to the desired value. oc get pods Sample Output oc get pods NAME READY STATUS RESTARTS AGE load-generator-1930141919-1pqn0 1/1 Running 0 1h php-apache-3815965786-2jmm9 1/1 Running 0 1h php-apache-3815965786-4f0ck 1/1 Running 0 1h php-apache-3815965786-73w24 1/1 Running 0 1h php-apache-3815965786-80n2x 1/1 Running 0 1h php-apache-3815965786-c6w0k 1/1 Running 0 1h php-apache-3815965786-f06dg 1/1 Running 0 1h php-apache-3815965786-nfs8d 1/1 Running 0 1h php-apache-3815965786-phrhs 1/1 Running 0 1h php-apache-3815965786-z6rnm 1/1 Running 0 1h","title":"Increase load"},{"location":"6. Kubernetes Autoscaling/#stop-load","text":"In the terminal where we created the container with busybox image, terminate the load generation by typing + C Then we will verify the result state (after a minute or so) oc get hpa NAME REFERENCE TARGET CURRENT MINPODS MAXPODS AGE php-apache Deployment/php-apache/scale 50% 0% 1 10 11m $ oc get deployment php-apache NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE php-apache 1 1 1 1 27m","title":"Stop load"},{"location":"6. deploying_sample_app/","text":"Mini Project: Deploying Multi Tier Application Stack In this project , you would write definitions for deploying the vote application stack with all components/tiers which include, vote ui redis worker db results ui Tasks Create deployments for all applications Define services for each tier Launch/appy the definitions Following table depicts the state of readiness of the above services. App Deployment Service vote ready ready redis in progress ready worker in progress in progress db in progress todo results todo todo Deploying the sample application To create deploy the sample applications, oc create -f apps/voting/dev Sample output is like: deployment \"db\" created service \"db\" created deployment \"redis\" created service \"redis\" created deployment \"vote\" created service \"vote\" created deployment \"worker\" created deployment \"results\" created service \"results\" created To Validatecheck it: oc get svc -n dev Sample Output is: oc get service voting-app NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE vote 10.97.104.243 <pending> 80:31808/TCP 1h Here the port assigned is 31808, go to the browser and enter masterip:31808 This will load the page where you can vote. To check the result: oc get service result Sample Output is: oc get service result NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE result 10.101.112.16 <pending> 80:32511/TCP 1h Here the port assigned is 32511, go to the browser and enter masterip:32511 This is the page where we can see the results of the vote.","title":"Mini Project"},{"location":"6. deploying_sample_app/#mini-project-deploying-multi-tier-application-stack","text":"In this project , you would write definitions for deploying the vote application stack with all components/tiers which include, vote ui redis worker db results ui","title":"Mini Project: Deploying Multi Tier Application Stack"},{"location":"6. deploying_sample_app/#tasks","text":"Create deployments for all applications Define services for each tier Launch/appy the definitions Following table depicts the state of readiness of the above services. App Deployment Service vote ready ready redis in progress ready worker in progress in progress db in progress todo results todo todo","title":"Tasks"},{"location":"6. deploying_sample_app/#deploying-the-sample-application","text":"To create deploy the sample applications, oc create -f apps/voting/dev Sample output is like: deployment \"db\" created service \"db\" created deployment \"redis\" created service \"redis\" created deployment \"vote\" created service \"vote\" created deployment \"worker\" created deployment \"results\" created service \"results\" created","title":"Deploying the sample application"},{"location":"6. deploying_sample_app/#to-validatecheck-it","text":"oc get svc -n dev Sample Output is: oc get service voting-app NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE vote 10.97.104.243 <pending> 80:31808/TCP 1h Here the port assigned is 31808, go to the browser and enter masterip:31808 This will load the page where you can vote. To check the result: oc get service result Sample Output is: oc get service result NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE result 10.101.112.16 <pending> 80:32511/TCP 1h Here the port assigned is 32511, go to the browser and enter masterip:32511 This is the page where we can see the results of the vote.","title":"To Validatecheck it:"},{"location":"8. Using Configmaps and Secrets/","text":"Configmap is one of the ways to provide configurations to your application. Injecting env variables with configmaps Create our configmap for vote app file: apps/voting/dev/vote-cm.yaml apiVersion: v1 kind: ConfigMap metadata: name: vote namespace: dev data: OPTION_A: EMACS OPTION_B: VI In the above given configmap, we define two environment variables, 1. OPTION_A=EMACS 2. OPTION_B=VI In order to use this configmap in the deployment, we need to reference it from the deployment file. Check the deployment file for vote add for the following block. file: vote-deploy.yaml ... spec: containers: - image: schoolofdevops/vote imagePullPolicy: Always name: vote envFrom: - configMapRef: name: vote ports: - containerPort: 80 protocol: TCP restartPolicy: Always So when you create your deployment, these configurations will be made available to your application. In this example, the values defined in the configmap (EMACS and VI) will override the default values(CATS and DOGS) present in your source code. Configmap as a configuration file In the topic above we have seen how to use configmap as environment variables. Now let us see how to use configmap as redis configuration file. Syntax for consuming file as a configmap is as follows oc create configmap --from-file <CONF-FILE-PATH> <NAME-OF-CONFIGMAP> We have redis configuration as a file named apps/voting/config/redis.conf . We are going to convert this file into a configmap oc create configmap --from-file apps/voting/config/redis.conf redis Update your redis-deploy.yaml file to use this confimap. File: redis-deploy.yaml spec: containers: - image: schoolofdevops/redis:latest imagePullPolicy: Always name: redis ports: - containerPort: 6379 protocol: TCP volumeMounts: - name: redis subPath: redis.conf mountPath: /etc/redis.conf volumes: - name: redis configMap: name: redis restartPolicy: Always Secrets Secrets are for storing sensitive data like passwords and keychains . We will see how db deployment uses username and password in form of a secret. You would define two fields for db, * username * password To create secrets for db you need to generate base64 format as follows, echo \"admin\" | base64 echo \"password\" | base64 where admin and password are the actual values that you would want to inject into the pod environment. If you do not have a unix host, you can make use of online base64 utility to generate these strings. http://www.utilities-online.info/base64 Lets now add it to the secrets file, File: apps/voting/dev/db-secrets.yaml apiVersion: v1 kind: Secret metadata: name: db namespace: dev type: Opaque data: POSTGRES_USER: YWRtaW4= # base64 of admin POSTGRES_PASSWD: cGFzc3dvcmQ= # base64 of password To consume these secrets, update the deployment as file: db-deploy.yaml. apiVersion: extensions/v1beta1 kind: Deployment metadata: name: db namespace: dev spec: replicas: 1 selector: matchLabels: tier: back app: postgres minReadySeconds: 10 template: metadata: labels: app: postgres role: db tier: back spec: containers: - image: postgres:9.4 imagePullPolicy: Always name: db ports: - containerPort: 5432 protocol: TCP # Secret definition env: - name: POSTGRES_USER valueFrom: secretKeyRef: name: db key: POSTGRES_USER - name: POSTGRES_PASSWD valueFrom: secretKeyRef: name: db key: POSTGRES_PASSWD restartPolicy: Always","title":"Using Configmaps and Secrets"},{"location":"8. Using Configmaps and Secrets/#injecting-env-variables-with-configmaps","text":"Create our configmap for vote app file: apps/voting/dev/vote-cm.yaml apiVersion: v1 kind: ConfigMap metadata: name: vote namespace: dev data: OPTION_A: EMACS OPTION_B: VI In the above given configmap, we define two environment variables, 1. OPTION_A=EMACS 2. OPTION_B=VI In order to use this configmap in the deployment, we need to reference it from the deployment file. Check the deployment file for vote add for the following block. file: vote-deploy.yaml ... spec: containers: - image: schoolofdevops/vote imagePullPolicy: Always name: vote envFrom: - configMapRef: name: vote ports: - containerPort: 80 protocol: TCP restartPolicy: Always So when you create your deployment, these configurations will be made available to your application. In this example, the values defined in the configmap (EMACS and VI) will override the default values(CATS and DOGS) present in your source code.","title":"Injecting env variables with configmaps"},{"location":"8. Using Configmaps and Secrets/#configmap-as-a-configuration-file","text":"In the topic above we have seen how to use configmap as environment variables. Now let us see how to use configmap as redis configuration file. Syntax for consuming file as a configmap is as follows oc create configmap --from-file <CONF-FILE-PATH> <NAME-OF-CONFIGMAP> We have redis configuration as a file named apps/voting/config/redis.conf . We are going to convert this file into a configmap oc create configmap --from-file apps/voting/config/redis.conf redis Update your redis-deploy.yaml file to use this confimap. File: redis-deploy.yaml spec: containers: - image: schoolofdevops/redis:latest imagePullPolicy: Always name: redis ports: - containerPort: 6379 protocol: TCP volumeMounts: - name: redis subPath: redis.conf mountPath: /etc/redis.conf volumes: - name: redis configMap: name: redis restartPolicy: Always","title":"Configmap as a configuration file"},{"location":"8. Using Configmaps and Secrets/#secrets","text":"Secrets are for storing sensitive data like passwords and keychains . We will see how db deployment uses username and password in form of a secret. You would define two fields for db, * username * password To create secrets for db you need to generate base64 format as follows, echo \"admin\" | base64 echo \"password\" | base64 where admin and password are the actual values that you would want to inject into the pod environment. If you do not have a unix host, you can make use of online base64 utility to generate these strings. http://www.utilities-online.info/base64 Lets now add it to the secrets file, File: apps/voting/dev/db-secrets.yaml apiVersion: v1 kind: Secret metadata: name: db namespace: dev type: Opaque data: POSTGRES_USER: YWRtaW4= # base64 of admin POSTGRES_PASSWD: cGFzc3dvcmQ= # base64 of password To consume these secrets, update the deployment as file: db-deploy.yaml. apiVersion: extensions/v1beta1 kind: Deployment metadata: name: db namespace: dev spec: replicas: 1 selector: matchLabels: tier: back app: postgres minReadySeconds: 10 template: metadata: labels: app: postgres role: db tier: back spec: containers: - image: postgres:9.4 imagePullPolicy: Always name: db ports: - containerPort: 5432 protocol: TCP # Secret definition env: - name: POSTGRES_USER valueFrom: secretKeyRef: name: db key: POSTGRES_USER - name: POSTGRES_PASSWD valueFrom: secretKeyRef: name: db key: POSTGRES_PASSWD restartPolicy: Always","title":"Secrets"},{"location":"Deploying_Vote&Redis_withImages/","text":"Deploying Vote and Redis with Images - Images, Image Streams, Router Configs In this section we are talking about deploying the vote app with database redis using already build images. We are also talking about image Streams and Router config in the openshift. Setting openshift on a remote Fedora server in this section we are talking about how to set up openshift environment in fedora operating system System Requirements Physical or virtual system, or an instance running on a public or private IaaS. Base OS Docker 2 vCPU Minimum 8 GB RAM Minimum 15 GB hard disk space An additional minimum 15 GB unallocated space to be configured using docker-storage-setup use this link for installation documents. In this case we need to install docker use the following script for docker installation for fedora. #!/bin/bash sudo dnf remove -y docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine sudo dnf -y install dnf-plugins-core sudo dnf config-manager \\ --add-repo \\ https://download.docker.com/linux/fedora/docker-ce.repo sudo dnf install -y docker-ce sudo systemctl start docker I: \"Validating dokcer installation...\" sudo docker run hello-world edit the /etc/docker/daemon.json file and add the following cat > /ete/docker/daemon.json { \"insecure-registries\": [ \"172.30.0.0/16\" ] } After editing the config, reload systemd and restart the Docker daemon. sudo systemctl daemon-reload sudo systemctl restart docker Determine the Docker bridge network container subnet: docker network inspect -f \"{{range .IPAM.Config }}{{ .Subnet }}{{end}}\" bridge then download the openshift wget -c https://github.com/openshift/origin/releases/download/v3.10.0/openshift-origin-client-tools-v3.10.0-dd10d17-linux-64bit.tar.gz tar -xvf openshift-origin-client-tools-v3.10.0-dd10d17-linux-64bit.tar.gz cd openshift-origin-client-tools-v3.10.0-dd10d17-linux-64bit/ mv oc /usr/local/bin/ which oc [output] /usr/local/bin/oc oc cluster up --public-hostname=[host_ip] Deploying with image In this section we are talking about deploying app using existing image. just check the openshift status. oc ststus [output] In project demo on server https://128.199.213.193:8443 http://demoapp-demo.128.199.213.193.nip.io to pod port 8080-tcp (svc/demoapp) dc/demoapp deploys istag/demoapp:latest <- bc/demoapp source builds https://github.com/devopsdemoapps/devops-demo-app.git#master on openshift/php:7.1 deployment #2 deployed 2 days ago - 5 pods deployment #1 deployed 2 days ago svc/mariadb - 172.30.193.105:3306 dc/mariadb deploys openshift/mysql:5.7 deployment #1 failed 2 days ago: config change 3 infos identified, use 'oc status -v' to see details. create the project using following command. oc new-project instavote --display-name=\"instavote app\" --description=\"Sample voting application\" [output] Now using project \"instavote\" on server \"https://128.199.213.193:8443\". You can add applications to this project with the 'new-app' command. For example, try: oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git to build a new example application in Ruby. then check the projects. oc get projects [output] NAME DISPLAY NAME STATUS demo demo Active instavote instavote app Active myproject My Project Active deploy the vote application using existing image form docker hub registries. initcron/oc-vote:v1 click on your created project instavote then click on overview and click on deploy with image. provide some information on this page here , Provide name, environment variables and labels. then click on deploy. Exposing vote app with Router In this section we are talking about how to expose application with router. just check pods, replicas and deployment config. oc get pods,rc,dc [output] NAME READY STATUS RESTARTS AGE pod/oc-vote-2-7ckd5 1/1 Running 0 32s pod/oc-vote-2-jwp45 1/1 Running 0 9m pod/oc-vote-2-qkdrc 1/1 Running 0 33s pod/oc-vote-2-rlp25 1/1 Running 0 32s NAME DESIRED CURRENT READY AGE replicationcontroller/oc-vote-1 0 0 0 10m replicationcontroller/oc-vote-2 4 4 4 9m NAME REVISION DESIRED CURRENT TRIGGERED BY deploymentconfig.apps.openshift.io/oc-vote 2 4 4 config,image(oc-vote:v1) oc get svc [output] NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE oc-vote ClusterIP 172.30.206.1 <none> 80/TCP,8080/TCP 12m get endpoints using following command. oc get endpoints [output] NAME ENDPOINTS AGE oc-vote 172.17.0.11:80,172.17.0.12:80,172.17.0.13:80 + 5 more... 14m get image stream using following command. oc get is [output] NAME DOCKER REPO TAGS UPDATED oc-vote 172.30.1.1:5000/instavote/oc-vote v1 16 minutes ago oc describe is/oc-vote Name: oc-vote Namespace: instavote Created: 17 minutes ago Labels: app=oc-vote role=vote Annotations: openshift.io/image.dockerRepositoryCheck=2018-10-28T07:12:40Z Docker Pull Spec: 172.30.1.1:5000/instavote/oc-vote Image Lookup: local=false Unique Images: 1 Tags: 1 v1 tagged from initcron/oc-vote:v1 * initcron/oc-vote@sha256:b05f64d225a8671ecc09d22760a2c76bd65aaf50e12d00b31ab3b4fe6f377b06 17 minutes ago go to your openshift UI and click on application->routes. fill all the information Introduction to oc new-app command In this section we are talking about to oc new-app command. oc get all -l app=oc-vote [output] NAME READY STATUS RESTARTS AGE pod/oc-vote-2-7ckd5 1/1 Running 0 24m pod/oc-vote-2-jwp45 1/1 Running 0 33m pod/oc-vote-2-qkdrc 1/1 Running 0 24m pod/oc-vote-2-rlp25 1/1 Running 0 24m NAME DESIRED CURRENT READY AGE replicationcontroller/oc-vote-1 0 0 0 34m replicationcontroller/oc-vote-2 4 4 4 33m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/oc-vote ClusterIP 172.30.206.1 <none> 80/TCP,8080/TCP 34m NAME REVISION DESIRED CURRENT TRIGGERED BY deploymentconfig.apps.openshift.io/oc-vote 2 4 4 config,image(oc-vote:v1) NAME DOCKER REPO TAGS UPDATED imagestream.image.openshift.io/oc-vote 172.30.1.1:5000/instavote/oc-vote v1 34 minutes ago NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD route.route.openshift.io/oc-vote oc-vote-instavote.128.199.213.193.nip.io oc-vote 8080-tcp edge/Redirect None oc delete all -l app=oc-vote [output] pod \"oc-vote-2-7ckd5\" deleted pod \"oc-vote-2-jwp45\" deleted pod \"oc-vote-2-qkdrc\" deleted pod \"oc-vote-2-rlp25\" deleted replicationcontroller \"oc-vote-1\" deleted replicationcontroller \"oc-vote-2\" deleted service \"oc-vote\" deleted deploymentconfig.apps.openshift.io \"oc-vote\" deleted imagestream.image.openshift.io \"oc-vote\" deleted route.route.openshift.io \"oc-vote\" deleted oc new-app --list search template oc new-app --search --template=ruby --output=yaml [output] apiVersion: v1 items: [] kind: List metadata: {} Setup vote app with using oc cli utility In this section we are redeploying vote application using oc cli. oc new-app --docker-image=initcron/oc-vote:v1 --name=vote [output] --> Found Docker image c97dc33 (2 months old) from Docker Hub for \"initcron/oc-vote:v1\" * An image stream will be created as \"vote:v1\" that will track this image * This image will be deployed in deployment config \"vote\" * Ports 80/tcp, 8080/tcp will be load balanced by service \"vote\" * Other containers can access this service through the hostname \"vote\" --> Creating resources ... imagestream \"vote\" created deploymentconfig \"vote\" created service \"vote\" created --> Success Application is not exposed. You can expose services to the outside world by executing one or more of the commands below: 'oc expose svc/vote' Run 'oc status' to view your app. oc status [output] In project instavote app (instavote) on server https://128.199.213.193:8443 svc/vote - 172.30.83.46 ports 80, 8080 dc/vote deploys istag/vote:v1 deployment #1 deployed about a minute ago - 1 pod 2 infos identified, use 'oc status -v' to see details. oc expose svc/vote [output] route \"vote\" exposed oc status [output] In project instavote app (instavote) on server https://128.199.213.193:8443 http://vote-instavote.128.199.213.193.nip.io to pod port 80-tcp (svc/vote) dc/vote deploys istag/vote:v1 deployment #1 deployed 6 minutes ago - 1 pod 2 infos identified, use 'oc status -v' to see details. oc get routes [output] NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD vote vote-instavote.128.199.213.193.nip.io vote 80-tcp None oc delete route/vote [output] route.route.openshift.io \"vote\" deleted oc get svc [output] NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE vote ClusterIP 172.30.83.46 <none> 80/TCP,8080/TCP 11m oc expose service vote --port=8080 [output] route \"vote\" exposed Using annotations to change Router configurations In this section we are talking about how to change Router configurations. oc scale --replicas=4 dc/vote [output] deploymentconfig.apps.openshift.io \"vote\" scaled we need to edit add following line haproxy.router.openshift.io/disble_cookies: 'true' Deploying Redis and connecting it woth the vote frontend In this section we are talking about database connectivity of our vote frontend application. We have already deployed voted application. we will connect the database that application. we will use redis as backend. use the following command to redis deployment. oc new-app --name redis --docker-image=redis:alpine [output] --> Found Docker image 05635ee (9 days old) from Docker Hub for \"redis:alpine\" * An image stream will be created as \"redis:alpine\" that will track this image * This image will be deployed in deployment config \"redis\" * Port 6379/tcp will be load balanced by service \"redis\" * Other containers can access this service through the hostname \"redis\" * This image declares volumes and will default to use non-persistent, host-local storage. You can add persistent volumes later by running 'volume dc/redis --add ...' * WARNING: Image \"redis:alpine\" runs as the 'root' user which may not be permitted by your cluster administrator --> Creating resources ... imagestream \"redis\" created deploymentconfig \"redis\" created service \"redis\" created --> Success Application is not exposed. You can expose services to the outside world by executing one or more of the commands below: 'oc expose svc/redis' Run 'oc status' to view your app. Update environment vars and images with oc set In this section we talking about how update environment variables using oc cli. We have already deployed vote application we will change env for vote application. we use the following command for that. oc set env dc/vote OPTION_A=myoption [output] deploymentconfig \"vote\" updated oc get is oc get istag update version of application. oc set image dc/vote vote=initcron/oc-vote:v2 [output] deploymentconfig \"vote\" image updated","title":"Deploying Vote&Redis withImages"},{"location":"Deploying_Vote&Redis_withImages/#deploying-vote-and-redis-with-images-images-image-streams-router-configs","text":"In this section we are talking about deploying the vote app with database redis using already build images. We are also talking about image Streams and Router config in the openshift.","title":"Deploying Vote and Redis with Images - Images, Image Streams, Router Configs"},{"location":"Deploying_Vote&Redis_withImages/#setting-openshift-on-a-remote-fedora-server","text":"in this section we are talking about how to set up openshift environment in fedora operating system","title":"Setting openshift on a remote Fedora server"},{"location":"Deploying_Vote&Redis_withImages/#system-requirements","text":"Physical or virtual system, or an instance running on a public or private IaaS. Base OS Docker 2 vCPU Minimum 8 GB RAM Minimum 15 GB hard disk space An additional minimum 15 GB unallocated space to be configured using docker-storage-setup use this link for installation documents. In this case we need to install docker use the following script for docker installation for fedora. #!/bin/bash sudo dnf remove -y docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine sudo dnf -y install dnf-plugins-core sudo dnf config-manager \\ --add-repo \\ https://download.docker.com/linux/fedora/docker-ce.repo sudo dnf install -y docker-ce sudo systemctl start docker I: \"Validating dokcer installation...\" sudo docker run hello-world edit the /etc/docker/daemon.json file and add the following cat > /ete/docker/daemon.json { \"insecure-registries\": [ \"172.30.0.0/16\" ] } After editing the config, reload systemd and restart the Docker daemon. sudo systemctl daemon-reload sudo systemctl restart docker Determine the Docker bridge network container subnet: docker network inspect -f \"{{range .IPAM.Config }}{{ .Subnet }}{{end}}\" bridge then download the openshift wget -c https://github.com/openshift/origin/releases/download/v3.10.0/openshift-origin-client-tools-v3.10.0-dd10d17-linux-64bit.tar.gz tar -xvf openshift-origin-client-tools-v3.10.0-dd10d17-linux-64bit.tar.gz cd openshift-origin-client-tools-v3.10.0-dd10d17-linux-64bit/ mv oc /usr/local/bin/ which oc [output] /usr/local/bin/oc oc cluster up --public-hostname=[host_ip]","title":"System Requirements"},{"location":"Deploying_Vote&Redis_withImages/#deploying-with-image","text":"In this section we are talking about deploying app using existing image. just check the openshift status. oc ststus [output] In project demo on server https://128.199.213.193:8443 http://demoapp-demo.128.199.213.193.nip.io to pod port 8080-tcp (svc/demoapp) dc/demoapp deploys istag/demoapp:latest <- bc/demoapp source builds https://github.com/devopsdemoapps/devops-demo-app.git#master on openshift/php:7.1 deployment #2 deployed 2 days ago - 5 pods deployment #1 deployed 2 days ago svc/mariadb - 172.30.193.105:3306 dc/mariadb deploys openshift/mysql:5.7 deployment #1 failed 2 days ago: config change 3 infos identified, use 'oc status -v' to see details. create the project using following command. oc new-project instavote --display-name=\"instavote app\" --description=\"Sample voting application\" [output] Now using project \"instavote\" on server \"https://128.199.213.193:8443\". You can add applications to this project with the 'new-app' command. For example, try: oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git to build a new example application in Ruby. then check the projects. oc get projects [output] NAME DISPLAY NAME STATUS demo demo Active instavote instavote app Active myproject My Project Active deploy the vote application using existing image form docker hub registries. initcron/oc-vote:v1 click on your created project instavote then click on overview and click on deploy with image. provide some information on this page here , Provide name, environment variables and labels. then click on deploy.","title":"Deploying with image"},{"location":"Deploying_Vote&Redis_withImages/#exposing-vote-app-with-router","text":"In this section we are talking about how to expose application with router. just check pods, replicas and deployment config. oc get pods,rc,dc [output] NAME READY STATUS RESTARTS AGE pod/oc-vote-2-7ckd5 1/1 Running 0 32s pod/oc-vote-2-jwp45 1/1 Running 0 9m pod/oc-vote-2-qkdrc 1/1 Running 0 33s pod/oc-vote-2-rlp25 1/1 Running 0 32s NAME DESIRED CURRENT READY AGE replicationcontroller/oc-vote-1 0 0 0 10m replicationcontroller/oc-vote-2 4 4 4 9m NAME REVISION DESIRED CURRENT TRIGGERED BY deploymentconfig.apps.openshift.io/oc-vote 2 4 4 config,image(oc-vote:v1) oc get svc [output] NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE oc-vote ClusterIP 172.30.206.1 <none> 80/TCP,8080/TCP 12m get endpoints using following command. oc get endpoints [output] NAME ENDPOINTS AGE oc-vote 172.17.0.11:80,172.17.0.12:80,172.17.0.13:80 + 5 more... 14m get image stream using following command. oc get is [output] NAME DOCKER REPO TAGS UPDATED oc-vote 172.30.1.1:5000/instavote/oc-vote v1 16 minutes ago oc describe is/oc-vote Name: oc-vote Namespace: instavote Created: 17 minutes ago Labels: app=oc-vote role=vote Annotations: openshift.io/image.dockerRepositoryCheck=2018-10-28T07:12:40Z Docker Pull Spec: 172.30.1.1:5000/instavote/oc-vote Image Lookup: local=false Unique Images: 1 Tags: 1 v1 tagged from initcron/oc-vote:v1 * initcron/oc-vote@sha256:b05f64d225a8671ecc09d22760a2c76bd65aaf50e12d00b31ab3b4fe6f377b06 17 minutes ago go to your openshift UI and click on application->routes. fill all the information","title":"Exposing vote app with Router"},{"location":"Deploying_Vote&Redis_withImages/#introduction-to-oc-new-app-command","text":"In this section we are talking about to oc new-app command. oc get all -l app=oc-vote [output] NAME READY STATUS RESTARTS AGE pod/oc-vote-2-7ckd5 1/1 Running 0 24m pod/oc-vote-2-jwp45 1/1 Running 0 33m pod/oc-vote-2-qkdrc 1/1 Running 0 24m pod/oc-vote-2-rlp25 1/1 Running 0 24m NAME DESIRED CURRENT READY AGE replicationcontroller/oc-vote-1 0 0 0 34m replicationcontroller/oc-vote-2 4 4 4 33m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/oc-vote ClusterIP 172.30.206.1 <none> 80/TCP,8080/TCP 34m NAME REVISION DESIRED CURRENT TRIGGERED BY deploymentconfig.apps.openshift.io/oc-vote 2 4 4 config,image(oc-vote:v1) NAME DOCKER REPO TAGS UPDATED imagestream.image.openshift.io/oc-vote 172.30.1.1:5000/instavote/oc-vote v1 34 minutes ago NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD route.route.openshift.io/oc-vote oc-vote-instavote.128.199.213.193.nip.io oc-vote 8080-tcp edge/Redirect None oc delete all -l app=oc-vote [output] pod \"oc-vote-2-7ckd5\" deleted pod \"oc-vote-2-jwp45\" deleted pod \"oc-vote-2-qkdrc\" deleted pod \"oc-vote-2-rlp25\" deleted replicationcontroller \"oc-vote-1\" deleted replicationcontroller \"oc-vote-2\" deleted service \"oc-vote\" deleted deploymentconfig.apps.openshift.io \"oc-vote\" deleted imagestream.image.openshift.io \"oc-vote\" deleted route.route.openshift.io \"oc-vote\" deleted oc new-app --list search template oc new-app --search --template=ruby --output=yaml [output] apiVersion: v1 items: [] kind: List metadata: {}","title":"Introduction to oc new-app command"},{"location":"Deploying_Vote&Redis_withImages/#setup-vote-app-with-using-oc-cli-utility","text":"In this section we are redeploying vote application using oc cli. oc new-app --docker-image=initcron/oc-vote:v1 --name=vote [output] --> Found Docker image c97dc33 (2 months old) from Docker Hub for \"initcron/oc-vote:v1\" * An image stream will be created as \"vote:v1\" that will track this image * This image will be deployed in deployment config \"vote\" * Ports 80/tcp, 8080/tcp will be load balanced by service \"vote\" * Other containers can access this service through the hostname \"vote\" --> Creating resources ... imagestream \"vote\" created deploymentconfig \"vote\" created service \"vote\" created --> Success Application is not exposed. You can expose services to the outside world by executing one or more of the commands below: 'oc expose svc/vote' Run 'oc status' to view your app. oc status [output] In project instavote app (instavote) on server https://128.199.213.193:8443 svc/vote - 172.30.83.46 ports 80, 8080 dc/vote deploys istag/vote:v1 deployment #1 deployed about a minute ago - 1 pod 2 infos identified, use 'oc status -v' to see details. oc expose svc/vote [output] route \"vote\" exposed oc status [output] In project instavote app (instavote) on server https://128.199.213.193:8443 http://vote-instavote.128.199.213.193.nip.io to pod port 80-tcp (svc/vote) dc/vote deploys istag/vote:v1 deployment #1 deployed 6 minutes ago - 1 pod 2 infos identified, use 'oc status -v' to see details. oc get routes [output] NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD vote vote-instavote.128.199.213.193.nip.io vote 80-tcp None oc delete route/vote [output] route.route.openshift.io \"vote\" deleted oc get svc [output] NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE vote ClusterIP 172.30.83.46 <none> 80/TCP,8080/TCP 11m oc expose service vote --port=8080 [output] route \"vote\" exposed","title":"Setup vote app with using oc cli utility"},{"location":"Deploying_Vote&Redis_withImages/#using-annotations-to-change-router-configurations","text":"In this section we are talking about how to change Router configurations. oc scale --replicas=4 dc/vote [output] deploymentconfig.apps.openshift.io \"vote\" scaled we need to edit add following line haproxy.router.openshift.io/disble_cookies: 'true'","title":"Using annotations to change Router configurations"},{"location":"Deploying_Vote&Redis_withImages/#deploying-redis-and-connecting-it-woth-the-vote-frontend","text":"In this section we are talking about database connectivity of our vote frontend application. We have already deployed voted application. we will connect the database that application. we will use redis as backend. use the following command to redis deployment. oc new-app --name redis --docker-image=redis:alpine [output] --> Found Docker image 05635ee (9 days old) from Docker Hub for \"redis:alpine\" * An image stream will be created as \"redis:alpine\" that will track this image * This image will be deployed in deployment config \"redis\" * Port 6379/tcp will be load balanced by service \"redis\" * Other containers can access this service through the hostname \"redis\" * This image declares volumes and will default to use non-persistent, host-local storage. You can add persistent volumes later by running 'volume dc/redis --add ...' * WARNING: Image \"redis:alpine\" runs as the 'root' user which may not be permitted by your cluster administrator --> Creating resources ... imagestream \"redis\" created deploymentconfig \"redis\" created service \"redis\" created --> Success Application is not exposed. You can expose services to the outside world by executing one or more of the commands below: 'oc expose svc/redis' Run 'oc status' to view your app.","title":"Deploying Redis and connecting it woth the vote frontend"},{"location":"Deploying_Vote&Redis_withImages/#update-environment-vars-and-images-with-oc-set","text":"In this section we talking about how update environment variables using oc cli. We have already deployed vote application we will change env for vote application. we use the following command for that. oc set env dc/vote OPTION_A=myoption [output] deploymentconfig \"vote\" updated oc get is oc get istag update version of application. oc set image dc/vote vote=initcron/oc-vote:v2 [output] deploymentconfig \"vote\" image updated","title":"Update environment vars and images with oc set"},{"location":"Just_docker_for_openshift/","text":"Just Enough Docker for a Openshift Practitioner Setting up and validating docker environment In this chapter, we are going to set docker environment. Visit docs.docker.com this page provides all the information of how to install docker on ubuntu, mac or windows. In this page left side you can see couple of options . when you select docker CE (Docker Community Edition). Thre is also provides instructions on different os platform. There are two options docker EE docker CE Here , we going for docker for ubuntu you can use the following script. #!/bin/bash apt-get update apt-get install -y git wget # Install Docker apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - apt-key fingerprint 0EBFCD88 add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" apt-get update apt-get install -yq docker-ce After install docker using above script you can validate using following command docker version [output] Client: Version: 18.03.1-ce API version: 1.37 Go version: go1.9.5 Git commit: 9ee9f40 Built: Thu Apr 26 07:17:20 2018 OS/Arch: linux/amd64 Experimental: false Orchestrator: swarm Server: Engine: Version: 18.03.1-ce API version: 1.37 (minimum version 1.12) Go version: go1.9.5 Git commit: 9ee9f40 Built: Thu Apr 26 07:15:30 2018 OS/Arch: linux/amd64 Experimental: false Run docker Hello-world sudo docker run hello-world [output] Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world d1725b59e92d: Pull complete Digest: sha256:0add3ace90ecb4adbf7777e9aacf18357296e799f81cabc9fde470971e499788 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ Play-with-docker If you don't have way to install docker locally even remote server. you can also use protal play-with-docker Here, we have to login with your docker hub id and password. if you don't have docker hub id and password you should need to create your docker hub id and password docker hub . Play-with-docker gives you working docker environment. you can see the UI Here, In this Page left side create instance label click on on that label they provide you docker envirenment. you can also ren the following command. docker version command shows you to all the information of docker like version, api version Git commit etc. docker version [output] Client: Version: 18.03.1-ce API version: 1.37 Go version: go1.9.5 Git commit: 9ee9f40 Built: Thu Apr 26 07:17:20 2018 OS/Arch: linux/amd64 Experimental: false Orchestrator: swarm Server: Engine: Version: 18.03.1-ce API version: 1.37 (minimum version 1.12) Go version: go1.9.5 Git commit: 9ee9f40 Built: Thu Apr 26 07:15:30 2018 OS/Arch: linux/amd64 Experimental: false Docker hello world image docker run hello-world [output] Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world d1725b59e92d: Pull complete Digest: sha256:0add3ace90ecb4adbf7777e9aacf18357296e799f81cabc9fde470971e499788 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ Running your first container when your enviromnment ready you can start your first docker container . when you running your forst docker container you need docker registry. Go to docker public registry . when you vist docker hub you can see bunch of images avaible here. we are going to pick basic os image alpine . alpine is distribution of linux, ubuntu etc why we are choosing this image because of footprint of image and you can look at the size of the image look like 2 0r 3 mb that is relly good for smoke testing and running samaller images. Now we have a basic understanding of docker command and sub commands, let us dive straight into launching our very first container docker container run alpine:3.6 uptime Where, we are using docker client to run a application/command uptime using an image by name alpine [output] Unable to find image 'alpine:3.6' locally 3.6: Pulling from library/alpine 117f30b7ae3d: Pull complete Digest: sha256:02eb5cfe4b721495135728ab4aea87418fd4edbfbf83612130a81191f0b2aae3 Status: Downloaded newer image for alpine:3.6 07:45:40 up 3:13, load average: 0.00, 0.00, 0.00 What happened? This command will Pull the alpine image file from docker hub, a cloud registry Create a runtime environment/ container with the above image Launch a program (called uptime) inside that container Stream that output to the terminal Stop the container once the program is exited Let's see what happens when we run that command again, [Output] docker run alpine uptime 07:48:06 up 3:15, load average: 0.00, 0.00, 0.00 Making container persist with -idt options We can interact with docker containers by giving -it flags at the run time. These flags stand for i - Interactive t - tty d - detach docker container run -it alpine:3.4 sh [ouput] Unable to find image 'alpine:3.4' locally 3.4: Pulling from library/alpine e110a4a17941: Pull complete Digest: sha256:3dcdb92d7432d56604d4545cbd324b14e647b313626d99b889d0626de158f73a Status: Downloaded newer image for alpine:3.4 / # As you see, we have landed straight into sh shell of that container. This is the result of using -it flags and mentioning that container to run the sh shell. Don't try to exit that container yet. We have to execute some other commands in it to understand the next topic if you go inside the container Namespaced Like a full fledged OS, Docker container has its own namespaces This enables Docker container to isolate itself from the host as well as other containers Run the following commands and see that alpine container has its own namespaces and not inheriting much from host OS cat /etc/issue Welcome to Alpine Linux 3.4 Kernel \\r on an \\m (\\l) / # command pa aux [output] PID USER TIME COMMAND 1 root 0:00 sh 7 root 0:00 ps aux ifconfig [output] eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:64 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:9402 (9.1 KiB) TX bytes:0 (0.0 B) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) Essential Container Operations - list, logs, exec, cp, inspect, stop, rm In this secttion we are looking for some of the essential container operations like list,logs, exec etc. First we do list the containers docker ps -a [output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 60c643264937 alpine:3.4 \"sh\" 16 minutes ago Exited (0) 2 minutes ago kind_babbage 105b0de546cb ubuntu \"bash\" About an hour ago Exited (0) About an hour ago admiring_cori 8801c9dc6617 hello-world \"/hello\" 2 hours ago Exited (0) 2 hours ago hardcore_blackwell change the container name docker rename kind_babbage loop [output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 60c643264937 alpine:3.4 \"sh\" 17 minutes ago Exited (0) 2 minutes ago loop 105b0de546cb ubuntu \"bash\" About an hour ago Exited (0) About an hour ago admiring_cori 8801c9dc6617 hello-world \"/hello\" 2 hours ago Exited (0) 2 hours ago hardcore_blackwell look at the name of the first container If you want to follow the log in real-time, use -f flag docker logs 5a75df45379c [output] PID USER TIME COMMAND 1 root 0:00 ps aux , uptime docker exec this command allows you to run command inside container docker exec e9f957dca1b7 ps aux [output] PID USER TIME COMMAND 1 root 0:00 /bin/sh 6 root 0:00 ps aux you can also use docker inspect command . this command gives you detail information about container docker inspect e9f957dca1b7 [output] [ { \"Id\": \"e9f957dca1b727be04357c77edbb2e2b257b22c0832d9f13b4ff06e3854a1237\", \"Created\": \"2018-09-25T08:49:47.619188383Z\", \"Path\": \"/bin/sh\", \"Args\": [], \"State\": { \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 19679, \"ExitCode\": 0, \"Error\": \"\", \"StartedAt\": \"2018-09-25T08:49:48.794801063Z\", \"FinishedAt\": \"0001-01-01T00:00:00Z\" }, \"Image\": \"sha256:174b26fe09c724368aa2c3cc8f2b979b915a33f7b50c94cd215380d56147cd60\", \"ResolvConfPath\": \"/var/lib/docker/containers/e9f957dca1b727be04357c77edbb2e2b257b22c0832d9f13b4ff06e3854a1237/resolv.conf\", \"HostnamePath\": \"/var/lib/docker/containers/e9f957dca1b727be04357c77edbb2e2b257b22c0832d9f13b4ff06e3854a1237/hostname\", \"HostsPath\": \"/var/lib/docker/containers/e9f957dca1b727be04357c77edbb2e2b257b22c0832d9f13b4ff06e3854a1237/hosts\", \"LogPath\": \"/var/lib/docker/containers/e9f957dca1b727be04357c77edbb2e2b257b22c0832d9f13b4ff06e3854a1237/e9f957dca1b727be04357c77edbb2e2b257b22c0832d9f13b4ff06e3854a1237-json.log\", \"Name\": \"/xenodochial_hugle\", \"RestartCount\": 0, \"Driver\": \"overlay2\", \"Platform\": \"linux\", \"MountLabel\": \"\", \"ProcessLabel\": \"\", \"AppArmorProfile\": \"docker-default\", \"ExecIDs\": null, \"HostConfig\": { \"Binds\": null, \"ContainerIDFile\": \"\", \"LogConfig\": { \"Type\": \"json-file\", \"Config\": {} }, \"NetworkMode\": \"default\", \"PortBindings\": {}, \"RestartPolicy\": { \"Name\": \"no\", \"MaximumRetryCount\": 0 }, \"AutoRemove\": false, \"VolumeDriver\": \"\", \"VolumesFrom\": null, \"CapAdd\": null, \"CapDrop\": null, \"Dns\": [], \"DnsOptions\": [], \"DnsSearch\": [], \"ExtraHosts\": null, \"GroupAdd\": null, \"IpcMode\": \"shareable\", \"Cgroup\": \"\", \"Links\": null, \"OomScoreAdj\": 0, \"PidMode\": \"\", \"Privileged\": false, \"PublishAllPorts\": false, \"ReadonlyRootfs\": false, \"SecurityOpt\": null, \"UTSMode\": \"\", \"UsernsMode\": \"\", \"ShmSize\": 67108864, \"Runtime\": \"runc\", \"ConsoleSize\": [ 0, 0 ], \"Isolation\": \"\", \"CpuShares\": 0, \"Memory\": 0, \"NanoCpus\": 0, \"CgroupParent\": \"\", \"BlkioWeight\": 0, \"BlkioWeightDevice\": [], \"BlkioDeviceReadBps\": null, \"BlkioDeviceWriteBps\": null, \"BlkioDeviceReadIOps\": null, \"BlkioDeviceWriteIOps\": null, \"CpuPeriod\": 0, \"CpuQuota\": 0, \"CpuRealtimePeriod\": 0, \"CpuRealtimeRuntime\": 0, \"CpusetCpus\": \"\", \"CpusetMems\": \"\", \"Devices\": [], \"DeviceCgroupRules\": null, \"DiskQuota\": 0, \"KernelMemory\": 0, \"MemoryReservation\": 0, \"MemorySwap\": 0, \"MemorySwappiness\": null, \"OomKillDisable\": false, \"PidsLimit\": 0, \"Ulimits\": null, \"CpuCount\": 0, \"CpuPercent\": 0, \"IOMaximumIOps\": 0, \"IOMaximumBandwidth\": 0 }, \"GraphDriver\": { \"Data\": { \"LowerDir\": \"/var/lib/docker/overlay2/517ee8ad38d79c97b5b4d9351058cf658265bbd752ffca764d6123cd5a45d7a3-init/diff:/var/lib/docker/overlay2/0ff2c00ee39d00f3cbdee2538e5bd08c0650b0a7531e6277513fb1411177c056/diff\", \"MergedDir\": \"/var/lib/docker/overlay2/517ee8ad38d79c97b5b4d9351058cf658265bbd752ffca764d6123cd5a45d7a3/merged\", \"UpperDir\": \"/var/lib/docker/overlay2/517ee8ad38d79c97b5b4d9351058cf658265bbd752ffca764d6123cd5a45d7a3/diff\", \"WorkDir\": \"/var/lib/docker/overlay2/517ee8ad38d79c97b5b4d9351058cf658265bbd752ffca764d6123cd5a45d7a3/work\" }, \"Name\": \"overlay2\" }, \"Mounts\": [], \"Config\": { \"Hostname\": \"e9f957dca1b7\", \"Domainname\": \"\", \"User\": \"\", \"AttachStdin\": false, \"AttachStdout\": false, \"AttachStderr\": false, \"Tty\": true, \"OpenStdin\": true, \"StdinOnce\": false, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], \"Cmd\": [ \"/bin/sh\" ], \"ArgsEscaped\": true, \"Image\": \"alpine:3.4\", \"Volumes\": null, \"WorkingDir\": \"\", \"Entrypoint\": null, \"OnBuild\": null, \"Labels\": {} }, \"NetworkSettings\": { \"Bridge\": \"\", \"SandboxID\": \"a819dc4b5b24f71699cc58804ba227dfbfcd1431deab0bea5fe27ab5b97cc95e\", \"HairpinMode\": false, \"LinkLocalIPv6Address\": \"\", \"LinkLocalIPv6PrefixLen\": 0, \"Ports\": {}, \"SandboxKey\": \"/var/run/docker/netns/a819dc4b5b24\", \"SecondaryIPAddresses\": null, \"SecondaryIPv6Addresses\": null, \"EndpointID\": \"9de8e6914d71fc9b6d569a56ebaeae58aeb1fa21717aa1d94f200d22d82e0d37\", \"Gateway\": \"172.17.0.1\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"IPAddress\": \"172.17.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"MacAddress\": \"02:42:ac:11:00:02\", \"Networks\": { \"bridge\": { \"IPAMConfig\": null, \"Links\": null, \"Aliases\": null, \"NetworkID\": \"a379dcbffa8fff8004d04727d8898d46cf032a830a18d8507d7acbb3d14c552a\", \"EndpointID\": \"9de8e6914d71fc9b6d569a56ebaeae58aeb1fa21717aa1d94f200d22d82e0d37\", \"Gateway\": \"172.17.0.1\", \"IPAddress\": \"172.17.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"MacAddress\": \"02:42:ac:11:00:02\", \"DriverOpts\": null } } } } ] docker copy docker cp testfile e9f957dca1b7:/opt docker diff docker diff e9f957dca1b7 [output] A /opt docker stop docker stop e9f 1b7 [output] e9f 1b7 docker remove docker rm e9f 1b7 [output] e9f 1b7 Publishing containers using port mapping Now, we have already start container, now access that application outside world, we are going through to launch container nginx web server image you can choose the image on docker hub registry for latest version. docker container run -idt -P nginx [output] Unable to find image 'nginx:latest' locally latest: Pulling from library/nginx 802b00ed6f79: Pull complete e9d0e0ea682b: Pull complete d8b7092b9221: Pull complete Digest: sha256:24a0c4b4a4c0eb97a1aabb8e29f18e917d05abfe1b7a7c07857230879ce7d3d3 Status: Downloaded newer image for nginx:latest 6d631d2ecfddb76481e2e75c6f14373dde5907e442231c38c062574cc4b880da Check the port docker ps [output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6d631d2ecfdd nginx \"nginx -g 'daemon of\u2026\" 49 seconds ago Up 47 seconds 0.0.0.0:32768->80/tcp dreamy_gates the container are running on inside port 80 . If you want to acess on outside use the port 32768. docker are automatically pick up the port access this outside use your host_ip:32768 in browser. you can also define specific port use following command docker container run -idt -p 8888:80 nginx docker ps [output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 3104f3f3c062 nginx \"nginx -g 'daemon of\u2026\" 52 seconds ago Up 50 seconds 0.0.0.0:8888->80/tcp mystifying_golick 6d631d2ecfdd nginx \"nginx -g 'daemon of\u2026\" 11 minutes ago Up 11 minutes 0.0.0.0:32768->80/tcp dreamy_gates we going to deploy another web based application ghost docker run -d --name ghost -p 3001:2368 ghost:alpine [output] Unable to find image 'ghost:alpine' locally alpine: Pulling from library/ghost 4fe2ade4980c: Already exists eeb7d76f44e7: Pull complete e35f88fcc259: Pull complete b4d59ef07366: Pull complete dcee404d51ae: Pull complete f0d2c5f09664: Pull complete 6feecb37b3bd: Pull complete 4e29bf9bf09f: Pull complete Digest: sha256:d1d329a9e28096003ddbce69f3fc4a81b72c2c0c9e88426fc432fd3f0e1146e1 Status: Downloaded newer image for ghost:alpine 4c84890e1f4438a647b287751beeda02490ed7a312c05ae4a64ba7f01047a76b docker ps [output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4c84890e1f44 ghost:alpine \"docker-entrypoint.s\u2026\" 40 seconds ago Up 38 seconds 0.0.0.0:3001->2368/tcp ghost 3104f3f3c062 nginx \"nginx -g 'daemon of\u2026\" 8 minutes ago Up 8 minutes 0.0.0.0:8888->80/tcp mystifying_golick 6d631d2ecfdd nginx \"nginx -g 'daemon of\u2026\" 19 minutes ago Up 19 minutes 0.0.0.0:32768->80/tcp dreamy_gates this application are running on port 3001 you can see on web browser host_ip:3001 or localhost:3001 Using docker instead of VMs to create development environments If you have using vm to runing docker container it takes some extra time to run docker container . if you want to see all the images use following command. this shows you all the present images in your local environment. docker images [output] ghost alpine fd8dde6880e2 4 days ago 422MB alpine 3.4 174b26fe09c7 13 days ago 4.82MB alpine latest 196d12cf6ab1 13 days ago 4.41MB hello-world latest 4ab4c602aa5e 2 weeks ago 1.84kB ubuntu latest cd6d8154f1e1 2 weeks ago 84.1MB if wanna see all the layers of docker image use following command docker image history ghost:alpine [output] IMAGE CREATED CREATED BY SIZE COMMENT fd8dde6880e2 4 days ago /bin/sh -c #(nop) CMD [\"node\" \"current/inde\u2026 0B <missing> 4 days ago /bin/sh -c #(nop) EXPOSE 2368/tcp 0B <missing> 4 days ago /bin/sh -c #(nop) ENTRYPOINT [\"docker-entry\u2026 0B <missing> 4 days ago /bin/sh -c #(nop) COPY file:984b6359fb5468bd\u2026 584B <missing> 4 days ago /bin/sh -c #(nop) VOLUME [/var/lib/ghost/co\u2026 0B <missing> 4 days ago /bin/sh -c #(nop) WORKDIR /var/lib/ghost 0B <missing> 4 days ago /bin/sh -c set -ex; mkdir -p \"$GHOST_INSTAL\u2026 301MB <missing> 4 days ago /bin/sh -c #(nop) ENV GHOST_VERSION=2.1.3 0B <missing> 10 days ago /bin/sh -c #(nop) ENV GHOST_CONTENT=/var/li\u2026 0B <missing> 10 days ago /bin/sh -c #(nop) ENV GHOST_INSTALL=/var/li\u2026 0B <missing> 10 days ago /bin/sh -c npm install -g \"ghost-cli@$GHOST_\u2026 51.3MB <missing> 10 days ago /bin/sh -c #(nop) ENV GHOST_CLI_VERSION=1.9\u2026 0B <missing> 13 days ago /bin/sh -c #(nop) ENV NODE_ENV=production 0B <missing> 13 days ago /bin/sh -c apk add --no-cache bash 3.82MB <missing> 13 days ago /bin/sh -c apk add --no-cache 'su-exec>=0.2' 31.8kB <missing> 13 days ago /bin/sh -c #(nop) CMD [\"node\"] 0B <missing> 13 days ago /bin/sh -c apk add --no-cache --virtual .bui\u2026 4.53MB <missing> 13 days ago /bin/sh -c #(nop) ENV YARN_VERSION=1.9.4 0B <missing> 13 days ago /bin/sh -c addgroup -g 1000 node && addu\u2026 56.7MB <missing> 13 days ago /bin/sh -c #(nop) ENV NODE_VERSION=8.12.0 0B <missing> 13 days ago /bin/sh -c #(nop) CMD [\"/bin/sh\"] 0B <missing> 13 days ago /bin/sh -c #(nop) ADD file:25c10b1d1b41d46a1\u2026 4.41MB If you want to use docker devlopment envireonment just pull the ubuntu and centos image you can pull images using docker images docker pull ubuntu [outpot] Using default tag: latest latest: Pulling from library/ubuntu Digest: sha256:de774a3145f7ca4f0bd144c7d4ffb2931e06634f11529653b23eba85aef8e378 Status: Image is up to date for ubuntu:latest pulling centos docker pull centos [output] Using default tag: latest latest: Pulling from library/centos 256b176beaff: Pull complete Digest: sha256:6f6d986d425aeabdc3a02cb61c02abb2e78e57357e92417d6d58332856024faf Status: Downloaded newer image for centos:latest here, the images are ready we are going to create dev environment using ubuntu image docker container run -idt --name dev --net host ubuntu bash 158dfe96692f9381842d009abfea428614fed4d9a16de68d18b408549315fbd9 docker container run -idt --name dev-centos --net host centos bash 99be0c7548ab3c762d7af2fa0cc73a5ad1505589424d82bea2c065a4f1d3bdf7 docker ps -n 2 [output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 99be0c7548ab centos \"bash\" 57 seconds ago Up 56 seconds dev-centos 158dfe96692f ubuntu \"bash\" 2 minutes ago Up 2 minutes dev if you want to go inside container use following command docker exec -it dev bash you can check procees inside the container ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.0 18508 3004 pts/0 Ss+ 10:15 0:00 bash root 10 0.0 0.0 18508 3384 pts/1 Ss 10:20 0:00 bash root 20 0.0 0.0 34400 2772 pts/1 R+ 10:21 0:00 ps aux you can run fowwing command also apt-get update apt-get install vim touch /opt/testfile docker stop dev docker start dev you can persist the data across. make the changes of dev environment as well , we wiiljust stop and start contaoner see the changes before start and stop. docker exec -it dev bash excute the following caommand which vim ls /opt/ Portainer - Web console to managing Docker Environemnts In this Section we are goiing for portainer web console which will allow you web based application which manages your local or remote docker environment. you can also visit the portainer docker volume create portainer_data [output] portainer_data After creating volume excute following command docker run -d -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer [output] Unable to find image 'portainer/portainer:latest' locally latest: Pulling from portainer/portainer d1e017099d17: Pull complete d4e5419541f5: Pull complete Digest: sha256:07c0e19e28e18414dd02c313c36b293758acf197d5af45077e3dd69c630e25cc Status: Downloaded newer image for portainer/portainer:latest db7a8c18cdfcae46d6ccb9d3d5ad0a48568fdc8e5827f478f0c44b95b8235bdf which will aloow container to connect to docker daemon and mange its on. docker ps [output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES db7a8c18cdfc portainer/portainer \"/portainer\" About a minute ago Up About a minute 0.0.0.0:9000->9000/tcp zen_jepsen 99be0c7548ab centos \"bash\" 32 minutes ago Up 32 minutes dev-centos 158dfe96692f ubuntu \"bash\" 34 minutes ago Up 34 minutes dev 4c84890e1f44 ghost:alpine \"docker-entrypoint.s\u2026\" About an hour ago Up About an hour 0.0.0.0:3001->2368/tcp ghost 3104f3f3c062 nginx \"nginx -g 'daemon of\u2026\" About an hour ago Up About an hour 0.0.0.0:8888->80/tcp mystifying_golick 6d631d2ecfdd nginx \"nginx -g 'daemon of\u2026\" 2 hours ago Up 2 hours 0.0.0.0:32768->80/tcp dreamy_gates go to web browser use host_ip:9000 or loaclhost:9000 here,we have create the password you it at least 8 char long here i have to use my local environment click on as per your environment click on connect above are the portainer page is already been setup. we dont worry abot how to launch this portainer set up this alredy avaible on docker images. clik on local up here you can see how many container are present , network, volume etc Launching Application Stack with Docker Compose In this sections we are going to launch prometheus applications. the prometheus application are multiple serices present like pushgateawy and alertmanager that wy we need docker-compose file. just clone prometus repo. git clone https://github.com/vegasbrianc/prometheus.git cd prometheus cat docker-compose.yaml [output] version: '3.1' volumes: prometheus_data: {} grafana_data: {} networks: front-tier: back-tier: services: prometheus: image: prom/prometheus:v2.1.0 volumes: - ./prometheus/:/etc/prometheus/ - prometheus_data:/prometheus command: - '--config.file=/etc/prometheus/prometheus.yml' - '--storage.tsdb.path=/prometheus' - '--web.console.libraries=/usr/share/prometheus/console_libraries' - '--web.console.templates=/usr/share/prometheus/consoles' ports: - 9090:9090 links: - cadvisor:cadvisor - alertmanager:alertmanager depends_on: - cadvisor networks: - back-tier restart: always # deploy: # placement: # constraints: # - node.hostname == ${HOSTNAME} node-exporter: image: prom/node-exporter volumes: - /proc:/host/proc:ro - /sys:/host/sys:ro - /:/rootfs:ro command: - '--path.procfs=/host/proc' - '--path.sysfs=/host/sys' - --collector.filesystem.ignored-mount-points - \"^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs)($$|/)\" ports: - 9100:9100 networks: - back-tier restart: always deploy: mode: global alertmanager: image: prom/alertmanager ports: - 9093:9093 volumes: - ./alertmanager/:/etc/alertmanager/ networks: - back-tier restart: always command: - '--config.file=/etc/alertmanager/config.yml' - '--storage.path=/alertmanager' # deploy: # placement: # constraints: # - node.hostname == ${HOSTNAME} cadvisor: image: google/cadvisor volumes: - /:/rootfs:ro - /var/run:/var/run:rw - /sys:/sys:ro - /var/lib/docker/:/var/lib/docker:ro ports: - 8080:8080 networks: - back-tier restart: always deploy: mode: global grafana: image: grafana/grafana user: \"104\" depends_on: - prometheus ports: - 3000:3000 volumes: - grafana_data:/var/lib/grafana - ./grafana/provisioning/:/etc/grafana/provisioning/ env_file: - ./grafana/config.monitoring networks: - back-tier - front-tier restart: always In this file you can see all the services are defined with portmapping. you can run the following command to run docker compose file docker-compose up -d also use stop and down . stop for stop the all created container and down for delete all create container. docker-compose stop docker-compose down Building an image manually with docker commit In this section we are going to see how to create image of application just excute following command git clone https://github.com/schoolofdevops/facebooc.git after cloning this repo we are going to launch ubuntu image this application running on port 1600 that why we are already exposing port. docker container run -idt --name fb -p 16000:16000 ubuntu bash connect to that container using following command docker exec -it fb bash after connecting that container use following instructions. Install following package: build-essential make libsqlite3-dev sqlite3 sudo apt-get update sudo apt-get install -yq build-essential make libsqlite3-dev sqlite3 after installing this packages we need source code cpoy your source code in insisde the container. docker cp facebooc/ fb:/opt/ after copying the data go inside the container docker exec -it fb bash switch the dir. cd /opt/facebooc/ then Build the application using following command make all Run the application using binary bin/facebooc now go to the web browser host_ip:16000 or localhost:16000 then exit the container commit container using following command including your own tag with your docker hub id docker container commit fb initcron/fb:v1 After creating image push to docker hub registry docker login docker image push initcron/fb:v1 Automatiing image builds with a Dockerfile Above section we build the image using manual approch. In this we going to dockerfile to build image automation. just clone the repo. using following command. git clone https://github.com/schoolofdevops/facebooc.git cd facebooc git checkout docker you can see the Dockerfile cat Dockerfile [output] FROM ubuntu WORKDIR /opt/facebooc RUN apt-get update && \\ apt-get install -yq build-essential make git libsqlite3-dev sqlite3 COPY . /opt/facebooc RUN make all EXPOSE 16000 CMD \"bin/facebooc\" then build docker image image docker image build -t initcron/fb:v2 . after build image launch it docker container run -idt -P initcron/fb:v2","title":"Just docker for openshift"},{"location":"Just_docker_for_openshift/#just-enough-docker-for-a-openshift-practitioner","text":"","title":"Just Enough Docker for a Openshift Practitioner"},{"location":"Just_docker_for_openshift/#setting-up-and-validating-docker-environment","text":"In this chapter, we are going to set docker environment. Visit docs.docker.com this page provides all the information of how to install docker on ubuntu, mac or windows. In this page left side you can see couple of options . when you select docker CE (Docker Community Edition). Thre is also provides instructions on different os platform. There are two options docker EE docker CE Here , we going for docker for ubuntu you can use the following script. #!/bin/bash apt-get update apt-get install -y git wget # Install Docker apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - apt-key fingerprint 0EBFCD88 add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" apt-get update apt-get install -yq docker-ce After install docker using above script you can validate using following command docker version [output] Client: Version: 18.03.1-ce API version: 1.37 Go version: go1.9.5 Git commit: 9ee9f40 Built: Thu Apr 26 07:17:20 2018 OS/Arch: linux/amd64 Experimental: false Orchestrator: swarm Server: Engine: Version: 18.03.1-ce API version: 1.37 (minimum version 1.12) Go version: go1.9.5 Git commit: 9ee9f40 Built: Thu Apr 26 07:15:30 2018 OS/Arch: linux/amd64 Experimental: false Run docker Hello-world sudo docker run hello-world [output] Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world d1725b59e92d: Pull complete Digest: sha256:0add3ace90ecb4adbf7777e9aacf18357296e799f81cabc9fde470971e499788 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/","title":"Setting up and validating docker environment"},{"location":"Just_docker_for_openshift/#play-with-docker","text":"If you don't have way to install docker locally even remote server. you can also use protal play-with-docker Here, we have to login with your docker hub id and password. if you don't have docker hub id and password you should need to create your docker hub id and password docker hub . Play-with-docker gives you working docker environment. you can see the UI Here, In this Page left side create instance label click on on that label they provide you docker envirenment. you can also ren the following command. docker version command shows you to all the information of docker like version, api version Git commit etc. docker version [output] Client: Version: 18.03.1-ce API version: 1.37 Go version: go1.9.5 Git commit: 9ee9f40 Built: Thu Apr 26 07:17:20 2018 OS/Arch: linux/amd64 Experimental: false Orchestrator: swarm Server: Engine: Version: 18.03.1-ce API version: 1.37 (minimum version 1.12) Go version: go1.9.5 Git commit: 9ee9f40 Built: Thu Apr 26 07:15:30 2018 OS/Arch: linux/amd64 Experimental: false Docker hello world image docker run hello-world [output] Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world d1725b59e92d: Pull complete Digest: sha256:0add3ace90ecb4adbf7777e9aacf18357296e799f81cabc9fde470971e499788 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/","title":"Play-with-docker"},{"location":"Just_docker_for_openshift/#running-your-first-container","text":"when your enviromnment ready you can start your first docker container . when you running your forst docker container you need docker registry. Go to docker public registry . when you vist docker hub you can see bunch of images avaible here. we are going to pick basic os image alpine . alpine is distribution of linux, ubuntu etc why we are choosing this image because of footprint of image and you can look at the size of the image look like 2 0r 3 mb that is relly good for smoke testing and running samaller images. Now we have a basic understanding of docker command and sub commands, let us dive straight into launching our very first container docker container run alpine:3.6 uptime Where, we are using docker client to run a application/command uptime using an image by name alpine [output] Unable to find image 'alpine:3.6' locally 3.6: Pulling from library/alpine 117f30b7ae3d: Pull complete Digest: sha256:02eb5cfe4b721495135728ab4aea87418fd4edbfbf83612130a81191f0b2aae3 Status: Downloaded newer image for alpine:3.6 07:45:40 up 3:13, load average: 0.00, 0.00, 0.00 What happened? This command will Pull the alpine image file from docker hub, a cloud registry Create a runtime environment/ container with the above image Launch a program (called uptime) inside that container Stream that output to the terminal Stop the container once the program is exited Let's see what happens when we run that command again, [Output] docker run alpine uptime 07:48:06 up 3:15, load average: 0.00, 0.00, 0.00","title":"Running your first container"},{"location":"Just_docker_for_openshift/#making-container-persist-with-idt-options","text":"We can interact with docker containers by giving -it flags at the run time. These flags stand for i - Interactive t - tty d - detach docker container run -it alpine:3.4 sh [ouput] Unable to find image 'alpine:3.4' locally 3.4: Pulling from library/alpine e110a4a17941: Pull complete Digest: sha256:3dcdb92d7432d56604d4545cbd324b14e647b313626d99b889d0626de158f73a Status: Downloaded newer image for alpine:3.4 / # As you see, we have landed straight into sh shell of that container. This is the result of using -it flags and mentioning that container to run the sh shell. Don't try to exit that container yet. We have to execute some other commands in it to understand the next topic if you go inside the container","title":"Making container persist with -idt options"},{"location":"Just_docker_for_openshift/#namespaced","text":"Like a full fledged OS, Docker container has its own namespaces This enables Docker container to isolate itself from the host as well as other containers Run the following commands and see that alpine container has its own namespaces and not inheriting much from host OS cat /etc/issue Welcome to Alpine Linux 3.4 Kernel \\r on an \\m (\\l) / # command pa aux [output] PID USER TIME COMMAND 1 root 0:00 sh 7 root 0:00 ps aux ifconfig [output] eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:64 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:9402 (9.1 KiB) TX bytes:0 (0.0 B) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B)","title":"Namespaced"},{"location":"Just_docker_for_openshift/#essential-container-operations-list-logs-exec-cp-inspect-stop-rm","text":"In this secttion we are looking for some of the essential container operations like list,logs, exec etc. First we do list the containers docker ps -a [output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 60c643264937 alpine:3.4 \"sh\" 16 minutes ago Exited (0) 2 minutes ago kind_babbage 105b0de546cb ubuntu \"bash\" About an hour ago Exited (0) About an hour ago admiring_cori 8801c9dc6617 hello-world \"/hello\" 2 hours ago Exited (0) 2 hours ago hardcore_blackwell change the container name docker rename kind_babbage loop [output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 60c643264937 alpine:3.4 \"sh\" 17 minutes ago Exited (0) 2 minutes ago loop 105b0de546cb ubuntu \"bash\" About an hour ago Exited (0) About an hour ago admiring_cori 8801c9dc6617 hello-world \"/hello\" 2 hours ago Exited (0) 2 hours ago hardcore_blackwell look at the name of the first container If you want to follow the log in real-time, use -f flag docker logs 5a75df45379c [output] PID USER TIME COMMAND 1 root 0:00 ps aux , uptime docker exec this command allows you to run command inside container docker exec e9f957dca1b7 ps aux [output] PID USER TIME COMMAND 1 root 0:00 /bin/sh 6 root 0:00 ps aux you can also use docker inspect command . this command gives you detail information about container docker inspect e9f957dca1b7 [output] [ { \"Id\": \"e9f957dca1b727be04357c77edbb2e2b257b22c0832d9f13b4ff06e3854a1237\", \"Created\": \"2018-09-25T08:49:47.619188383Z\", \"Path\": \"/bin/sh\", \"Args\": [], \"State\": { \"Status\": \"running\", \"Running\": true, \"Paused\": false, \"Restarting\": false, \"OOMKilled\": false, \"Dead\": false, \"Pid\": 19679, \"ExitCode\": 0, \"Error\": \"\", \"StartedAt\": \"2018-09-25T08:49:48.794801063Z\", \"FinishedAt\": \"0001-01-01T00:00:00Z\" }, \"Image\": \"sha256:174b26fe09c724368aa2c3cc8f2b979b915a33f7b50c94cd215380d56147cd60\", \"ResolvConfPath\": \"/var/lib/docker/containers/e9f957dca1b727be04357c77edbb2e2b257b22c0832d9f13b4ff06e3854a1237/resolv.conf\", \"HostnamePath\": \"/var/lib/docker/containers/e9f957dca1b727be04357c77edbb2e2b257b22c0832d9f13b4ff06e3854a1237/hostname\", \"HostsPath\": \"/var/lib/docker/containers/e9f957dca1b727be04357c77edbb2e2b257b22c0832d9f13b4ff06e3854a1237/hosts\", \"LogPath\": \"/var/lib/docker/containers/e9f957dca1b727be04357c77edbb2e2b257b22c0832d9f13b4ff06e3854a1237/e9f957dca1b727be04357c77edbb2e2b257b22c0832d9f13b4ff06e3854a1237-json.log\", \"Name\": \"/xenodochial_hugle\", \"RestartCount\": 0, \"Driver\": \"overlay2\", \"Platform\": \"linux\", \"MountLabel\": \"\", \"ProcessLabel\": \"\", \"AppArmorProfile\": \"docker-default\", \"ExecIDs\": null, \"HostConfig\": { \"Binds\": null, \"ContainerIDFile\": \"\", \"LogConfig\": { \"Type\": \"json-file\", \"Config\": {} }, \"NetworkMode\": \"default\", \"PortBindings\": {}, \"RestartPolicy\": { \"Name\": \"no\", \"MaximumRetryCount\": 0 }, \"AutoRemove\": false, \"VolumeDriver\": \"\", \"VolumesFrom\": null, \"CapAdd\": null, \"CapDrop\": null, \"Dns\": [], \"DnsOptions\": [], \"DnsSearch\": [], \"ExtraHosts\": null, \"GroupAdd\": null, \"IpcMode\": \"shareable\", \"Cgroup\": \"\", \"Links\": null, \"OomScoreAdj\": 0, \"PidMode\": \"\", \"Privileged\": false, \"PublishAllPorts\": false, \"ReadonlyRootfs\": false, \"SecurityOpt\": null, \"UTSMode\": \"\", \"UsernsMode\": \"\", \"ShmSize\": 67108864, \"Runtime\": \"runc\", \"ConsoleSize\": [ 0, 0 ], \"Isolation\": \"\", \"CpuShares\": 0, \"Memory\": 0, \"NanoCpus\": 0, \"CgroupParent\": \"\", \"BlkioWeight\": 0, \"BlkioWeightDevice\": [], \"BlkioDeviceReadBps\": null, \"BlkioDeviceWriteBps\": null, \"BlkioDeviceReadIOps\": null, \"BlkioDeviceWriteIOps\": null, \"CpuPeriod\": 0, \"CpuQuota\": 0, \"CpuRealtimePeriod\": 0, \"CpuRealtimeRuntime\": 0, \"CpusetCpus\": \"\", \"CpusetMems\": \"\", \"Devices\": [], \"DeviceCgroupRules\": null, \"DiskQuota\": 0, \"KernelMemory\": 0, \"MemoryReservation\": 0, \"MemorySwap\": 0, \"MemorySwappiness\": null, \"OomKillDisable\": false, \"PidsLimit\": 0, \"Ulimits\": null, \"CpuCount\": 0, \"CpuPercent\": 0, \"IOMaximumIOps\": 0, \"IOMaximumBandwidth\": 0 }, \"GraphDriver\": { \"Data\": { \"LowerDir\": \"/var/lib/docker/overlay2/517ee8ad38d79c97b5b4d9351058cf658265bbd752ffca764d6123cd5a45d7a3-init/diff:/var/lib/docker/overlay2/0ff2c00ee39d00f3cbdee2538e5bd08c0650b0a7531e6277513fb1411177c056/diff\", \"MergedDir\": \"/var/lib/docker/overlay2/517ee8ad38d79c97b5b4d9351058cf658265bbd752ffca764d6123cd5a45d7a3/merged\", \"UpperDir\": \"/var/lib/docker/overlay2/517ee8ad38d79c97b5b4d9351058cf658265bbd752ffca764d6123cd5a45d7a3/diff\", \"WorkDir\": \"/var/lib/docker/overlay2/517ee8ad38d79c97b5b4d9351058cf658265bbd752ffca764d6123cd5a45d7a3/work\" }, \"Name\": \"overlay2\" }, \"Mounts\": [], \"Config\": { \"Hostname\": \"e9f957dca1b7\", \"Domainname\": \"\", \"User\": \"\", \"AttachStdin\": false, \"AttachStdout\": false, \"AttachStderr\": false, \"Tty\": true, \"OpenStdin\": true, \"StdinOnce\": false, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], \"Cmd\": [ \"/bin/sh\" ], \"ArgsEscaped\": true, \"Image\": \"alpine:3.4\", \"Volumes\": null, \"WorkingDir\": \"\", \"Entrypoint\": null, \"OnBuild\": null, \"Labels\": {} }, \"NetworkSettings\": { \"Bridge\": \"\", \"SandboxID\": \"a819dc4b5b24f71699cc58804ba227dfbfcd1431deab0bea5fe27ab5b97cc95e\", \"HairpinMode\": false, \"LinkLocalIPv6Address\": \"\", \"LinkLocalIPv6PrefixLen\": 0, \"Ports\": {}, \"SandboxKey\": \"/var/run/docker/netns/a819dc4b5b24\", \"SecondaryIPAddresses\": null, \"SecondaryIPv6Addresses\": null, \"EndpointID\": \"9de8e6914d71fc9b6d569a56ebaeae58aeb1fa21717aa1d94f200d22d82e0d37\", \"Gateway\": \"172.17.0.1\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"IPAddress\": \"172.17.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"MacAddress\": \"02:42:ac:11:00:02\", \"Networks\": { \"bridge\": { \"IPAMConfig\": null, \"Links\": null, \"Aliases\": null, \"NetworkID\": \"a379dcbffa8fff8004d04727d8898d46cf032a830a18d8507d7acbb3d14c552a\", \"EndpointID\": \"9de8e6914d71fc9b6d569a56ebaeae58aeb1fa21717aa1d94f200d22d82e0d37\", \"Gateway\": \"172.17.0.1\", \"IPAddress\": \"172.17.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"MacAddress\": \"02:42:ac:11:00:02\", \"DriverOpts\": null } } } } ] docker copy docker cp testfile e9f957dca1b7:/opt docker diff docker diff e9f957dca1b7 [output] A /opt docker stop docker stop e9f 1b7 [output] e9f 1b7 docker remove docker rm e9f 1b7 [output] e9f 1b7","title":"Essential Container Operations - list, logs, exec, cp, inspect, stop, rm"},{"location":"Just_docker_for_openshift/#publishing-containers-using-port-mapping","text":"Now, we have already start container, now access that application outside world, we are going through to launch container nginx web server image you can choose the image on docker hub registry for latest version. docker container run -idt -P nginx [output] Unable to find image 'nginx:latest' locally latest: Pulling from library/nginx 802b00ed6f79: Pull complete e9d0e0ea682b: Pull complete d8b7092b9221: Pull complete Digest: sha256:24a0c4b4a4c0eb97a1aabb8e29f18e917d05abfe1b7a7c07857230879ce7d3d3 Status: Downloaded newer image for nginx:latest 6d631d2ecfddb76481e2e75c6f14373dde5907e442231c38c062574cc4b880da Check the port docker ps [output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6d631d2ecfdd nginx \"nginx -g 'daemon of\u2026\" 49 seconds ago Up 47 seconds 0.0.0.0:32768->80/tcp dreamy_gates the container are running on inside port 80 . If you want to acess on outside use the port 32768. docker are automatically pick up the port access this outside use your host_ip:32768 in browser. you can also define specific port use following command docker container run -idt -p 8888:80 nginx docker ps [output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 3104f3f3c062 nginx \"nginx -g 'daemon of\u2026\" 52 seconds ago Up 50 seconds 0.0.0.0:8888->80/tcp mystifying_golick 6d631d2ecfdd nginx \"nginx -g 'daemon of\u2026\" 11 minutes ago Up 11 minutes 0.0.0.0:32768->80/tcp dreamy_gates we going to deploy another web based application ghost docker run -d --name ghost -p 3001:2368 ghost:alpine [output] Unable to find image 'ghost:alpine' locally alpine: Pulling from library/ghost 4fe2ade4980c: Already exists eeb7d76f44e7: Pull complete e35f88fcc259: Pull complete b4d59ef07366: Pull complete dcee404d51ae: Pull complete f0d2c5f09664: Pull complete 6feecb37b3bd: Pull complete 4e29bf9bf09f: Pull complete Digest: sha256:d1d329a9e28096003ddbce69f3fc4a81b72c2c0c9e88426fc432fd3f0e1146e1 Status: Downloaded newer image for ghost:alpine 4c84890e1f4438a647b287751beeda02490ed7a312c05ae4a64ba7f01047a76b docker ps [output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4c84890e1f44 ghost:alpine \"docker-entrypoint.s\u2026\" 40 seconds ago Up 38 seconds 0.0.0.0:3001->2368/tcp ghost 3104f3f3c062 nginx \"nginx -g 'daemon of\u2026\" 8 minutes ago Up 8 minutes 0.0.0.0:8888->80/tcp mystifying_golick 6d631d2ecfdd nginx \"nginx -g 'daemon of\u2026\" 19 minutes ago Up 19 minutes 0.0.0.0:32768->80/tcp dreamy_gates this application are running on port 3001 you can see on web browser host_ip:3001 or localhost:3001","title":"Publishing containers using port mapping"},{"location":"Just_docker_for_openshift/#using-docker-instead-of-vms-to-create-development-environments","text":"If you have using vm to runing docker container it takes some extra time to run docker container . if you want to see all the images use following command. this shows you all the present images in your local environment. docker images [output] ghost alpine fd8dde6880e2 4 days ago 422MB alpine 3.4 174b26fe09c7 13 days ago 4.82MB alpine latest 196d12cf6ab1 13 days ago 4.41MB hello-world latest 4ab4c602aa5e 2 weeks ago 1.84kB ubuntu latest cd6d8154f1e1 2 weeks ago 84.1MB if wanna see all the layers of docker image use following command docker image history ghost:alpine [output] IMAGE CREATED CREATED BY SIZE COMMENT fd8dde6880e2 4 days ago /bin/sh -c #(nop) CMD [\"node\" \"current/inde\u2026 0B <missing> 4 days ago /bin/sh -c #(nop) EXPOSE 2368/tcp 0B <missing> 4 days ago /bin/sh -c #(nop) ENTRYPOINT [\"docker-entry\u2026 0B <missing> 4 days ago /bin/sh -c #(nop) COPY file:984b6359fb5468bd\u2026 584B <missing> 4 days ago /bin/sh -c #(nop) VOLUME [/var/lib/ghost/co\u2026 0B <missing> 4 days ago /bin/sh -c #(nop) WORKDIR /var/lib/ghost 0B <missing> 4 days ago /bin/sh -c set -ex; mkdir -p \"$GHOST_INSTAL\u2026 301MB <missing> 4 days ago /bin/sh -c #(nop) ENV GHOST_VERSION=2.1.3 0B <missing> 10 days ago /bin/sh -c #(nop) ENV GHOST_CONTENT=/var/li\u2026 0B <missing> 10 days ago /bin/sh -c #(nop) ENV GHOST_INSTALL=/var/li\u2026 0B <missing> 10 days ago /bin/sh -c npm install -g \"ghost-cli@$GHOST_\u2026 51.3MB <missing> 10 days ago /bin/sh -c #(nop) ENV GHOST_CLI_VERSION=1.9\u2026 0B <missing> 13 days ago /bin/sh -c #(nop) ENV NODE_ENV=production 0B <missing> 13 days ago /bin/sh -c apk add --no-cache bash 3.82MB <missing> 13 days ago /bin/sh -c apk add --no-cache 'su-exec>=0.2' 31.8kB <missing> 13 days ago /bin/sh -c #(nop) CMD [\"node\"] 0B <missing> 13 days ago /bin/sh -c apk add --no-cache --virtual .bui\u2026 4.53MB <missing> 13 days ago /bin/sh -c #(nop) ENV YARN_VERSION=1.9.4 0B <missing> 13 days ago /bin/sh -c addgroup -g 1000 node && addu\u2026 56.7MB <missing> 13 days ago /bin/sh -c #(nop) ENV NODE_VERSION=8.12.0 0B <missing> 13 days ago /bin/sh -c #(nop) CMD [\"/bin/sh\"] 0B <missing> 13 days ago /bin/sh -c #(nop) ADD file:25c10b1d1b41d46a1\u2026 4.41MB If you want to use docker devlopment envireonment just pull the ubuntu and centos image you can pull images using docker images docker pull ubuntu [outpot] Using default tag: latest latest: Pulling from library/ubuntu Digest: sha256:de774a3145f7ca4f0bd144c7d4ffb2931e06634f11529653b23eba85aef8e378 Status: Image is up to date for ubuntu:latest pulling centos docker pull centos [output] Using default tag: latest latest: Pulling from library/centos 256b176beaff: Pull complete Digest: sha256:6f6d986d425aeabdc3a02cb61c02abb2e78e57357e92417d6d58332856024faf Status: Downloaded newer image for centos:latest here, the images are ready we are going to create dev environment using ubuntu image docker container run -idt --name dev --net host ubuntu bash 158dfe96692f9381842d009abfea428614fed4d9a16de68d18b408549315fbd9 docker container run -idt --name dev-centos --net host centos bash 99be0c7548ab3c762d7af2fa0cc73a5ad1505589424d82bea2c065a4f1d3bdf7 docker ps -n 2 [output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 99be0c7548ab centos \"bash\" 57 seconds ago Up 56 seconds dev-centos 158dfe96692f ubuntu \"bash\" 2 minutes ago Up 2 minutes dev if you want to go inside container use following command docker exec -it dev bash you can check procees inside the container ps aux USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1 0.0 0.0 18508 3004 pts/0 Ss+ 10:15 0:00 bash root 10 0.0 0.0 18508 3384 pts/1 Ss 10:20 0:00 bash root 20 0.0 0.0 34400 2772 pts/1 R+ 10:21 0:00 ps aux you can run fowwing command also apt-get update apt-get install vim touch /opt/testfile docker stop dev docker start dev you can persist the data across. make the changes of dev environment as well , we wiiljust stop and start contaoner see the changes before start and stop. docker exec -it dev bash excute the following caommand which vim ls /opt/","title":"Using docker instead of VMs to create development environments"},{"location":"Just_docker_for_openshift/#portainer-web-console-to-managing-docker-environemnts","text":"In this Section we are goiing for portainer web console which will allow you web based application which manages your local or remote docker environment. you can also visit the portainer docker volume create portainer_data [output] portainer_data After creating volume excute following command docker run -d -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer [output] Unable to find image 'portainer/portainer:latest' locally latest: Pulling from portainer/portainer d1e017099d17: Pull complete d4e5419541f5: Pull complete Digest: sha256:07c0e19e28e18414dd02c313c36b293758acf197d5af45077e3dd69c630e25cc Status: Downloaded newer image for portainer/portainer:latest db7a8c18cdfcae46d6ccb9d3d5ad0a48568fdc8e5827f478f0c44b95b8235bdf which will aloow container to connect to docker daemon and mange its on. docker ps [output] CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES db7a8c18cdfc portainer/portainer \"/portainer\" About a minute ago Up About a minute 0.0.0.0:9000->9000/tcp zen_jepsen 99be0c7548ab centos \"bash\" 32 minutes ago Up 32 minutes dev-centos 158dfe96692f ubuntu \"bash\" 34 minutes ago Up 34 minutes dev 4c84890e1f44 ghost:alpine \"docker-entrypoint.s\u2026\" About an hour ago Up About an hour 0.0.0.0:3001->2368/tcp ghost 3104f3f3c062 nginx \"nginx -g 'daemon of\u2026\" About an hour ago Up About an hour 0.0.0.0:8888->80/tcp mystifying_golick 6d631d2ecfdd nginx \"nginx -g 'daemon of\u2026\" 2 hours ago Up 2 hours 0.0.0.0:32768->80/tcp dreamy_gates go to web browser use host_ip:9000 or loaclhost:9000 here,we have create the password you it at least 8 char long here i have to use my local environment click on as per your environment click on connect above are the portainer page is already been setup. we dont worry abot how to launch this portainer set up this alredy avaible on docker images. clik on local up here you can see how many container are present , network, volume etc","title":"Portainer - Web console to managing Docker Environemnts"},{"location":"Just_docker_for_openshift/#launching-application-stack-with-docker-compose","text":"In this sections we are going to launch prometheus applications. the prometheus application are multiple serices present like pushgateawy and alertmanager that wy we need docker-compose file. just clone prometus repo. git clone https://github.com/vegasbrianc/prometheus.git cd prometheus cat docker-compose.yaml [output] version: '3.1' volumes: prometheus_data: {} grafana_data: {} networks: front-tier: back-tier: services: prometheus: image: prom/prometheus:v2.1.0 volumes: - ./prometheus/:/etc/prometheus/ - prometheus_data:/prometheus command: - '--config.file=/etc/prometheus/prometheus.yml' - '--storage.tsdb.path=/prometheus' - '--web.console.libraries=/usr/share/prometheus/console_libraries' - '--web.console.templates=/usr/share/prometheus/consoles' ports: - 9090:9090 links: - cadvisor:cadvisor - alertmanager:alertmanager depends_on: - cadvisor networks: - back-tier restart: always # deploy: # placement: # constraints: # - node.hostname == ${HOSTNAME} node-exporter: image: prom/node-exporter volumes: - /proc:/host/proc:ro - /sys:/host/sys:ro - /:/rootfs:ro command: - '--path.procfs=/host/proc' - '--path.sysfs=/host/sys' - --collector.filesystem.ignored-mount-points - \"^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs)($$|/)\" ports: - 9100:9100 networks: - back-tier restart: always deploy: mode: global alertmanager: image: prom/alertmanager ports: - 9093:9093 volumes: - ./alertmanager/:/etc/alertmanager/ networks: - back-tier restart: always command: - '--config.file=/etc/alertmanager/config.yml' - '--storage.path=/alertmanager' # deploy: # placement: # constraints: # - node.hostname == ${HOSTNAME} cadvisor: image: google/cadvisor volumes: - /:/rootfs:ro - /var/run:/var/run:rw - /sys:/sys:ro - /var/lib/docker/:/var/lib/docker:ro ports: - 8080:8080 networks: - back-tier restart: always deploy: mode: global grafana: image: grafana/grafana user: \"104\" depends_on: - prometheus ports: - 3000:3000 volumes: - grafana_data:/var/lib/grafana - ./grafana/provisioning/:/etc/grafana/provisioning/ env_file: - ./grafana/config.monitoring networks: - back-tier - front-tier restart: always In this file you can see all the services are defined with portmapping. you can run the following command to run docker compose file docker-compose up -d also use stop and down . stop for stop the all created container and down for delete all create container. docker-compose stop docker-compose down","title":"Launching Application Stack with Docker Compose"},{"location":"Just_docker_for_openshift/#building-an-image-manually-with-docker-commit","text":"In this section we are going to see how to create image of application just excute following command git clone https://github.com/schoolofdevops/facebooc.git after cloning this repo we are going to launch ubuntu image this application running on port 1600 that why we are already exposing port. docker container run -idt --name fb -p 16000:16000 ubuntu bash connect to that container using following command docker exec -it fb bash after connecting that container use following instructions. Install following package: build-essential make libsqlite3-dev sqlite3 sudo apt-get update sudo apt-get install -yq build-essential make libsqlite3-dev sqlite3 after installing this packages we need source code cpoy your source code in insisde the container. docker cp facebooc/ fb:/opt/ after copying the data go inside the container docker exec -it fb bash switch the dir. cd /opt/facebooc/ then Build the application using following command make all Run the application using binary bin/facebooc now go to the web browser host_ip:16000 or localhost:16000 then exit the container commit container using following command including your own tag with your docker hub id docker container commit fb initcron/fb:v1 After creating image push to docker hub registry docker login docker image push initcron/fb:v1","title":"Building an image manually with docker commit"},{"location":"Just_docker_for_openshift/#automatiing-image-builds-with-a-dockerfile","text":"Above section we build the image using manual approch. In this we going to dockerfile to build image automation. just clone the repo. using following command. git clone https://github.com/schoolofdevops/facebooc.git cd facebooc git checkout docker you can see the Dockerfile cat Dockerfile [output] FROM ubuntu WORKDIR /opt/facebooc RUN apt-get update && \\ apt-get install -yq build-essential make git libsqlite3-dev sqlite3 COPY . /opt/facebooc RUN make all EXPOSE 16000 CMD \"bin/facebooc\" then build docker image image docker image build -t initcron/fb:v2 . after build image launch it docker container run -idt -P initcron/fb:v2","title":"Automatiing image builds with a Dockerfile"},{"location":"configs/","text":"In this lesson we are going to cover the following topics Logging in Disaply Config Create a project Logging in oc login Listing Configurations Check current config oc get projects oc status Creating a project for voting app Projects offers separation of resources running on the same physical infrastructure into virtual clusters. It is typically useful in mid to large scale environments with multiple projects, teams and need separate scopes. Lets create a namespace called vote oc new-project voting To create namespace Now using project \"voting\" on server \"https://192.168.64.2:8443\". You can add applications to this project with the 'new-app' command. For example, try: oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git to build a new example application in Ruby.","title":"Configuring Cluster"},{"location":"configs/#logging-in","text":"oc login","title":"Logging in"},{"location":"configs/#listing-configurations","text":"Check current config oc get projects oc status","title":"Listing Configurations"},{"location":"configs/#creating-a-project-for-voting-app","text":"Projects offers separation of resources running on the same physical infrastructure into virtual clusters. It is typically useful in mid to large scale environments with multiple projects, teams and need separate scopes. Lets create a namespace called vote oc new-project voting To create namespace Now using project \"voting\" on server \"https://192.168.64.2:8443\". You can add applications to this project with the 'new-app' command. For example, try: oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git to build a new example application in Ruby.","title":"Creating a project for voting app"},{"location":"deploy_app_from_source_image/","text":"Deploying apps from Source, Template, Dockerfile - S2I Builders,Build Configs Launching demo app with source strategy In this section we are talking about how to deploy application using source. We have already deployed vote frontend application. In this section we are deploying result application and database. We will start with result application which is nodeJs application. First we deploy sample devops demo application which php application for practice. we will deploy this application using oc cli. oc new-app https://github.com/devopsdemoapps/devops-demo-app.git [output] --> Found image 9dd8c80 (2 weeks old) in image stream \"openshift/php\" under tag \"7.1\" for \"php\" Apache 2.4 with PHP 7.1 ----------------------- PHP 7.1 available as container is a base platform for building and running various PHP 7.1 applications and frameworks. PHP is an HTML-embedded scripting language. PHP attempts to make it easy for developers to write dynamically generated web pages. PHP also offers built-in database integration for several commercial and non-commercial database management systems, so writing a database-enabled webpage with PHP is fairly simple. The most common use of PHP coding is probably as a replacement for CGI scripts. Tags: builder, php, php71, rh-php71 * The source repository appears to match: php * A source build using source code from https://github.com/devopsdemoapps/devops-demo-app.git will be created * The resulting image will be pushed to image stream \"devops-demo-app:latest\" * Use 'start-build' to trigger a new build * This image will be deployed in deployment config \"devops-demo-app\" * Ports 8080/tcp, 8443/tcp will be load balanced by service \"devops-demo-app\" * Other containers can access this service through the hostname \"devops-demo-app\" --> Creating resources ... imagestream \"devops-demo-app\" created buildconfig \"devops-demo-app\" created deploymentconfig \"devops-demo-app\" created service \"devops-demo-app\" created --> Success Build scheduled, use 'oc logs -f bc/devops-demo-app' to track its progress. Application is not exposed. You can expose services to the outside world by executing one or more of the commands below: 'oc expose svc/devops-demo-app' Run 'oc status' to view your app. we need to expose that application using following command. oc expose svc/devops-demo-app [output] route \"devops-demo-app\" exposed check the status of application oc status [output] In project instavote app (instavote) on server https://128.199.213.193:8443 http://devops-demo-app-instavote.128.199.213.193.nip.io to pod port 8080-tcp (svc/devops-demo-app) dc/devops-demo-app deploys istag/devops-demo-app:latest <- bc/devops-demo-app source builds https://github.com/devopsdemoapps/devops-demo-app.git on openshift/php:7.1 build #1 failed about a minute ago - 5eb913a: v2 (Gourav Shah <gs@initcron.org>) deployment #1 waiting on image or update svc/redis - 172.30.57.198:6379 dc/redis deploys istag/redis:alpine deployment #2 deployed 14 minutes ago - 1 pod deployment #1 failed 2 hours ago: config change http://vote-instavote.128.199.213.193.nip.io to pod port 8080 (svc/vote) dc/vote deploys istag/vote:v1 deployment #2 deployed 14 minutes ago - 4 pods deployment #1 failed 2 hours ago: config change S2I Builder Primer In this we are talking about how to builds nodeJs application that is result. We are also talking about openshift Builder. search nodeJs stream in openshift. oc new-app --search nodejs [output] Templates (oc new-app --template=<template>) ----- nodejs-mongo-persistent Project: openshift An example Node.js application with a MongoDB database. For more information about using this template, including OpenShift considerations, see https://github.com/openshift/nodejs-ex/blob/master/README.md. Image streams (oc new-app --image-stream=<image-stream> [--code=<source>]) ----- nodejs Project: openshift Tags: 6, 8, latest oc get is [ouput] NAME DOCKER REPO TAGS UPDATED redis 172.30.1.1:5000/instavote/redis alpine 22 hours ago vote 172.30.1.1:5000/instavote/vote v1 23 hours ago list of image stream using namespace openshift oc get is -n openshift [output] NAME DOCKER REPO TAGS UPDATED dotnet 172.30.1.1:5000/openshift/dotnet 2.0,latest 11 days ago httpd 172.30.1.1:5000/openshift/httpd 2.4,latest 11 days ago jenkins 172.30.1.1:5000/openshift/jenkins 1,2,latest 11 days ago mariadb 172.30.1.1:5000/openshift/mariadb latest,10.1,10.2 11 days ago mongodb 172.30.1.1:5000/openshift/mongodb 3.2,3.4,latest + 2 more... 11 days ago mysql 172.30.1.1:5000/openshift/mysql 5.5,5.6,5.7 + 1 more... 11 days ago nginx 172.30.1.1:5000/openshift/nginx 1.10,1.12,1.8 + 1 more... 11 days ago nodejs 172.30.1.1:5000/openshift/nodejs 0.10,4,6 + 2 more... 11 days ago perl 172.30.1.1:5000/openshift/perl 5.16,5.20,5.24 + 1 more... 11 days ago php 172.30.1.1:5000/openshift/php 5.5,5.6,7.0 + 2 more... 11 days ago postgresql 172.30.1.1:5000/openshift/postgresql 9.6,latest,9.2 + 2 more... 11 days ago python 172.30.1.1:5000/openshift/python 2.7,3.3,3.4 + 3 more... 11 days ago redis 172.30.1.1:5000/openshift/redis latest,3.2 11 days ago ruby 172.30.1.1:5000/openshift/ruby 2.3,2.4,2.5 + 3 more... 11 days ago wildfly 172.30.1.1:5000/openshift/wildfly latest,10.0,10.1 + 4 more... 11 days ago Deploying results app with Nodejs S2I Builder In the section we are deploy node application using custom builder from the images stream nodeJs and tag 8. Search nodejs image using following command. oc new-app --search nodejs [output] Templates (oc new-app --template=<template>) ----- nodejs-mongo-persistent Project: openshift An example Node.js application with a MongoDB database. For more information about using this template, including OpenShift considerations, see https://github.com/openshift/nodejs-ex/blob/master/README.md. Image streams (oc new-app --image-stream=<image-stream> [--code=<source>]) ----- nodejs Project: openshift Tags: 6, 8, latest Then, Deploy the result application. oc new-app nodejs:8~https://github.com/initcron/example-voting-app.git --context-dir='result' --name=result [output] --> Found image a603093 (2 weeks old) in image stream \"openshift/nodejs\" under tag \"8\" for \"nodejs:8\" Node.js 8 --------- Node.js 8 available as container is a base platform for building and running various Node.js 8 applications and frameworks. Node.js is a platform built on Chrome's JavaScript runtime for easily building fast, scalable network applications. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient, perfect for data-intensive real-time applications that run across distributed devices. Tags: builder, nodejs, nodejs8 * A source build using source code from https://github.com/initcron/example-voting-app.git will be created * The resulting image will be pushed to image stream \"result:latest\" * Use 'start-build' to trigger a new build * This image will be deployed in deployment config \"result\" * Port 8080/tcp will be load balanced by service \"result\" * Other containers can access this service through the hostname \"result\" --> Creating resources ... imagestream \"result\" created buildconfig \"result\" created deploymentconfig \"result\" created service \"result\" created --> Success Build scheduled, use 'oc logs -f bc/result' to track its progress. Application is not exposed. You can expose services to the outside world by executing one or more of the commands below: 'oc expose svc/result' Run 'oc status' to view your app. Then, Expose the services oc expose svc/result [output] route \"result\" exposed show the build in oc including oc get bc [output] NAME TYPE FROM LATEST devops-demo-app Source Git 1 result Source Git 1 Launching db with a template and parameters In this section we are talking about result application database. We are connecting database to the result application. Database is postgresql which result connect. We will use the Template for postgresql. Just search postgresql database on openshift using following command. oc new-app --search postgres [output] Templates (oc new-app --template=<template>) ----- postgresql-persistent Project: openshift PostgreSQL database service, with persistent storage. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/postgresql-container/. NOTE: Scaling to more than one replica is not supported. You must have persistent volumes available in your cluster to use this template. Image streams (oc new-app --image-stream=<image-stream> [--code=<source>]) ----- postgresql Project: openshift Tags: 9.5, 9.6, latest Docker images (oc new-app --docker-image=<docker-image> [--code=<source>]) ----- postgres Registry: Docker Hub Tags: latest The , See available template in openshit. oc get templates -n openshift [output] NAME DESCRIPTION PARAMETERS OBJECTS cakephp-mysql-persistent An example CakePHP application with a MySQL database. For more information ab... 20 (4 blank) 9 dancer-mysql-persistent An example Dancer application with a MySQL database. For more information abo... 17 (5 blank) 9 django-psql-persistent An example Django application with a PostgreSQL database. For more informatio... 20 (5 blank) 9 jenkins-ephemeral Jenkins service, without persistent storage.... 7 (all set) 6 jenkins-pipeline-example This example showcases the new Jenkins Pipeline integration in OpenShift,... 16 (4 blank) 8 mariadb-persistent MariaDB database service, with persistent storage. For more information about... 9 (3 generated) 4 mongodb-persistent MongoDB database service, with persistent storage. For more information about... 9 (3 generated) 4 mysql-persistent MySQL database service, with persistent storage. For more information about u... 9 (3 generated) 4 nodejs-mongo-persistent An example Node.js application with a MongoDB database. For more information... 19 (4 blank) 9 postgresql-persistent PostgreSQL database service, with persistent storage. For more information ab... 8 (2 generated) 4 rails-pgsql-persistent An example Rails application with a PostgreSQL database. For more information... 21 (4 blank) 9 describe postgres templates oc describe template postgresql-persistent -n openshift [output] Name: postgresql-persistent Namespace: openshift Created: 11 days ago Labels: <none> Description: PostgreSQL database service, with persistent storage. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/postgresql-container/. NOTE: Scaling to more than one replica is not supported. You must have persistent volumes available in your cluster to use this template. Annotations: iconClass=icon-postgresql kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"template.openshift.io/v1\",\"kind\":\"Template\",\"labels\":{\"template\":\"postgresql-persistent-template\"},\"message\":\"The following service(s) have been created in your project: ${DATABASE_SERVICE_NAME}.\\n\\n Username: ${POSTGRESQL_USER}\\n Password: ${POSTGRESQL_PASSWORD}\\n Database Name: ${POSTGRESQL_DATABASE}\\n Connection URL: postgresql://${DATABASE_SERVICE_NAME}:5432/\\n\\nFor more information about using this template, including OpenShift considerations, see https://github.com/sclorg/postgresql-container/.\",\"metadata\":{\"annotations\":{\"description\":\"PostgreSQL database service, with persistent storage. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/postgresql-container/.\\n\\nNOTE: Scaling to more than one replica is not supported. You must have persistent volumes available in your cluster to use this template.\",\"iconClass\":\"icon-postgresql\",\"openshift.io/display-name\":\"PostgreSQL\",\"openshift.io/documentation-url\":\"https://docs.openshift.org/latest/using_images/db_images/postgresql.html\",\"openshift.io/long-description\":\"This template provides a standalone PostgreSQL server with a database created. The database is stored on persistent storage. The database name, username, and password are chosen via parameters when provisioning this service.\",\"openshift.io/provider-display-name\":\"Red Hat, Inc.\",\"openshift.io/support-url\":\"https://access.redhat.com\",\"tags\":\"database,postgresql\"},\"name\":\"postgresql-persistent\",\"namespace\":\"openshift\"},\"objects\":[{\"apiVersion\":\"v1\",\"kind\":\"Secret\",\"metadata\":{\"annotations\":{\"template.openshift.io/expose-database_name\":\"{.data['database-name']}\",\"template.openshift.io/expose-password\":\"{.data['database-password']}\",\"template.openshift.io/expose-username\":\"{.data['database-user']}\"},\"name\":\"${DATABASE_SERVICE_NAME}\"},\"stringData\":{\"database-name\":\"${POSTGRESQL_DATABASE}\",\"database-password\":\"${POSTGRESQL_PASSWORD}\",\"database-user\":\"${POSTGRESQL_USER}\"}},{\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{\"template.openshift.io/expose-uri\":\"postgres://{.spec.clusterIP}:{.spec.ports[?(.name==\\\"postgresql\\\")].port}\"},\"name\":\"${DATABASE_SERVICE_NAME}\"},\"spec\":{\"ports\":[{\"name\":\"postgresql\",\"nodePort\":0,\"port\":5432,\"protocol\":\"TCP\",\"targetPort\":5432}],\"selector\":{\"name\":\"${DATABASE_SERVICE_NAME}\"},\"sessionAffinity\":\"None\",\"type\":\"ClusterIP\"},\"status\":{\"loadBalancer\":{}}},{\"apiVersion\":\"v1\",\"kind\":\"PersistentVolumeClaim\",\"metadata\":{\"name\":\"${DATABASE_SERVICE_NAME}\"},\"spec\":{\"accessModes\":[\"ReadWriteOnce\"],\"resources\":{\"requests\":{\"storage\":\"${VOLUME_CAPACITY}\"}}}},{\"apiVersion\":\"v1\",\"kind\":\"DeploymentConfig\",\"metadata\":{\"annotations\":{\"template.alpha.openshift.io/wait-for-ready\":\"true\"},\"name\":\"${DATABASE_SERVICE_NAME}\"},\"spec\":{\"replicas\":1,\"selector\":{\"name\":\"${DATABASE_SERVICE_NAME}\"},\"strategy\":{\"type\":\"Recreate\"},\"template\":{\"metadata\":{\"labels\":{\"name\":\"${DATABASE_SERVICE_NAME}\"}},\"spec\":{\"containers\":[{\"capabilities\":{},\"env\":[{\"name\":\"POSTGRESQL_USER\",\"valueFrom\":{\"secretKeyRef\":{\"key\":\"database-user\",\"name\":\"${DATABASE_SERVICE_NAME}\"}}},{\"name\":\"POSTGRESQL_PASSWORD\",\"valueFrom\":{\"secretKeyRef\":{\"key\":\"database-password\",\"name\":\"${DATABASE_SERVICE_NAME}\"}}},{\"name\":\"POSTGRESQL_DATABASE\",\"valueFrom\":{\"secretKeyRef\":{\"key\":\"database-name\",\"name\":\"${DATABASE_SERVICE_NAME}\"}}}],\"image\":\" \",\"imagePullPolicy\":\"IfNotPresent\",\"livenessProbe\":{\"exec\":{\"command\":[\"/usr/libexec/check-container\",\"--live\"]},\"initialDelaySeconds\":120,\"timeoutSeconds\":10},\"name\":\"postgresql\",\"ports\":[{\"containerPort\":5432,\"protocol\":\"TCP\"}],\"readinessProbe\":{\"exec\":{\"command\":[\"/usr/libexec/check-container\"]},\"initialDelaySeconds\":5,\"timeoutSeconds\":1},\"resources\":{\"limits\":{\"memory\":\"${MEMORY_LIMIT}\"}},\"securityContext\":{\"capabilities\":{},\"privileged\":false},\"terminationMessagePath\":\"/dev/termination-log\",\"volumeMounts\":[{\"mountPath\":\"/var/lib/pgsql/data\",\"name\":\"${DATABASE_SERVICE_NAME}-data\"}]}],\"dnsPolicy\":\"ClusterFirst\",\"restartPolicy\":\"Always\",\"volumes\":[{\"name\":\"${DATABASE_SERVICE_NAME}-data\",\"persistentVolumeClaim\":{\"claimName\":\"${DATABASE_SERVICE_NAME}\"}}]}},\"triggers\":[{\"imageChangeParams\":{\"automatic\":true,\"containerNames\":[\"postgresql\"],\"from\":{\"kind\":\"ImageStreamTag\",\"name\":\"postgresql:${POSTGRESQL_VERSION}\",\"namespace\":\"${NAMESPACE}\"},\"lastTriggeredImage\":\"\"},\"type\":\"ImageChange\"},{\"type\":\"ConfigChange\"}]},\"status\":{}}],\"parameters\":[{\"description\":\"Maximum amount of memory the container can use.\",\"displayName\":\"Memory Limit\",\"name\":\"MEMORY_LIMIT\",\"required\":true,\"value\":\"512Mi\"},{\"description\":\"The OpenShift Namespace where the ImageStream resides.\",\"displayName\":\"Namespace\",\"name\":\"NAMESPACE\",\"value\":\"openshift\"},{\"description\":\"The name of the OpenShift Service exposed for the database.\",\"displayName\":\"Database Service Name\",\"name\":\"DATABASE_SERVICE_NAME\",\"required\":true,\"value\":\"postgresql\"},{\"description\":\"Username for PostgreSQL user that will be used for accessing the database.\",\"displayName\":\"PostgreSQL Connection Username\",\"from\":\"user[A-Z0-9]{3}\",\"generate\":\"expression\",\"name\":\"POSTGRESQL_USER\",\"required\":true},{\"description\":\"Password for the PostgreSQL connection user.\",\"displayName\":\"PostgreSQL Connection Password\",\"from\":\"[a-zA-Z0-9]{16}\",\"generate\":\"expression\",\"name\":\"POSTGRESQL_PASSWORD\",\"required\":true},{\"description\":\"Name of the PostgreSQL database accessed.\",\"displayName\":\"PostgreSQL Database Name\",\"name\":\"POSTGRESQL_DATABASE\",\"required\":true,\"value\":\"sampledb\"},{\"description\":\"Volume space available for data, e.g. 512Mi, 2Gi.\",\"displayName\":\"Volume Capacity\",\"name\":\"VOLUME_CAPACITY\",\"required\":true,\"value\":\"1Gi\"},{\"description\":\"Version of PostgreSQL image to be used (9.4, 9.5, 9.6 or latest).\",\"displayName\":\"Version of PostgreSQL Image\",\"name\":\"POSTGRESQL_VERSION\",\"required\":true,\"value\":\"9.6\"}]} openshift.io/display-name=PostgreSQL openshift.io/documentation-url=https://docs.openshift.org/latest/using_images/db_images/postgresql.html openshift.io/long-description=This template provides a standalone PostgreSQL server with a database created. The database is stored on persistent storage. The database name, username, and password are chosen via parameters when provisioning this service. openshift.io/provider-display-name=Red Hat, Inc. openshift.io/support-url=https://access.redhat.com tags=database,postgresql Parameters: Name: MEMORY_LIMIT Display Name: Memory Limit Description: Maximum amount of memory the container can use. Required: true Value: 512Mi Name: NAMESPACE Display Name: Namespace Description: The OpenShift Namespace where the ImageStream resides. Required: false Value: openshift Name: DATABASE_SERVICE_NAME Display Name: Database Service Name Description: The name of the OpenShift Service exposed for the database. Required: true Value: postgresql Name: POSTGRESQL_USER Display Name: PostgreSQL Connection Username Description: Username for PostgreSQL user that will be used for accessing the database. Required: true Generated: expression From: user[A-Z0-9]{3} Name: POSTGRESQL_PASSWORD Display Name: PostgreSQL Connection Password Description: Password for the PostgreSQL connection user. Required: true Generated: expression From: [a-zA-Z0-9]{16} Name: POSTGRESQL_DATABASE Display Name: PostgreSQL Database Name Description: Name of the PostgreSQL database accessed. Required: true Value: sampledb Name: VOLUME_CAPACITY Display Name: Volume Capacity Description: Volume space available for data, e.g. 512Mi, 2Gi. Required: true Value: 1Gi Name: POSTGRESQL_VERSION Display Name: Version of PostgreSQL Image Description: Version of PostgreSQL image to be used (9.4, 9.5, 9.6 or latest). Required: true Value: 9.6 Object Labels: template=postgresql-persistent-template Message: The following service(s) have been created in your project: ${DATABASE_SERVICE_NAME}. Username: ${POSTGRESQL_USER} Password: ${POSTGRESQL_PASSWORD} Database Name: ${POSTGRESQL_DATABASE} Connection URL: postgresql://${DATABASE_SERVICE_NAME}:5432/ For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/postgresql-container/. Objects: Secret ${DATABASE_SERVICE_NAME} Service ${DATABASE_SERVICE_NAME} PersistentVolumeClaim ${DATABASE_SERVICE_NAME} DeploymentConfig ${DATABASE_SERVICE_NAME} oc new-app --name=db --template=postgresql-persistent -p DATABASE_SERVICE_NAME=db -p POSTGRESQL_USER=postgres -p POSTGRESQL_PASSWORD=password -p POSTGRESQL_DATABASE=postgres [output] --> Deploying template \"openshift/postgresql-persistent\" to project instavote PostgreSQL --------- PostgreSQL database service, with persistent storage. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/postgresql-container/. NOTE: Scaling to more than one replica is not supported. You must have persistent volumes available in your cluster to use this template. The following service(s) have been created in your project: db. Username: postgres Password: password Database Name: postgres Connection URL: postgresql://db:5432/ For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/postgresql-container/. * With parameters: * Memory Limit=512Mi * Namespace=openshift * Database Service Name=db * PostgreSQL Connection Username=postgres * PostgreSQL Connection Password=password * PostgreSQL Database Name=postgres * Volume Capacity=1Gi * Version of PostgreSQL Image=9.6 --> Creating resources ... secret \"db\" created service \"db\" created persistentvolumeclaim \"db\" created deploymentconfig \"db\" created --> Success Application is not exposed. You can expose services to the outside world by executing one or more of the commands below: 'oc expose svc/db' Run 'oc status' to view your app. Using docker strategy with oc new-app to deploy worker app In this section we are talking about docker Registry with oc new-app to deploy worker application. We have already deployed vote, result and db application. worker application are java application. Use the following command. oc new-app --name=worker https://github.com/initcron/example-voting-app.git --context-dir=worker [output] error: multiple images or templates matched \"jee\": 7 The argument \"jee\" could apply to the following Docker images, OpenShift image streams, or templates: * Image stream \"wildfly\" (tag \"12.0\") in project \"openshift\" Use --image-stream=\"openshift/wildfly:12.0\" to specify this image or template * Image stream \"wildfly\" (tag \"8.1\") in project \"openshift\" Use --image-stream=\"openshift/wildfly:8.1\" to specify this image or template * Image stream \"wildfly\" (tag \"9.0\") in project \"openshift\" Use --image-stream=\"openshift/wildfly:9.0\" to specify this image or template * Image stream \"wildfly\" (tag \"latest\") in project \"openshift\" Use --image-stream=\"openshift/wildfly:latest\" to specify this image or template * Image stream \"wildfly\" (tag \"10.0\") in project \"openshift\" Use --image-stream=\"openshift/wildfly:10.0\" to specify this image or template To view a full list of matches, use 'oc new-app -S jee' oc new-app --name=worker https://github.com/initcron/example-voting-app.git#openshift --context-dir=worker [output] --> Found Docker image 991f779 (22 months old) from Docker Hub for \"schoolofdevops/maven\" * An image stream will be created as \"maven:latest\" that will track the source image * A Docker build using source code from https://github.com/initcron/example-voting-app.git#openshift will be created * The resulting image will be pushed to image stream \"worker:latest\" * Every time \"maven:latest\" changes a new build will be triggered * This image will be deployed in deployment config \"worker\" * The image does not expose any ports - if you want to load balance or send traffic to this component you will need to create a service with 'expose dc/worker --port=[port]' later * WARNING: Image \"schoolofdevops/maven\" runs as the 'root' user which may not be permitted by your cluster administrator --> Creating resources ... imagestream \"maven\" created imagestream \"worker\" created buildconfig \"worker\" created deploymentconfig \"worker\" created --> Success Build scheduled, use 'oc logs -f bc/worker' to track its progress. Run 'oc status' to view your app. Rebuilding from changes to local repository In the previous section we deployed java worker application but database not connected . In this section we will rebuild application. use following command for start build. oc start-build worker --from=example-voteing-app","title":"Deploy app from source image"},{"location":"deploy_app_from_source_image/#deploying-apps-from-source-template-dockerfile-s2i-buildersbuild-configs","text":"","title":"Deploying apps from Source, Template,  Dockerfile - S2I Builders,Build Configs"},{"location":"deploy_app_from_source_image/#launching-demo-app-with-source-strategy","text":"In this section we are talking about how to deploy application using source. We have already deployed vote frontend application. In this section we are deploying result application and database. We will start with result application which is nodeJs application. First we deploy sample devops demo application which php application for practice. we will deploy this application using oc cli. oc new-app https://github.com/devopsdemoapps/devops-demo-app.git [output] --> Found image 9dd8c80 (2 weeks old) in image stream \"openshift/php\" under tag \"7.1\" for \"php\" Apache 2.4 with PHP 7.1 ----------------------- PHP 7.1 available as container is a base platform for building and running various PHP 7.1 applications and frameworks. PHP is an HTML-embedded scripting language. PHP attempts to make it easy for developers to write dynamically generated web pages. PHP also offers built-in database integration for several commercial and non-commercial database management systems, so writing a database-enabled webpage with PHP is fairly simple. The most common use of PHP coding is probably as a replacement for CGI scripts. Tags: builder, php, php71, rh-php71 * The source repository appears to match: php * A source build using source code from https://github.com/devopsdemoapps/devops-demo-app.git will be created * The resulting image will be pushed to image stream \"devops-demo-app:latest\" * Use 'start-build' to trigger a new build * This image will be deployed in deployment config \"devops-demo-app\" * Ports 8080/tcp, 8443/tcp will be load balanced by service \"devops-demo-app\" * Other containers can access this service through the hostname \"devops-demo-app\" --> Creating resources ... imagestream \"devops-demo-app\" created buildconfig \"devops-demo-app\" created deploymentconfig \"devops-demo-app\" created service \"devops-demo-app\" created --> Success Build scheduled, use 'oc logs -f bc/devops-demo-app' to track its progress. Application is not exposed. You can expose services to the outside world by executing one or more of the commands below: 'oc expose svc/devops-demo-app' Run 'oc status' to view your app. we need to expose that application using following command. oc expose svc/devops-demo-app [output] route \"devops-demo-app\" exposed check the status of application oc status [output] In project instavote app (instavote) on server https://128.199.213.193:8443 http://devops-demo-app-instavote.128.199.213.193.nip.io to pod port 8080-tcp (svc/devops-demo-app) dc/devops-demo-app deploys istag/devops-demo-app:latest <- bc/devops-demo-app source builds https://github.com/devopsdemoapps/devops-demo-app.git on openshift/php:7.1 build #1 failed about a minute ago - 5eb913a: v2 (Gourav Shah <gs@initcron.org>) deployment #1 waiting on image or update svc/redis - 172.30.57.198:6379 dc/redis deploys istag/redis:alpine deployment #2 deployed 14 minutes ago - 1 pod deployment #1 failed 2 hours ago: config change http://vote-instavote.128.199.213.193.nip.io to pod port 8080 (svc/vote) dc/vote deploys istag/vote:v1 deployment #2 deployed 14 minutes ago - 4 pods deployment #1 failed 2 hours ago: config change","title":"Launching demo app with source strategy"},{"location":"deploy_app_from_source_image/#s2i-builder-primer","text":"In this we are talking about how to builds nodeJs application that is result. We are also talking about openshift Builder. search nodeJs stream in openshift. oc new-app --search nodejs [output] Templates (oc new-app --template=<template>) ----- nodejs-mongo-persistent Project: openshift An example Node.js application with a MongoDB database. For more information about using this template, including OpenShift considerations, see https://github.com/openshift/nodejs-ex/blob/master/README.md. Image streams (oc new-app --image-stream=<image-stream> [--code=<source>]) ----- nodejs Project: openshift Tags: 6, 8, latest oc get is [ouput] NAME DOCKER REPO TAGS UPDATED redis 172.30.1.1:5000/instavote/redis alpine 22 hours ago vote 172.30.1.1:5000/instavote/vote v1 23 hours ago list of image stream using namespace openshift oc get is -n openshift [output] NAME DOCKER REPO TAGS UPDATED dotnet 172.30.1.1:5000/openshift/dotnet 2.0,latest 11 days ago httpd 172.30.1.1:5000/openshift/httpd 2.4,latest 11 days ago jenkins 172.30.1.1:5000/openshift/jenkins 1,2,latest 11 days ago mariadb 172.30.1.1:5000/openshift/mariadb latest,10.1,10.2 11 days ago mongodb 172.30.1.1:5000/openshift/mongodb 3.2,3.4,latest + 2 more... 11 days ago mysql 172.30.1.1:5000/openshift/mysql 5.5,5.6,5.7 + 1 more... 11 days ago nginx 172.30.1.1:5000/openshift/nginx 1.10,1.12,1.8 + 1 more... 11 days ago nodejs 172.30.1.1:5000/openshift/nodejs 0.10,4,6 + 2 more... 11 days ago perl 172.30.1.1:5000/openshift/perl 5.16,5.20,5.24 + 1 more... 11 days ago php 172.30.1.1:5000/openshift/php 5.5,5.6,7.0 + 2 more... 11 days ago postgresql 172.30.1.1:5000/openshift/postgresql 9.6,latest,9.2 + 2 more... 11 days ago python 172.30.1.1:5000/openshift/python 2.7,3.3,3.4 + 3 more... 11 days ago redis 172.30.1.1:5000/openshift/redis latest,3.2 11 days ago ruby 172.30.1.1:5000/openshift/ruby 2.3,2.4,2.5 + 3 more... 11 days ago wildfly 172.30.1.1:5000/openshift/wildfly latest,10.0,10.1 + 4 more... 11 days ago","title":"S2I Builder Primer"},{"location":"deploy_app_from_source_image/#deploying-results-app-with-nodejs-s2i-builder","text":"In the section we are deploy node application using custom builder from the images stream nodeJs and tag 8. Search nodejs image using following command. oc new-app --search nodejs [output] Templates (oc new-app --template=<template>) ----- nodejs-mongo-persistent Project: openshift An example Node.js application with a MongoDB database. For more information about using this template, including OpenShift considerations, see https://github.com/openshift/nodejs-ex/blob/master/README.md. Image streams (oc new-app --image-stream=<image-stream> [--code=<source>]) ----- nodejs Project: openshift Tags: 6, 8, latest Then, Deploy the result application. oc new-app nodejs:8~https://github.com/initcron/example-voting-app.git --context-dir='result' --name=result [output] --> Found image a603093 (2 weeks old) in image stream \"openshift/nodejs\" under tag \"8\" for \"nodejs:8\" Node.js 8 --------- Node.js 8 available as container is a base platform for building and running various Node.js 8 applications and frameworks. Node.js is a platform built on Chrome's JavaScript runtime for easily building fast, scalable network applications. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient, perfect for data-intensive real-time applications that run across distributed devices. Tags: builder, nodejs, nodejs8 * A source build using source code from https://github.com/initcron/example-voting-app.git will be created * The resulting image will be pushed to image stream \"result:latest\" * Use 'start-build' to trigger a new build * This image will be deployed in deployment config \"result\" * Port 8080/tcp will be load balanced by service \"result\" * Other containers can access this service through the hostname \"result\" --> Creating resources ... imagestream \"result\" created buildconfig \"result\" created deploymentconfig \"result\" created service \"result\" created --> Success Build scheduled, use 'oc logs -f bc/result' to track its progress. Application is not exposed. You can expose services to the outside world by executing one or more of the commands below: 'oc expose svc/result' Run 'oc status' to view your app. Then, Expose the services oc expose svc/result [output] route \"result\" exposed show the build in oc including oc get bc [output] NAME TYPE FROM LATEST devops-demo-app Source Git 1 result Source Git 1","title":"Deploying results app with Nodejs S2I Builder"},{"location":"deploy_app_from_source_image/#launching-db-with-a-template-and-parameters","text":"In this section we are talking about result application database. We are connecting database to the result application. Database is postgresql which result connect. We will use the Template for postgresql. Just search postgresql database on openshift using following command. oc new-app --search postgres [output] Templates (oc new-app --template=<template>) ----- postgresql-persistent Project: openshift PostgreSQL database service, with persistent storage. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/postgresql-container/. NOTE: Scaling to more than one replica is not supported. You must have persistent volumes available in your cluster to use this template. Image streams (oc new-app --image-stream=<image-stream> [--code=<source>]) ----- postgresql Project: openshift Tags: 9.5, 9.6, latest Docker images (oc new-app --docker-image=<docker-image> [--code=<source>]) ----- postgres Registry: Docker Hub Tags: latest The , See available template in openshit. oc get templates -n openshift [output] NAME DESCRIPTION PARAMETERS OBJECTS cakephp-mysql-persistent An example CakePHP application with a MySQL database. For more information ab... 20 (4 blank) 9 dancer-mysql-persistent An example Dancer application with a MySQL database. For more information abo... 17 (5 blank) 9 django-psql-persistent An example Django application with a PostgreSQL database. For more informatio... 20 (5 blank) 9 jenkins-ephemeral Jenkins service, without persistent storage.... 7 (all set) 6 jenkins-pipeline-example This example showcases the new Jenkins Pipeline integration in OpenShift,... 16 (4 blank) 8 mariadb-persistent MariaDB database service, with persistent storage. For more information about... 9 (3 generated) 4 mongodb-persistent MongoDB database service, with persistent storage. For more information about... 9 (3 generated) 4 mysql-persistent MySQL database service, with persistent storage. For more information about u... 9 (3 generated) 4 nodejs-mongo-persistent An example Node.js application with a MongoDB database. For more information... 19 (4 blank) 9 postgresql-persistent PostgreSQL database service, with persistent storage. For more information ab... 8 (2 generated) 4 rails-pgsql-persistent An example Rails application with a PostgreSQL database. For more information... 21 (4 blank) 9 describe postgres templates oc describe template postgresql-persistent -n openshift [output] Name: postgresql-persistent Namespace: openshift Created: 11 days ago Labels: <none> Description: PostgreSQL database service, with persistent storage. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/postgresql-container/. NOTE: Scaling to more than one replica is not supported. You must have persistent volumes available in your cluster to use this template. Annotations: iconClass=icon-postgresql kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"template.openshift.io/v1\",\"kind\":\"Template\",\"labels\":{\"template\":\"postgresql-persistent-template\"},\"message\":\"The following service(s) have been created in your project: ${DATABASE_SERVICE_NAME}.\\n\\n Username: ${POSTGRESQL_USER}\\n Password: ${POSTGRESQL_PASSWORD}\\n Database Name: ${POSTGRESQL_DATABASE}\\n Connection URL: postgresql://${DATABASE_SERVICE_NAME}:5432/\\n\\nFor more information about using this template, including OpenShift considerations, see https://github.com/sclorg/postgresql-container/.\",\"metadata\":{\"annotations\":{\"description\":\"PostgreSQL database service, with persistent storage. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/postgresql-container/.\\n\\nNOTE: Scaling to more than one replica is not supported. You must have persistent volumes available in your cluster to use this template.\",\"iconClass\":\"icon-postgresql\",\"openshift.io/display-name\":\"PostgreSQL\",\"openshift.io/documentation-url\":\"https://docs.openshift.org/latest/using_images/db_images/postgresql.html\",\"openshift.io/long-description\":\"This template provides a standalone PostgreSQL server with a database created. The database is stored on persistent storage. The database name, username, and password are chosen via parameters when provisioning this service.\",\"openshift.io/provider-display-name\":\"Red Hat, Inc.\",\"openshift.io/support-url\":\"https://access.redhat.com\",\"tags\":\"database,postgresql\"},\"name\":\"postgresql-persistent\",\"namespace\":\"openshift\"},\"objects\":[{\"apiVersion\":\"v1\",\"kind\":\"Secret\",\"metadata\":{\"annotations\":{\"template.openshift.io/expose-database_name\":\"{.data['database-name']}\",\"template.openshift.io/expose-password\":\"{.data['database-password']}\",\"template.openshift.io/expose-username\":\"{.data['database-user']}\"},\"name\":\"${DATABASE_SERVICE_NAME}\"},\"stringData\":{\"database-name\":\"${POSTGRESQL_DATABASE}\",\"database-password\":\"${POSTGRESQL_PASSWORD}\",\"database-user\":\"${POSTGRESQL_USER}\"}},{\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{\"template.openshift.io/expose-uri\":\"postgres://{.spec.clusterIP}:{.spec.ports[?(.name==\\\"postgresql\\\")].port}\"},\"name\":\"${DATABASE_SERVICE_NAME}\"},\"spec\":{\"ports\":[{\"name\":\"postgresql\",\"nodePort\":0,\"port\":5432,\"protocol\":\"TCP\",\"targetPort\":5432}],\"selector\":{\"name\":\"${DATABASE_SERVICE_NAME}\"},\"sessionAffinity\":\"None\",\"type\":\"ClusterIP\"},\"status\":{\"loadBalancer\":{}}},{\"apiVersion\":\"v1\",\"kind\":\"PersistentVolumeClaim\",\"metadata\":{\"name\":\"${DATABASE_SERVICE_NAME}\"},\"spec\":{\"accessModes\":[\"ReadWriteOnce\"],\"resources\":{\"requests\":{\"storage\":\"${VOLUME_CAPACITY}\"}}}},{\"apiVersion\":\"v1\",\"kind\":\"DeploymentConfig\",\"metadata\":{\"annotations\":{\"template.alpha.openshift.io/wait-for-ready\":\"true\"},\"name\":\"${DATABASE_SERVICE_NAME}\"},\"spec\":{\"replicas\":1,\"selector\":{\"name\":\"${DATABASE_SERVICE_NAME}\"},\"strategy\":{\"type\":\"Recreate\"},\"template\":{\"metadata\":{\"labels\":{\"name\":\"${DATABASE_SERVICE_NAME}\"}},\"spec\":{\"containers\":[{\"capabilities\":{},\"env\":[{\"name\":\"POSTGRESQL_USER\",\"valueFrom\":{\"secretKeyRef\":{\"key\":\"database-user\",\"name\":\"${DATABASE_SERVICE_NAME}\"}}},{\"name\":\"POSTGRESQL_PASSWORD\",\"valueFrom\":{\"secretKeyRef\":{\"key\":\"database-password\",\"name\":\"${DATABASE_SERVICE_NAME}\"}}},{\"name\":\"POSTGRESQL_DATABASE\",\"valueFrom\":{\"secretKeyRef\":{\"key\":\"database-name\",\"name\":\"${DATABASE_SERVICE_NAME}\"}}}],\"image\":\" \",\"imagePullPolicy\":\"IfNotPresent\",\"livenessProbe\":{\"exec\":{\"command\":[\"/usr/libexec/check-container\",\"--live\"]},\"initialDelaySeconds\":120,\"timeoutSeconds\":10},\"name\":\"postgresql\",\"ports\":[{\"containerPort\":5432,\"protocol\":\"TCP\"}],\"readinessProbe\":{\"exec\":{\"command\":[\"/usr/libexec/check-container\"]},\"initialDelaySeconds\":5,\"timeoutSeconds\":1},\"resources\":{\"limits\":{\"memory\":\"${MEMORY_LIMIT}\"}},\"securityContext\":{\"capabilities\":{},\"privileged\":false},\"terminationMessagePath\":\"/dev/termination-log\",\"volumeMounts\":[{\"mountPath\":\"/var/lib/pgsql/data\",\"name\":\"${DATABASE_SERVICE_NAME}-data\"}]}],\"dnsPolicy\":\"ClusterFirst\",\"restartPolicy\":\"Always\",\"volumes\":[{\"name\":\"${DATABASE_SERVICE_NAME}-data\",\"persistentVolumeClaim\":{\"claimName\":\"${DATABASE_SERVICE_NAME}\"}}]}},\"triggers\":[{\"imageChangeParams\":{\"automatic\":true,\"containerNames\":[\"postgresql\"],\"from\":{\"kind\":\"ImageStreamTag\",\"name\":\"postgresql:${POSTGRESQL_VERSION}\",\"namespace\":\"${NAMESPACE}\"},\"lastTriggeredImage\":\"\"},\"type\":\"ImageChange\"},{\"type\":\"ConfigChange\"}]},\"status\":{}}],\"parameters\":[{\"description\":\"Maximum amount of memory the container can use.\",\"displayName\":\"Memory Limit\",\"name\":\"MEMORY_LIMIT\",\"required\":true,\"value\":\"512Mi\"},{\"description\":\"The OpenShift Namespace where the ImageStream resides.\",\"displayName\":\"Namespace\",\"name\":\"NAMESPACE\",\"value\":\"openshift\"},{\"description\":\"The name of the OpenShift Service exposed for the database.\",\"displayName\":\"Database Service Name\",\"name\":\"DATABASE_SERVICE_NAME\",\"required\":true,\"value\":\"postgresql\"},{\"description\":\"Username for PostgreSQL user that will be used for accessing the database.\",\"displayName\":\"PostgreSQL Connection Username\",\"from\":\"user[A-Z0-9]{3}\",\"generate\":\"expression\",\"name\":\"POSTGRESQL_USER\",\"required\":true},{\"description\":\"Password for the PostgreSQL connection user.\",\"displayName\":\"PostgreSQL Connection Password\",\"from\":\"[a-zA-Z0-9]{16}\",\"generate\":\"expression\",\"name\":\"POSTGRESQL_PASSWORD\",\"required\":true},{\"description\":\"Name of the PostgreSQL database accessed.\",\"displayName\":\"PostgreSQL Database Name\",\"name\":\"POSTGRESQL_DATABASE\",\"required\":true,\"value\":\"sampledb\"},{\"description\":\"Volume space available for data, e.g. 512Mi, 2Gi.\",\"displayName\":\"Volume Capacity\",\"name\":\"VOLUME_CAPACITY\",\"required\":true,\"value\":\"1Gi\"},{\"description\":\"Version of PostgreSQL image to be used (9.4, 9.5, 9.6 or latest).\",\"displayName\":\"Version of PostgreSQL Image\",\"name\":\"POSTGRESQL_VERSION\",\"required\":true,\"value\":\"9.6\"}]} openshift.io/display-name=PostgreSQL openshift.io/documentation-url=https://docs.openshift.org/latest/using_images/db_images/postgresql.html openshift.io/long-description=This template provides a standalone PostgreSQL server with a database created. The database is stored on persistent storage. The database name, username, and password are chosen via parameters when provisioning this service. openshift.io/provider-display-name=Red Hat, Inc. openshift.io/support-url=https://access.redhat.com tags=database,postgresql Parameters: Name: MEMORY_LIMIT Display Name: Memory Limit Description: Maximum amount of memory the container can use. Required: true Value: 512Mi Name: NAMESPACE Display Name: Namespace Description: The OpenShift Namespace where the ImageStream resides. Required: false Value: openshift Name: DATABASE_SERVICE_NAME Display Name: Database Service Name Description: The name of the OpenShift Service exposed for the database. Required: true Value: postgresql Name: POSTGRESQL_USER Display Name: PostgreSQL Connection Username Description: Username for PostgreSQL user that will be used for accessing the database. Required: true Generated: expression From: user[A-Z0-9]{3} Name: POSTGRESQL_PASSWORD Display Name: PostgreSQL Connection Password Description: Password for the PostgreSQL connection user. Required: true Generated: expression From: [a-zA-Z0-9]{16} Name: POSTGRESQL_DATABASE Display Name: PostgreSQL Database Name Description: Name of the PostgreSQL database accessed. Required: true Value: sampledb Name: VOLUME_CAPACITY Display Name: Volume Capacity Description: Volume space available for data, e.g. 512Mi, 2Gi. Required: true Value: 1Gi Name: POSTGRESQL_VERSION Display Name: Version of PostgreSQL Image Description: Version of PostgreSQL image to be used (9.4, 9.5, 9.6 or latest). Required: true Value: 9.6 Object Labels: template=postgresql-persistent-template Message: The following service(s) have been created in your project: ${DATABASE_SERVICE_NAME}. Username: ${POSTGRESQL_USER} Password: ${POSTGRESQL_PASSWORD} Database Name: ${POSTGRESQL_DATABASE} Connection URL: postgresql://${DATABASE_SERVICE_NAME}:5432/ For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/postgresql-container/. Objects: Secret ${DATABASE_SERVICE_NAME} Service ${DATABASE_SERVICE_NAME} PersistentVolumeClaim ${DATABASE_SERVICE_NAME} DeploymentConfig ${DATABASE_SERVICE_NAME} oc new-app --name=db --template=postgresql-persistent -p DATABASE_SERVICE_NAME=db -p POSTGRESQL_USER=postgres -p POSTGRESQL_PASSWORD=password -p POSTGRESQL_DATABASE=postgres [output] --> Deploying template \"openshift/postgresql-persistent\" to project instavote PostgreSQL --------- PostgreSQL database service, with persistent storage. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/postgresql-container/. NOTE: Scaling to more than one replica is not supported. You must have persistent volumes available in your cluster to use this template. The following service(s) have been created in your project: db. Username: postgres Password: password Database Name: postgres Connection URL: postgresql://db:5432/ For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/postgresql-container/. * With parameters: * Memory Limit=512Mi * Namespace=openshift * Database Service Name=db * PostgreSQL Connection Username=postgres * PostgreSQL Connection Password=password * PostgreSQL Database Name=postgres * Volume Capacity=1Gi * Version of PostgreSQL Image=9.6 --> Creating resources ... secret \"db\" created service \"db\" created persistentvolumeclaim \"db\" created deploymentconfig \"db\" created --> Success Application is not exposed. You can expose services to the outside world by executing one or more of the commands below: 'oc expose svc/db' Run 'oc status' to view your app.","title":"Launching db with a template and parameters"},{"location":"deploy_app_from_source_image/#using-docker-strategy-with-oc-new-app-to-deploy-worker-app","text":"In this section we are talking about docker Registry with oc new-app to deploy worker application. We have already deployed vote, result and db application. worker application are java application. Use the following command. oc new-app --name=worker https://github.com/initcron/example-voting-app.git --context-dir=worker [output] error: multiple images or templates matched \"jee\": 7 The argument \"jee\" could apply to the following Docker images, OpenShift image streams, or templates: * Image stream \"wildfly\" (tag \"12.0\") in project \"openshift\" Use --image-stream=\"openshift/wildfly:12.0\" to specify this image or template * Image stream \"wildfly\" (tag \"8.1\") in project \"openshift\" Use --image-stream=\"openshift/wildfly:8.1\" to specify this image or template * Image stream \"wildfly\" (tag \"9.0\") in project \"openshift\" Use --image-stream=\"openshift/wildfly:9.0\" to specify this image or template * Image stream \"wildfly\" (tag \"latest\") in project \"openshift\" Use --image-stream=\"openshift/wildfly:latest\" to specify this image or template * Image stream \"wildfly\" (tag \"10.0\") in project \"openshift\" Use --image-stream=\"openshift/wildfly:10.0\" to specify this image or template To view a full list of matches, use 'oc new-app -S jee' oc new-app --name=worker https://github.com/initcron/example-voting-app.git#openshift --context-dir=worker [output] --> Found Docker image 991f779 (22 months old) from Docker Hub for \"schoolofdevops/maven\" * An image stream will be created as \"maven:latest\" that will track the source image * A Docker build using source code from https://github.com/initcron/example-voting-app.git#openshift will be created * The resulting image will be pushed to image stream \"worker:latest\" * Every time \"maven:latest\" changes a new build will be triggered * This image will be deployed in deployment config \"worker\" * The image does not expose any ports - if you want to load balance or send traffic to this component you will need to create a service with 'expose dc/worker --port=[port]' later * WARNING: Image \"schoolofdevops/maven\" runs as the 'root' user which may not be permitted by your cluster administrator --> Creating resources ... imagestream \"maven\" created imagestream \"worker\" created buildconfig \"worker\" created deploymentconfig \"worker\" created --> Success Build scheduled, use 'oc logs -f bc/worker' to track its progress. Run 'oc status' to view your app.","title":"Using docker strategy with oc new-app to deploy worker app"},{"location":"deploy_app_from_source_image/#rebuilding-from-changes-to-local-repository","text":"In the previous section we deployed java worker application but database not connected . In this section we will rebuild application. use following command for start build. oc start-build worker --from=example-voteing-app","title":"Rebuilding from changes to local repository"},{"location":"deploying_pods/","text":"Deploying Pods Life of a pod Pending : in progress Running Succeeded : successfully exited Failed Unknown Probes livenessProbe : Containers are Alive readinessProbe : Ready to Serve Traffic Resource Configs Each entity created with Openshift is a resource including pod, service, deployments, replication controller etc. Resources can be defined as YAML or JSON. Here is the syntax to create a YAML specification. AKMS => Resource Configs Specs apiVersion: v1 kind: metadata: spec: Spec Schema: https://kubernetes.io/docs/user-guide/pods/multi-container/ Common Configurations Througout this tutorial, we would be deploying differnt components of example voting application. Lets assume we are deploying it in a dev environment. Lets create the common specs for this app with the AKMS schema discussed above. file: common.yml apiVersion: v1 kind: metadata: name: vote labels: app: vote role: ui tier: front spec: Lets now create the Pod config by adding the kind and specs to above schema. Filename: vote-pod.yaml apiVersion: v1 kind: Pod metadata: name: vote labels: app: vote role: ui tier: front spec: containers: - name: vote image: schoolofdevops/vote:latest ports: - containerPort: 80 Updating Security Context To allow containers created with images with root user, security context needs to be edited. By default openshift restricts running any container which runs as root. oc login -u system:admin oc edit scc restricted Change the runAsUser.Type strategy to RunAsAny. Launching and operating a Pod Syntax: oc apply -f FILE To Launch pod using configs above, oc apply -f vote-pod.yaml To view pods oc get pods oc get pods vote To get detailed info oc describe pods vote [Output:] Name: vote Namespace: default Node: kube-3/192.168.0.80 Start Time: Tue, 07 Feb 2017 16:16:40 +0000 Labels: app=voting Status: Running IP: 10.40.0.2 Controllers: <none> Containers: vote: Container ID: docker://48304b35b9457d627b341e424228a725d05c2ed97cc9970bbff32a1b365d9a5d Image: schoolofdevops/vote:latest Image ID: docker-pullable://schoolofdevops/vote@sha256:3d89bfc1993d4630a58b831a6d44ef73d2be76a7862153e02e7a7c0cf2936731 Port: 80/TCP State: Running Started: Tue, 07 Feb 2017 16:16:52 +0000 Ready: True Restart Count: 0 Volume Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-2n6j1 (ro) Environment Variables: <none> Conditions: Type Status Initialized True Ready True PodScheduled True Volumes: default-token-2n6j1: Type: Secret (a volume populated by a Secret) SecretName: default-token-2n6j1 QoS Class: BestEffort Tolerations: <none> Events: FirstSeen LastSeen Count From SubObjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 21s 21s 1 {default-scheduler } Normal Scheduled Successfully assigned vote to kube-3 20s 20s 1 {kubelet kube-3} spec.containers{vote} Normal Pulling pulling image \"schoolofdevops/vote:latest\" 10s 10s 1 {kubelet kube-3} spec.containers{vote} Normal Pulled Successfully pulled image \"schoolofdevops/vote:latest\" 9s 9s 1 {kubelet kube-3} spec.containers{vote} Normal Created Created container with docker id 48304b35b945; Security:[seccomp=unconfined] 9s 9s 1 {kubelet kube-3} spec.containers{vote} Normal Started Started container with docker id 48304b35b945 Commands to operate the pod oc exec -it vote ps sh oc exec -it vote sh oc logs vote delete oc delete pod vote oc get pods Attach a Volume to the Pod Lets create a pod for database and attach a volume to it. To achieve this we will need to create a volumes definition attach volume to container using VolumeMounts property Volumes are of two types: * emptyDir * hostPath File: db-pod.yaml apiVersion: v1 kind: Pod metadata: name: db labels: app: postgres role: database tier: back spec: containers: - name: db image: postgres:9.4 ports: - containerPort: 5432 volumeMounts: - name: db-data mountPath: /var/lib/postgresql/data volumes: - name: db-data emptyDir: {} To create this pod, oc apply -f db-pod.yaml oc describe pod db oc get events Selecting Node to run on oc get nodes --show-labels oc label nodes <node-name> rack=1 oc get nodes --show-labels Update pod definition with nodeSelector file: vote-pod.yml apiVersion: v1 kind: Pod metadata: name: vote labels: app: vote role: ui tier: front spec: containers: - name: vote image: schoolofdevops/vote:latest ports: - containerPort: 80 nodeSelector: rack: '1' For this change, pod needs to be re created. oc apply -f vote-pod.yaml Creating Multi Container Pods file: multi_container_pod.yml apiVersion: v1 kind: Pod metadata: name: web labels: app: nginx role: ui tier: front spec: containers: - name: nginx image: nginx ports: - containerPort: 80 volumeMounts: - name: data mountPath: /opt/d1 - name: loop image: schoolofdevops/loop volumeMounts: - name: data mountPath: /opt/d1 volumes: - name: data emptyDir: {} To create this pod oc apply -f multi_container_pod.yml Check Status root@kube-01:~# oc get pods NAME READY STATUS RESTARTS AGE nginx 0/2 ContainerCreating 0 7s vote 1/1 Running 0 3m Checking logs, logging in oc logs web -c synch oc logs web -c nginx oc exec -it web sh -c nginx oc exec -it web sh -c synch Exercise Create a pod definition for redis and deploy. Reading List : Node Selectors, Affinity https://kubernetes.io/docs/concepts/configuration/assign-pod-node/","title":"Launching Pods"},{"location":"deploying_pods/#deploying-pods","text":"Life of a pod Pending : in progress Running Succeeded : successfully exited Failed Unknown","title":"Deploying Pods"},{"location":"deploying_pods/#probes","text":"livenessProbe : Containers are Alive readinessProbe : Ready to Serve Traffic","title":"Probes"},{"location":"deploying_pods/#resource-configs","text":"Each entity created with Openshift is a resource including pod, service, deployments, replication controller etc. Resources can be defined as YAML or JSON. Here is the syntax to create a YAML specification. AKMS => Resource Configs Specs apiVersion: v1 kind: metadata: spec: Spec Schema: https://kubernetes.io/docs/user-guide/pods/multi-container/","title":"Resource Configs"},{"location":"deploying_pods/#common-configurations","text":"Througout this tutorial, we would be deploying differnt components of example voting application. Lets assume we are deploying it in a dev environment. Lets create the common specs for this app with the AKMS schema discussed above. file: common.yml apiVersion: v1 kind: metadata: name: vote labels: app: vote role: ui tier: front spec: Lets now create the Pod config by adding the kind and specs to above schema. Filename: vote-pod.yaml apiVersion: v1 kind: Pod metadata: name: vote labels: app: vote role: ui tier: front spec: containers: - name: vote image: schoolofdevops/vote:latest ports: - containerPort: 80","title":"Common Configurations"},{"location":"deploying_pods/#updating-security-context","text":"To allow containers created with images with root user, security context needs to be edited. By default openshift restricts running any container which runs as root. oc login -u system:admin oc edit scc restricted Change the runAsUser.Type strategy to RunAsAny.","title":"Updating Security Context"},{"location":"deploying_pods/#launching-and-operating-a-pod","text":"Syntax: oc apply -f FILE To Launch pod using configs above, oc apply -f vote-pod.yaml To view pods oc get pods oc get pods vote To get detailed info oc describe pods vote [Output:] Name: vote Namespace: default Node: kube-3/192.168.0.80 Start Time: Tue, 07 Feb 2017 16:16:40 +0000 Labels: app=voting Status: Running IP: 10.40.0.2 Controllers: <none> Containers: vote: Container ID: docker://48304b35b9457d627b341e424228a725d05c2ed97cc9970bbff32a1b365d9a5d Image: schoolofdevops/vote:latest Image ID: docker-pullable://schoolofdevops/vote@sha256:3d89bfc1993d4630a58b831a6d44ef73d2be76a7862153e02e7a7c0cf2936731 Port: 80/TCP State: Running Started: Tue, 07 Feb 2017 16:16:52 +0000 Ready: True Restart Count: 0 Volume Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-2n6j1 (ro) Environment Variables: <none> Conditions: Type Status Initialized True Ready True PodScheduled True Volumes: default-token-2n6j1: Type: Secret (a volume populated by a Secret) SecretName: default-token-2n6j1 QoS Class: BestEffort Tolerations: <none> Events: FirstSeen LastSeen Count From SubObjectPath Type Reason Message --------- -------- ----- ---- ------------- -------- ------ ------- 21s 21s 1 {default-scheduler } Normal Scheduled Successfully assigned vote to kube-3 20s 20s 1 {kubelet kube-3} spec.containers{vote} Normal Pulling pulling image \"schoolofdevops/vote:latest\" 10s 10s 1 {kubelet kube-3} spec.containers{vote} Normal Pulled Successfully pulled image \"schoolofdevops/vote:latest\" 9s 9s 1 {kubelet kube-3} spec.containers{vote} Normal Created Created container with docker id 48304b35b945; Security:[seccomp=unconfined] 9s 9s 1 {kubelet kube-3} spec.containers{vote} Normal Started Started container with docker id 48304b35b945 Commands to operate the pod oc exec -it vote ps sh oc exec -it vote sh oc logs vote delete oc delete pod vote oc get pods","title":"Launching and operating a Pod"},{"location":"deploying_pods/#attach-a-volume-to-the-pod","text":"Lets create a pod for database and attach a volume to it. To achieve this we will need to create a volumes definition attach volume to container using VolumeMounts property Volumes are of two types: * emptyDir * hostPath File: db-pod.yaml apiVersion: v1 kind: Pod metadata: name: db labels: app: postgres role: database tier: back spec: containers: - name: db image: postgres:9.4 ports: - containerPort: 5432 volumeMounts: - name: db-data mountPath: /var/lib/postgresql/data volumes: - name: db-data emptyDir: {} To create this pod, oc apply -f db-pod.yaml oc describe pod db oc get events","title":"Attach a Volume to the Pod"},{"location":"deploying_pods/#selecting-node-to-run-on","text":"oc get nodes --show-labels oc label nodes <node-name> rack=1 oc get nodes --show-labels Update pod definition with nodeSelector file: vote-pod.yml apiVersion: v1 kind: Pod metadata: name: vote labels: app: vote role: ui tier: front spec: containers: - name: vote image: schoolofdevops/vote:latest ports: - containerPort: 80 nodeSelector: rack: '1' For this change, pod needs to be re created. oc apply -f vote-pod.yaml","title":"Selecting Node to run on"},{"location":"deploying_pods/#creating-multi-container-pods","text":"file: multi_container_pod.yml apiVersion: v1 kind: Pod metadata: name: web labels: app: nginx role: ui tier: front spec: containers: - name: nginx image: nginx ports: - containerPort: 80 volumeMounts: - name: data mountPath: /opt/d1 - name: loop image: schoolofdevops/loop volumeMounts: - name: data mountPath: /opt/d1 volumes: - name: data emptyDir: {} To create this pod oc apply -f multi_container_pod.yml Check Status root@kube-01:~# oc get pods NAME READY STATUS RESTARTS AGE nginx 0/2 ContainerCreating 0 7s vote 1/1 Running 0 3m Checking logs, logging in oc logs web -c synch oc logs web -c nginx oc exec -it web sh -c nginx oc exec -it web sh -c synch","title":"Creating Multi Container Pods"},{"location":"deploying_pods/#exercise","text":"Create a pod definition for redis and deploy.","title":"Exercise"},{"location":"deploying_pods/#reading-list","text":"Node Selectors, Affinity https://kubernetes.io/docs/concepts/configuration/assign-pod-node/","title":"Reading List :"},{"location":"exposing_app_with_service/","text":"Exposing Application with a Service Types of Services: * ClusterIP * NodePort * LoadBalancer * ExternalName oc get pods oc get svc Sample Output: NAME READY STATUS RESTARTS AGE voting-appp-1j52x 1/1 Running 0 12m voting-appp-pr2xz 1/1 Running 0 9m voting-appp-qpxbm 1/1 Running 0 15m Filename: vote-svc.yaml --- apiVersion: v1 kind: Service metadata: labels: role: svc tier: front name: vote-svc namespace: dev spec: selector: app: vote ports: - port: 80 protocol: TCP targetPort: 80 type: NodePort Save the file. Now to create a service: oc create -f vote_svc.yaml oc get svc Now to check which port the pod is connected oc describe service vote-svc Check for the Nodeport here Sample Output Name: vote-svc Namespace: dev Labels: app=vote Selector: app=vote Type: NodePort IP: 10.99.147.158 Port: <unset> 80/TCP NodePort: <unset> 30308/TCP Endpoints: 10.40.0.2:80,10.40.0.3:80,10.40.0.4:80 + 1 more... Session Affinity: None No events. Go to browser and check hostip:NodePort Here the node port is 30308. Sample output will be:","title":"Service Endpoints"},{"location":"exposing_app_with_service/#exposing-application-with-a-service","text":"Types of Services: * ClusterIP * NodePort * LoadBalancer * ExternalName oc get pods oc get svc Sample Output: NAME READY STATUS RESTARTS AGE voting-appp-1j52x 1/1 Running 0 12m voting-appp-pr2xz 1/1 Running 0 9m voting-appp-qpxbm 1/1 Running 0 15m Filename: vote-svc.yaml --- apiVersion: v1 kind: Service metadata: labels: role: svc tier: front name: vote-svc namespace: dev spec: selector: app: vote ports: - port: 80 protocol: TCP targetPort: 80 type: NodePort Save the file. Now to create a service: oc create -f vote_svc.yaml oc get svc Now to check which port the pod is connected oc describe service vote-svc Check for the Nodeport here Sample Output Name: vote-svc Namespace: dev Labels: app=vote Selector: app=vote Type: NodePort IP: 10.99.147.158 Port: <unset> 80/TCP NodePort: <unset> 30308/TCP Endpoints: 10.40.0.2:80,10.40.0.3:80,10.40.0.4:80 + 1 more... Session Affinity: None No events. Go to browser and check hostip:NodePort Here the node port is 30308. Sample output will be:","title":"Exposing Application with  a Service"},{"location":"openshift-bootcamp/","text":"Openshift Bootcamp Duration 2days Level Intermediate, Advanced Modules 10 Flipped Class No Customizable Yes Objectives This course serves as a accelerator program to understand, setup and master openshift container platform for professionals who already have an understanding of docker and kubernetes. Who is this for ? This course is for someone who has already taken docker fundamentals, and kubernetes bootcamp courses/have equivalent knowledge, and would like to learn how to leverage Openshift Platform which is an abstraction on top of it. If you are a Operations/Systems personnel and would like to learn how to build a production grade scalable, fault tolerant and high available openshift infrastructure, and administer it, this course it for you. If you are a developer and would like to learn how to deploy your application stacks in production, on top of paas solution and also understand the underlying primitives, this course is for you. You could be developer/operations personnel and be in charge of securing application infrastructure and setup auxiliary services such as monitoring, centralized logging etc. this course is for you. Who is this not for ? If you are a advanced user of Openshift already, this course is definitely not for you. If you are interested in learning docker/container orchestration on windows , this course is not ideal for you as it focuses on linux containers. This is mix course for both developers and operations. If you are looking for a course which is very specific to an audience e.g. administrators or developers, ask for a custom outline. What will you do as part of this course ? As part of this course you will, GO through the theory to learn what is openshift platform, the core concepts relates and the advantages of using it. Install and configure a simple(non HA) multi node Openshift cluster with ansible . You would also have a conceptual understanding of how to build a production quality cluster with high availability, scalability, redundancy and security considerations. Learn how to deploy, configure, interconnect and publish and scale applications as well as isolate those with multi tenant environments that openshift provides and underlying kubernetes primitives that it leverages. Achieve Continuous Integration and Delivery with openshift's integration with git, jenkins and its implicit primitives including builds, pipelines and image streams. Learn how to manage persistent storage in a openshift environment Learn about the network and security considerations and features offered by Openshift. Learn openshift administration tasks such as setting up quotas, managing memberships, monitoring etc. What is not covered ? Even though this course covers many concepts related to kubernetes, since its a very vast topic, it still has the following areas uncovered. Cloud specific provisioning and integration HA installation of a kubernetes cluster with multi masters In depth openshift administration Writing Micro Services Applications Alternate container runtimes e.g. rocker/rkt, runc Pre Requisites Following are the pre requisite skills to attend this course. Since its a beginner level course, no prior experience with linux containers is assumed. Courses You should have attended the following course, or have demonstrable knowledge with the topics included in the following course. Docker Fundamentals Kubernetes Bootcamp Pre Assessment test will be conducted at the beginning of the course to asses the skills. Skills Docker Kubernetes Linux/Unix Systems Fundamentals Familiarity with Command Line Interface ( CLI ) Fundamental knowledge of editors on linux (any one of vi/nano/emacs) Understanding of YAML syntax and familiarity with reading/writing basic YAML specifications Recommended to have a basic understanding of Ansible Hardware and Software Requirements These are the prerequisites for each attendee. Hardware Requirements Software Requirements Laptop/Desktop with high speed internet connection Base Operating System : Windows / Mac OSX 8 GB RAM VirtualBox 4 CPU Cores Vagrant 20 GB Disk Space available ConEmu (Windows Only) Git for Windows (windows only) minishift Lab Setup : Instructions can be found at xxx Supporting Content/Materials Following is the supporting material which will be provided to you before/during the course Slides (online) Workshop (online link) Video Course - XXX by School of Devops Pre Class Checklist All participants should have completed the following checklist before attending the course . Successfully Completed Docker Fundamentals Course, or have equivalent skills. Successfully Completed Kubernetes Fundamentals Course, or have equivalent skills. Verify your system meets the hardware pre requisites. Validate the setup : verify all pre requisite software is installed on your system and is functional. Join our kubernetes channel on gitter Topics Following are the topics which would be covered as part of this course. Detailed course outline follows. Introduction to Openshift Openshift Quick Dive Application Lifecycle Management Application Stack Mapping Continuous Integration and Delivery Designing Production Grade Openshift Architecture Setup a Openshift Cluster Securing Openshift Openshift Administration Detailed Course Outline This is the detailed course outline with day wise list of contents Day I Introduction and Pre Assessment Trainer, class and course introduction Pre Assessment Test Module 1: Introduction to Openshift What is Openshift? Kubernetes Vs Openshift Key Features Architecture Module 2: Openshift Quick Dive Minishift Setup Minishift Create a Openshift environment Login and validate Module 3: Application Lifecycle Management Projects Types of application deployments Catalogue Image Image stream Bring your own image YAML files Module 4: Application Stack Mapping Deployments Pods Service Routes ConfigMaps Secrets Auto Scaling Module 5: Continuous Integration and Delivery Image streams Builds Git Integration and auto tagging Pipelines and Jenkins integration Automated Delivery Module 6: Designing Production Grade Openshift Architecture Components of Openshift etcd Masters and Nodes Registry Web console Achieving High Availability(HA) Module 7: Setup a Openshift Cluster Types of Openshfit installations Provision nodes Installing Openshift with Ansible Configuring inventory Defining variables Run a playbook Openshift Configuration Master and Node configs Authentication Authorization Network Configuration SDN with OpenVswitch SDN plugins Alternate configs Packet flow Registry Configuration Web Console Configuration Persistent Storage Storage plugins PV, PVC, Storage Class, etc ., Module 8: Securing Openshift Authentication Authorization RBAC Cluster and Local Roles Rules and Bindings SSC Security Context Constraints Security considerations Module 9: Openshift Administration Quota Memberships Monitoring Logging","title":"About the Course"},{"location":"openshift-bootcamp/#openshift-bootcamp","text":"Duration 2days Level Intermediate, Advanced Modules 10 Flipped Class No Customizable Yes","title":"Openshift Bootcamp"},{"location":"openshift-bootcamp/#objectives","text":"This course serves as a accelerator program to understand, setup and master openshift container platform for professionals who already have an understanding of docker and kubernetes.","title":"Objectives"},{"location":"openshift-bootcamp/#who-is-this-for","text":"This course is for someone who has already taken docker fundamentals, and kubernetes bootcamp courses/have equivalent knowledge, and would like to learn how to leverage Openshift Platform which is an abstraction on top of it. If you are a Operations/Systems personnel and would like to learn how to build a production grade scalable, fault tolerant and high available openshift infrastructure, and administer it, this course it for you. If you are a developer and would like to learn how to deploy your application stacks in production, on top of paas solution and also understand the underlying primitives, this course is for you. You could be developer/operations personnel and be in charge of securing application infrastructure and setup auxiliary services such as monitoring, centralized logging etc. this course is for you.","title":"Who is this for ?"},{"location":"openshift-bootcamp/#who-is-this-not-for","text":"If you are a advanced user of Openshift already, this course is definitely not for you. If you are interested in learning docker/container orchestration on windows , this course is not ideal for you as it focuses on linux containers. This is mix course for both developers and operations. If you are looking for a course which is very specific to an audience e.g. administrators or developers, ask for a custom outline.","title":"Who is this not for ?"},{"location":"openshift-bootcamp/#what-will-you-do-as-part-of-this-course","text":"As part of this course you will, GO through the theory to learn what is openshift platform, the core concepts relates and the advantages of using it. Install and configure a simple(non HA) multi node Openshift cluster with ansible . You would also have a conceptual understanding of how to build a production quality cluster with high availability, scalability, redundancy and security considerations. Learn how to deploy, configure, interconnect and publish and scale applications as well as isolate those with multi tenant environments that openshift provides and underlying kubernetes primitives that it leverages. Achieve Continuous Integration and Delivery with openshift's integration with git, jenkins and its implicit primitives including builds, pipelines and image streams. Learn how to manage persistent storage in a openshift environment Learn about the network and security considerations and features offered by Openshift. Learn openshift administration tasks such as setting up quotas, managing memberships, monitoring etc.","title":"What will you do as part of this course ?"},{"location":"openshift-bootcamp/#what-is-not-covered","text":"Even though this course covers many concepts related to kubernetes, since its a very vast topic, it still has the following areas uncovered. Cloud specific provisioning and integration HA installation of a kubernetes cluster with multi masters In depth openshift administration Writing Micro Services Applications Alternate container runtimes e.g. rocker/rkt, runc","title":"What is not covered ?"},{"location":"openshift-bootcamp/#pre-requisites","text":"Following are the pre requisite skills to attend this course. Since its a beginner level course, no prior experience with linux containers is assumed.","title":"Pre Requisites"},{"location":"openshift-bootcamp/#courses","text":"You should have attended the following course, or have demonstrable knowledge with the topics included in the following course. Docker Fundamentals Kubernetes Bootcamp Pre Assessment test will be conducted at the beginning of the course to asses the skills.","title":"Courses"},{"location":"openshift-bootcamp/#skills","text":"Docker Kubernetes Linux/Unix Systems Fundamentals Familiarity with Command Line Interface ( CLI ) Fundamental knowledge of editors on linux (any one of vi/nano/emacs) Understanding of YAML syntax and familiarity with reading/writing basic YAML specifications Recommended to have a basic understanding of Ansible","title":"Skills"},{"location":"openshift-bootcamp/#hardware-and-software-requirements","text":"These are the prerequisites for each attendee. Hardware Requirements Software Requirements Laptop/Desktop with high speed internet connection Base Operating System : Windows / Mac OSX 8 GB RAM VirtualBox 4 CPU Cores Vagrant 20 GB Disk Space available ConEmu (Windows Only) Git for Windows (windows only) minishift Lab Setup : Instructions can be found at xxx","title":"Hardware and Software  Requirements"},{"location":"openshift-bootcamp/#supporting-contentmaterials","text":"Following is the supporting material which will be provided to you before/during the course Slides (online) Workshop (online link) Video Course - XXX by School of Devops","title":"Supporting Content/Materials"},{"location":"openshift-bootcamp/#pre-class-checklist","text":"All participants should have completed the following checklist before attending the course . Successfully Completed Docker Fundamentals Course, or have equivalent skills. Successfully Completed Kubernetes Fundamentals Course, or have equivalent skills. Verify your system meets the hardware pre requisites. Validate the setup : verify all pre requisite software is installed on your system and is functional. Join our kubernetes channel on gitter","title":"Pre Class Checklist"},{"location":"openshift-bootcamp/#topics","text":"Following are the topics which would be covered as part of this course. Detailed course outline follows. Introduction to Openshift Openshift Quick Dive Application Lifecycle Management Application Stack Mapping Continuous Integration and Delivery Designing Production Grade Openshift Architecture Setup a Openshift Cluster Securing Openshift Openshift Administration","title":"Topics"},{"location":"openshift-bootcamp/#detailed-course-outline","text":"This is the detailed course outline with day wise list of contents","title":"Detailed Course Outline"},{"location":"openshift-bootcamp/#day-i","text":"","title":"Day I"},{"location":"openshift-bootcamp/#introduction-and-pre-assessment","text":"Trainer, class and course introduction Pre Assessment Test","title":"Introduction and Pre Assessment"},{"location":"openshift-bootcamp/#module-1-introduction-to-openshift","text":"What is Openshift? Kubernetes Vs Openshift Key Features Architecture","title":"Module 1: Introduction to Openshift"},{"location":"openshift-bootcamp/#module-2-openshift-quick-dive","text":"Minishift Setup Minishift Create a Openshift environment Login and validate","title":"Module 2: Openshift Quick Dive"},{"location":"openshift-bootcamp/#module-3-application-lifecycle-management","text":"Projects Types of application deployments Catalogue Image Image stream Bring your own image YAML files","title":"Module 3: Application Lifecycle Management"},{"location":"openshift-bootcamp/#module-4-application-stack-mapping","text":"Deployments Pods Service Routes ConfigMaps Secrets Auto Scaling","title":"Module 4: Application Stack Mapping"},{"location":"openshift-bootcamp/#module-5-continuous-integration-and-delivery","text":"Image streams Builds Git Integration and auto tagging Pipelines and Jenkins integration Automated Delivery","title":"Module 5: Continuous Integration and Delivery"},{"location":"openshift-bootcamp/#module-6-designing-production-grade-openshift-architecture","text":"Components of Openshift etcd Masters and Nodes Registry Web console Achieving High Availability(HA)","title":"Module 6: Designing Production Grade Openshift Architecture"},{"location":"openshift-bootcamp/#module-7-setup-a-openshift-cluster","text":"Types of Openshfit installations Provision nodes Installing Openshift with Ansible Configuring inventory Defining variables Run a playbook Openshift Configuration Master and Node configs Authentication Authorization Network Configuration SDN with OpenVswitch SDN plugins Alternate configs Packet flow Registry Configuration Web Console Configuration Persistent Storage Storage plugins PV, PVC, Storage Class, etc .,","title":"Module 7: Setup a Openshift Cluster"},{"location":"openshift-bootcamp/#module-8-securing-openshift","text":"Authentication Authorization RBAC Cluster and Local Roles Rules and Bindings SSC Security Context Constraints Security considerations","title":"Module 8: Securing Openshift"},{"location":"openshift-bootcamp/#module-9-openshift-administration","text":"Quota Memberships Monitoring Logging","title":"Module 9: Openshift Administration"},{"location":"openshift-cheatsheet/","text":"oc login Resources oc apply oc create oc delete oc replace oc export oc observe oc get oc describe oc edit oc set oc label oc annotate Getting Help oc types : openshift components oc explain : oc cluster : Examples oc explain routes oc explain pods Projects oc status oc new-project oc project oc projects Applications oc rollout oc rollback Administrators oc adm Pods oc get pods oc describe pods xxx oc rsh xxx oc exec oc attach oc logs oc port-forward oc rsync oc cp","title":"Openshift cheatsheet"},{"location":"openshift-cheatsheet/#resources","text":"oc apply oc create oc delete oc replace oc export oc observe oc get oc describe oc edit oc set oc label oc annotate","title":"Resources"},{"location":"openshift-cheatsheet/#getting-help","text":"oc types : openshift components oc explain : oc cluster :","title":"Getting Help"},{"location":"openshift-cheatsheet/#examples","text":"oc explain routes oc explain pods","title":"Examples"},{"location":"openshift-cheatsheet/#projects","text":"oc status oc new-project oc project oc projects","title":"Projects"},{"location":"openshift-cheatsheet/#applications","text":"oc rollout oc rollback","title":"Applications"},{"location":"openshift-cheatsheet/#administrators","text":"oc adm","title":"Administrators"},{"location":"openshift-cheatsheet/#pods","text":"oc get pods oc describe pods xxx oc rsh xxx oc exec oc attach oc logs oc port-forward oc rsync oc cp","title":"Pods"},{"location":"openshift_kubernetes_way/","text":"Openshift the Kubernetes way Creating a project and switching context In this section we are deploying instavote app app from GitHub repo. we use the same cluster earlier deployed simple aaplication with diffrent project namespace. This docs demonstrates how to get a simple project up and running on OpenShift. Application that will serve a welcome page Welcome to the DevOps Demo Application. Go to Home Page. click on create project. Enter the project name Display name and describtions. Download git repo. using following command. git clone https://github.com/schoolofdevops/openshift-code.git cd openshift-code/ OpenShift CommandLine When we are using OpenShift many of the tasks that you need to do can be performed through the OpenShift web Broser console or directly using the oc command line tool. or you can observe here backed utility. In this case we need the OpenShift token . we can create token using opnshift web console click on user Devloper and there is option copy login command click on this paste on your console. oc login https://128.199.213.193:8443 --token=< your token> [output] ``` Logged into \"https://128.199.213.193:8443\" as \"developer\" using the token provided. You have access to the following projects and can switch between them with 'oc project ': devops-demo-app * myproject Using project \"myproject\". here display your all the project with the curent project namespace. you can swaitch your project namespace using following Command check how many projects are present using following command. oc projects [ouput] You have access to the following projects and can switch between them with 'oc project ': instavote - instavote app stack myproject - My Project Using project \"myproject\" on server \"https://128.199.213.193:8443\". switch one project to another project using following command. oc project instavote [ouput] Now using project \"instavote\" on server \"https://128.199.213.193:8443\". check the context using following command. oc config get-contexts [output] CURRENT NAME CLUSTER AUTHINFO NAMESPACE /128-199-213-193:8443/developer 128-199-213-193:8443 developer/128-199-213-193:8443 default/128-199-213-193:8443/system:admin 128-199-213-193:8443 system:admin/128-199-213-193:8443 default devops-demo-app/128-199-213-193:8443/developer 128-199-213-193:8443 developer/128-199-213-193:8443 devops-demo-app * instavote/128-199-213-193:8443/developer 128-199-213-193:8443 developer/128-199-213-193:8443 instavote myproject/128-199-213-193:8443/developer 128-199-213-193:8443 developer/128-199-213-193:8443 myproject #### Writing a pod spec and applying it with oc cli In this section we talking about how to write pod spec Kubernetes way and deploy it on openshift. openshift underneath uses Kubernetes just see [openshift docs](https://docs.openshift.com/container-platform/3.10/welcome/index.html) . you can see here kubernetes all spec. openshift extends kubernetes api on top of kubernetes. we will start how to write pod spec. just clone repo. git clone https://github.com/schoolofdevops/openshift-code.git cd openshift-code/pods/ ##### Writing Pod Spec Lets now create the Pod config by adding the kind and specs to schme given in the file vote-pod.yaml as follows. apiVersion: v1 kind: metadata: spec: Lets edit this and add the pod specs apiVersion: v1 kind: Pod metadata: name: vote labels: app: python role: vote version: v1 spec: containers: - name: app image: schoolofdevops/vote:v1 ports: - containerPort: 80 protocol: TCP ##### Launching and operating a Pod use the following command to check syntax of yaml file oc apply -f vote-pod.yaml --dry-run use the following command to create pod . just remove --dry-run. oc apply -f vote-pod.yaml oc get pods [output] NAME READY STATUS RESTARTS AGE vote 1/1 Running 0 22s after few seconds shows pods status error because it can't connect port 80. here port 80 need root access that why it not working. openshift does not allow root privilege . just see the the security policy. oc get scc [output] Error from server (Forbidden): securitycontextconstraints.security.openshift.io is forbidden: User \"developer\" cannot list securitycontextconstraints.security.openshift.io at the cluster scope: User \"developer\" cannot list all securitycontextconstraints.security.openshift.io in the cluster find / -type f -name admin.kubeconfig [output] /home/openshift.local.clusterup/kube-apiserver/admin.kubeconfig /home/openshift.local.clusterup/openshift-controller-manager/admin.kubeconfig /home/openshift.local.clusterup/openshift-apiserver/admin.kubeconfig /root/openshift.local.clusterup/kube-apiserver/admin.kubeconfig /root/openshift.local.clusterup/openshift-controller-manager/admin.kubeconfig /root/openshift.local.clusterup/openshift-apiserver/admin.kubeconfig oc --config=/root/openshift.local.clusterup/kube-apiserver/admin.kubeconfig get scc [output] NAME PRIV CAPS SELINUX RUNASUSER FSGROUP SUPGROUP PRIORITY READONLYROOTFS VOLUMES anyuid false [] MustRunAs RunAsAny RunAsAny RunAsAny 10 false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] hostaccess false [] MustRunAs MustRunAsRange MustRunAs RunAsAny false [configMap downwardAPI emptyDir hostPath persistentVolumeClaim projected secret] hostmount-anyuid false [] MustRunAs RunAsAny RunAsAny RunAsAny false [configMap downwardAPI emptyDir hostPath nfs persistentVolumeClaim projected secret] hostnetwork false [] MustRunAs MustRunAsRange MustRunAs MustRunAs false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] nonroot false [] MustRunAs MustRunAsNonRoot RunAsAny RunAsAny false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] privileged true [ ] RunAsAny RunAsAny RunAsAny RunAsAny false [ ] restricted false [] MustRunAs MustRunAsRange MustRunAs RunAsAny false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] #### Rebuilding docker image with non privileged User and Port In the previous section we try writing pod spec. but it won't work because port mapping . In this section we will rebuild the image with different privilege. just clone the following repo. git clone https://github.com/initcron/example-voting-app.git cd example-voting-app/vote/ just edit Dockerfile Using official python runtime base image FROM schoolofdevops/voteapp-python:v0.1.0 ARG user=app ARG group=app ARG uid=1000 ARG gid=1000 Set the application directory WORKDIR /app Install our requirements.txt ADD requirements.txt /app/requirements.txt RUN pip install -r requirements.txt RUN addgroup -g ${gid} ${group} \\ && adduser -u ${uid} -G ${group} -s /bin/sh -D ${user} \\ && chown -R ${user}:${group} /app Copy our code from the current folder to /app inside the container ADD . /app Make port 80 available for links and/or publish EXPOSE 8080 USER ${user} Define our command to be run when launching the container CMD [\"gunicorn\", \"app:app\", \"-b\", \"0.0.0.0:8080\", \"--log-file\", \"-\", \"--access-logfile\", \"-\", \"--workers\", \"4\", \"--keep-alive\", \"0\"] then build docker build -t initcron/oc-vote:v1 then push image your docker hub registry . docker push initcron/oc-vote:v1 then change pod spec. apiVersion: v1 kind: Pod metadata: name: vote labels: app: python role: vote version: v1 spec: containers: - name: app image: initcron/oc-vote:v1 then run following CMD oc apply -f vote-pod.yml #### Pod operations - connect with shell, check logs, attach, delete This section we are looking about some of pod operation. start with following command. oc get pods [output] NAME READY STATUS RESTARTS AGE vote 1/1 Running 14 54m oc get pods -o wide [output] NAME READY STATUS RESTARTS AGE IP NODE vote 1/1 Running 14 55m 172.17.0.3 localhost oc descibe pod vote [output] Name: vote Namespace: instavote Node: localhost/128.199.213.193 Start Time: Wed, 17 Oct 2018 12:59:34 +0000 Labels: app=python role=vote version=v1 Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{},\"labels\":{\"app\":\"python\",\"role\":\"vote\",\"version\":\"v1\"},\"name\":\"vote\",\"namespace\":\"instavot... openshift.io/scc=restricted Status: Running IP: 172.17.0.3 Containers: app: Container ID: docker://e71c8b4e1f2da2b07a3fb030ae4d81e603d8d3764bb070a7aaef96bcab7c354e Image: initcron/oc-vote:v1 Image ID: docker-pullable://initcron/oc-vote@sha256:b05f64d225a8671ecc09d22760a2c76bd65aaf50e12d00b31ab3b4fe6f377b06 Port: Host Port: State: Running Started: Wed, 17 Oct 2018 13:46:31 +0000 Last State: Terminated Reason: Error Exit Code: 1 Started: Wed, 17 Oct 2018 13:41:54 +0000 Finished: Wed, 17 Oct 2018 13:42:00 +0000 Ready: True Restart Count: 14 Environment: Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-b4xlj (ro) Conditions: Type Status Initialized True Ready True PodScheduled True Volumes: default-token-b4xlj: Type: Secret (a volume populated by a Secret) SecretName: default-token-b4xlj Optional: false QoS Class: BestEffort Node-Selectors: Tolerations: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 56m default-scheduler Successfully assigned vote to localhost Normal Pulled 54m (x5 over 56m) kubelet, localhost Container image \"schoolofdevops/vote:v1\" already present on machine Normal Created 54m (x5 over 56m) kubelet, localhost Created container Normal Started 54m (x5 over 56m) kubelet, localhost Started container Warning BackOff 11m (x197 over 56m) kubelet, localhost Back-off restarting failed container use following command to go inside container. oc exec -it vote sh /app $ cat /etc/issue Welcome to Alpine Linux 3.4 Kernel \\r on an \\m (\\l) ```","title":"Openshift kubernetes way"},{"location":"openshift_kubernetes_way/#openshift-the-kubernetes-way","text":"","title":"Openshift the Kubernetes way"},{"location":"openshift_kubernetes_way/#creating-a-project-and-switching-context","text":"In this section we are deploying instavote app app from GitHub repo. we use the same cluster earlier deployed simple aaplication with diffrent project namespace. This docs demonstrates how to get a simple project up and running on OpenShift. Application that will serve a welcome page Welcome to the DevOps Demo Application. Go to Home Page. click on create project. Enter the project name Display name and describtions. Download git repo. using following command. git clone https://github.com/schoolofdevops/openshift-code.git cd openshift-code/","title":"Creating a project and switching context"},{"location":"openshift_kubernetes_way/#openshift-commandline","text":"When we are using OpenShift many of the tasks that you need to do can be performed through the OpenShift web Broser console or directly using the oc command line tool. or you can observe here backed utility. In this case we need the OpenShift token . we can create token using opnshift web console click on user Devloper and there is option copy login command click on this paste on your console. oc login https://128.199.213.193:8443 --token=< your token> [output] ``` Logged into \"https://128.199.213.193:8443\" as \"developer\" using the token provided. You have access to the following projects and can switch between them with 'oc project ': devops-demo-app * myproject Using project \"myproject\". here display your all the project with the curent project namespace. you can swaitch your project namespace using following Command check how many projects are present using following command. oc projects [ouput] You have access to the following projects and can switch between them with 'oc project ': instavote - instavote app stack myproject - My Project Using project \"myproject\" on server \"https://128.199.213.193:8443\". switch one project to another project using following command. oc project instavote [ouput] Now using project \"instavote\" on server \"https://128.199.213.193:8443\". check the context using following command. oc config get-contexts [output] CURRENT NAME CLUSTER AUTHINFO NAMESPACE /128-199-213-193:8443/developer 128-199-213-193:8443 developer/128-199-213-193:8443 default/128-199-213-193:8443/system:admin 128-199-213-193:8443 system:admin/128-199-213-193:8443 default devops-demo-app/128-199-213-193:8443/developer 128-199-213-193:8443 developer/128-199-213-193:8443 devops-demo-app * instavote/128-199-213-193:8443/developer 128-199-213-193:8443 developer/128-199-213-193:8443 instavote myproject/128-199-213-193:8443/developer 128-199-213-193:8443 developer/128-199-213-193:8443 myproject #### Writing a pod spec and applying it with oc cli In this section we talking about how to write pod spec Kubernetes way and deploy it on openshift. openshift underneath uses Kubernetes just see [openshift docs](https://docs.openshift.com/container-platform/3.10/welcome/index.html) . you can see here kubernetes all spec. openshift extends kubernetes api on top of kubernetes. we will start how to write pod spec. just clone repo. git clone https://github.com/schoolofdevops/openshift-code.git cd openshift-code/pods/ ##### Writing Pod Spec Lets now create the Pod config by adding the kind and specs to schme given in the file vote-pod.yaml as follows. apiVersion: v1 kind: metadata: spec: Lets edit this and add the pod specs apiVersion: v1 kind: Pod metadata: name: vote labels: app: python role: vote version: v1 spec: containers: - name: app image: schoolofdevops/vote:v1 ports: - containerPort: 80 protocol: TCP ##### Launching and operating a Pod use the following command to check syntax of yaml file oc apply -f vote-pod.yaml --dry-run use the following command to create pod . just remove --dry-run. oc apply -f vote-pod.yaml oc get pods [output] NAME READY STATUS RESTARTS AGE vote 1/1 Running 0 22s after few seconds shows pods status error because it can't connect port 80. here port 80 need root access that why it not working. openshift does not allow root privilege . just see the the security policy. oc get scc [output] Error from server (Forbidden): securitycontextconstraints.security.openshift.io is forbidden: User \"developer\" cannot list securitycontextconstraints.security.openshift.io at the cluster scope: User \"developer\" cannot list all securitycontextconstraints.security.openshift.io in the cluster find / -type f -name admin.kubeconfig [output] /home/openshift.local.clusterup/kube-apiserver/admin.kubeconfig /home/openshift.local.clusterup/openshift-controller-manager/admin.kubeconfig /home/openshift.local.clusterup/openshift-apiserver/admin.kubeconfig /root/openshift.local.clusterup/kube-apiserver/admin.kubeconfig /root/openshift.local.clusterup/openshift-controller-manager/admin.kubeconfig /root/openshift.local.clusterup/openshift-apiserver/admin.kubeconfig oc --config=/root/openshift.local.clusterup/kube-apiserver/admin.kubeconfig get scc [output] NAME PRIV CAPS SELINUX RUNASUSER FSGROUP SUPGROUP PRIORITY READONLYROOTFS VOLUMES anyuid false [] MustRunAs RunAsAny RunAsAny RunAsAny 10 false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] hostaccess false [] MustRunAs MustRunAsRange MustRunAs RunAsAny false [configMap downwardAPI emptyDir hostPath persistentVolumeClaim projected secret] hostmount-anyuid false [] MustRunAs RunAsAny RunAsAny RunAsAny false [configMap downwardAPI emptyDir hostPath nfs persistentVolumeClaim projected secret] hostnetwork false [] MustRunAs MustRunAsRange MustRunAs MustRunAs false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] nonroot false [] MustRunAs MustRunAsNonRoot RunAsAny RunAsAny false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] privileged true [ ] RunAsAny RunAsAny RunAsAny RunAsAny false [ ] restricted false [] MustRunAs MustRunAsRange MustRunAs RunAsAny false [configMap downwardAPI emptyDir persistentVolumeClaim projected secret] #### Rebuilding docker image with non privileged User and Port In the previous section we try writing pod spec. but it won't work because port mapping . In this section we will rebuild the image with different privilege. just clone the following repo. git clone https://github.com/initcron/example-voting-app.git cd example-voting-app/vote/ just edit Dockerfile","title":"OpenShift CommandLine"},{"location":"openshift_kubernetes_way/#using-official-python-runtime-base-image","text":"FROM schoolofdevops/voteapp-python:v0.1.0 ARG user=app ARG group=app ARG uid=1000 ARG gid=1000","title":"Using official python runtime base image"},{"location":"openshift_kubernetes_way/#set-the-application-directory","text":"WORKDIR /app","title":"Set the application directory"},{"location":"openshift_kubernetes_way/#install-our-requirementstxt","text":"ADD requirements.txt /app/requirements.txt RUN pip install -r requirements.txt RUN addgroup -g ${gid} ${group} \\ && adduser -u ${uid} -G ${group} -s /bin/sh -D ${user} \\ && chown -R ${user}:${group} /app","title":"Install our requirements.txt"},{"location":"openshift_kubernetes_way/#copy-our-code-from-the-current-folder-to-app-inside-the-container","text":"ADD . /app","title":"Copy our code from the current folder to /app inside the container"},{"location":"openshift_kubernetes_way/#make-port-80-available-for-links-andor-publish","text":"EXPOSE 8080 USER ${user}","title":"Make port 80 available for links and/or publish"},{"location":"openshift_kubernetes_way/#define-our-command-to-be-run-when-launching-the-container","text":"CMD [\"gunicorn\", \"app:app\", \"-b\", \"0.0.0.0:8080\", \"--log-file\", \"-\", \"--access-logfile\", \"-\", \"--workers\", \"4\", \"--keep-alive\", \"0\"] then build docker build -t initcron/oc-vote:v1 then push image your docker hub registry . docker push initcron/oc-vote:v1 then change pod spec. apiVersion: v1 kind: Pod metadata: name: vote labels: app: python role: vote version: v1 spec: containers: - name: app image: initcron/oc-vote:v1 then run following CMD oc apply -f vote-pod.yml #### Pod operations - connect with shell, check logs, attach, delete This section we are looking about some of pod operation. start with following command. oc get pods [output] NAME READY STATUS RESTARTS AGE vote 1/1 Running 14 54m oc get pods -o wide [output] NAME READY STATUS RESTARTS AGE IP NODE vote 1/1 Running 14 55m 172.17.0.3 localhost oc descibe pod vote [output] Name: vote Namespace: instavote Node: localhost/128.199.213.193 Start Time: Wed, 17 Oct 2018 12:59:34 +0000 Labels: app=python role=vote version=v1 Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":{\"annotations\":{},\"labels\":{\"app\":\"python\",\"role\":\"vote\",\"version\":\"v1\"},\"name\":\"vote\",\"namespace\":\"instavot... openshift.io/scc=restricted Status: Running IP: 172.17.0.3 Containers: app: Container ID: docker://e71c8b4e1f2da2b07a3fb030ae4d81e603d8d3764bb070a7aaef96bcab7c354e Image: initcron/oc-vote:v1 Image ID: docker-pullable://initcron/oc-vote@sha256:b05f64d225a8671ecc09d22760a2c76bd65aaf50e12d00b31ab3b4fe6f377b06 Port: Host Port: State: Running Started: Wed, 17 Oct 2018 13:46:31 +0000 Last State: Terminated Reason: Error Exit Code: 1 Started: Wed, 17 Oct 2018 13:41:54 +0000 Finished: Wed, 17 Oct 2018 13:42:00 +0000 Ready: True Restart Count: 14 Environment: Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-b4xlj (ro) Conditions: Type Status Initialized True Ready True PodScheduled True Volumes: default-token-b4xlj: Type: Secret (a volume populated by a Secret) SecretName: default-token-b4xlj Optional: false QoS Class: BestEffort Node-Selectors: Tolerations: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 56m default-scheduler Successfully assigned vote to localhost Normal Pulled 54m (x5 over 56m) kubelet, localhost Container image \"schoolofdevops/vote:v1\" already present on machine Normal Created 54m (x5 over 56m) kubelet, localhost Created container Normal Started 54m (x5 over 56m) kubelet, localhost Started container Warning BackOff 11m (x197 over 56m) kubelet, localhost Back-off restarting failed container use following command to go inside container. oc exec -it vote sh /app $ cat /etc/issue Welcome to Alpine Linux 3.4 Kernel \\r on an \\m (\\l) ```","title":"Define our command to be run when launching the container"},{"location":"rollouts-and-rollbacks/","text":"Rolling updates with deployments Update the version of the image in vote_deploy.yaml File: vote_deploy.yaml ... app: vote spec: containers: - image: schoolofdevops/vote:movies Apply Changes and monitor the rollout oc apply -f vote-deploy.yaml oc rollout status deployment/vote Rolling Back a Failed Update Lets update the image to a tag which is non existant. We intentionally introduce this intentional error to fail fail the deployment. File: vote_deploy.yaml ... app: vote spec: containers: - image: schoolofdevops/vote:movi Do a new rollout and monitor oc apply -f vote_deploy.yaml oc rollout status deployment/vote Also watch the pod status which might look like vote-3040199436-sdq17 1/1 Running 0 9m vote-4086029260-0vjjb 0/1 ErrImagePull 0 16s vote-4086029260-zvgmd 0/1 ImagePullBackOff 0 15s vote-rc-fsdsd 1/1 Running 0 27m vote-rc-mcxs5 1/1 Running 0 To get the revision history and details oc rollout history deployment/vote oc rollout history deployment/vote --revision=x [replace x with the latest revision] [Sample Output] root@kube-01:~# oc rollout history deployment/vote deployments \"vote\" REVISION CHANGE-CAUSE 1 oc scale deployment/vote --replicas=5 3 <none> 6 <none> 7 <none> root@kube-01:~# oc rollout history deployment/vote --revision=7 deployments \"vote\" with revision #7 Pod Template: Labels: app=vote env=dev pod-template-hash=4086029260 role=ui stack=voting tier=front Containers: vote: Image: schoolofdevops/vote:movi Port: 80/TCP Environment: <none> Mounts: <none> Volumes: <none> To undo rollout, oc rollout undo deployment/vote or oc rollout undo deployment/vote --to-revision=1 oc get rs oc describe deployment vote","title":"Rollouts and Rollbacks"},{"location":"rollouts-and-rollbacks/#rolling-updates-with-deployments","text":"Update the version of the image in vote_deploy.yaml File: vote_deploy.yaml ... app: vote spec: containers: - image: schoolofdevops/vote:movies Apply Changes and monitor the rollout oc apply -f vote-deploy.yaml oc rollout status deployment/vote","title":"Rolling updates with deployments"},{"location":"rollouts-and-rollbacks/#rolling-back-a-failed-update","text":"Lets update the image to a tag which is non existant. We intentionally introduce this intentional error to fail fail the deployment. File: vote_deploy.yaml ... app: vote spec: containers: - image: schoolofdevops/vote:movi Do a new rollout and monitor oc apply -f vote_deploy.yaml oc rollout status deployment/vote Also watch the pod status which might look like vote-3040199436-sdq17 1/1 Running 0 9m vote-4086029260-0vjjb 0/1 ErrImagePull 0 16s vote-4086029260-zvgmd 0/1 ImagePullBackOff 0 15s vote-rc-fsdsd 1/1 Running 0 27m vote-rc-mcxs5 1/1 Running 0 To get the revision history and details oc rollout history deployment/vote oc rollout history deployment/vote --revision=x [replace x with the latest revision] [Sample Output] root@kube-01:~# oc rollout history deployment/vote deployments \"vote\" REVISION CHANGE-CAUSE 1 oc scale deployment/vote --replicas=5 3 <none> 6 <none> 7 <none> root@kube-01:~# oc rollout history deployment/vote --revision=7 deployments \"vote\" with revision #7 Pod Template: Labels: app=vote env=dev pod-template-hash=4086029260 role=ui stack=voting tier=front Containers: vote: Image: schoolofdevops/vote:movi Port: 80/TCP Environment: <none> Mounts: <none> Volumes: <none> To undo rollout, oc rollout undo deployment/vote or oc rollout undo deployment/vote --to-revision=1 oc get rs oc describe deployment vote","title":"Rolling Back a Failed Update"}]}